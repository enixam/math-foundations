[
["index.html", "Mathematical Notes for Machine Learning Preface", " Mathematical Notes for Machine Learning Qiushi Yan 2020-09-18 Preface This document aims to cover various mathematical topics relevant to machine learning applications, including linear algebra, multivariate calculus, probability theory, computational learning theory, and optimization (statistics should be in another stand-alone document). Here is an short outline of each big part … Major references (books, online courses, videos, papers) include: general Mathematics for Machine Learning by Garret Thomas Mathematics for Machine Learning (Deisenroth, Faisal, and Ong 2020) Advanced Data Analysis from an Elementary Point of View by Cosma Rohilla Shalizi Linear Algebra Linear Algebra and its Applications (Lay 2006) Linear Algebra and Optimization for Machine Learning (Aggarwal 2020) Matrix Algebra: Theory, Computations, and Applications in Statistics (Gentle 2017) Linear Algebra and its Applications (Strang 2006) Linear Algebra Review and Reference on Stanford’s cs229 website Gilbert Strang. 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. Spring 2018. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA. N. Johnston. Advanced Linear and Matrix Algebra. Springer International Publishing, 2020.(Johnston 2020) Multivariate Calculus Thomas’s Calculus (Weir, Thomas, and Hass 2018) Steve Butler. Multivariate Calculus. Lowa State Probability Theory Probability for Statistics and Machine Learning: Fundamentals and Advanced Topics (DasGupta 2011) Introduction to Probability (K. Blitzstein and Hwang 2019) Introduction to Probability (Bertsekas and Tsitsiklis 2008) John Tsitsiklis, and Patrick Jaillet. RES.6-012 Introduction to Probability. Spring 2018. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA. Computational Learning Theory Learning from Data (Abu-Mostafa, Magdon-Ismail, and Lin 2012) The document is generated by the bookdown package (Xie 2020). "],
["basic-matrix-algebra.html", "Chapter 1 Basic Matrix Algebra 1.1 Matrix Multiplication 1.2 Elemetary matrix and row operations 1.3 LU Factorization 1.4 Determinants 1.5 Trace 1.6 Matrix Inversion 1.7 Complexity of Matrix Computation", " Chapter 1 Basic Matrix Algebra 1.1 Matrix Multiplication A common way of looking at matrix-vector multiplication \\(A\\bar{x}\\) is to think of as a linear combination of column vectors in \\(A\\): \\[ \\begin{aligned} A\\bar{x} &amp;= [\\bar{a}_1 \\;\\; \\cdots \\;\\; \\bar{a}_n] \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\\\ &amp;= x_1\\bar{a}_1 + \\cdots + x_n\\bar{a}_n \\end{aligned} \\] Likewise, for any \\(\\bar{x}^{T} = [x_1, \\cdots, x_n]^T\\) and matrix \\(A_{m \\times n}\\), \\(x^{T}A\\) can be thought of as a linear combination of rows in \\(A\\) to produce a new row vector: \\[ [x_1 \\;\\; \\cdots \\;\\; x_n] \\begin{bmatrix} \\bar{a}_1^T \\\\ \\vdots \\\\ \\bar{a}_n^T \\end{bmatrix} = x_1\\bar{a}_1^T + \\dots + x_n\\bar{a}_n^T \\] For matrix-matrix multiplication \\(AB\\), besides the dot product definition we can see it as column row expansion. Theorem 1.1 (Column-row expansion of \\(AB\\)) if \\(A\\) is \\(m \\times n\\) and \\(B\\) is \\(n \\times p\\), then \\[ \\begin{aligned} AB &amp;= [\\text{col}_1(A) \\cdots \\text{col}_n(A)] \\begin{bmatrix} \\text{row}_1(B) \\\\ \\vdots \\\\ \\text{row}_n(B) \\end{bmatrix} \\\\ &amp;= \\text{col}_1(A)\\text{row}_1(B) + \\cdots + \\text{col}_n(A)\\text{row}_n(B) \\end{aligned} \\] Note that each \\(\\text{col}_1(A)\\text{row}_1(B)\\) is a rank 1 \\(m \\times p\\) matrix. The following subsections follows Chapter 7 of VMLS (Boyd and Vandenberghe 2018), introducing some special matrices and their effect in matrix multiplication. 1.1.1 Geometric Transformations scaling dilation rotation \\[ \\begin{bmatrix} \\cos\\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{bmatrix} \\begin{bmatrix} \\rho \\cos\\alpha \\\\ \\rho \\sin\\alpha \\end{bmatrix} = \\begin{bmatrix} \\rho(\\cos\\theta\\cos\\alpha - \\sin\\theta\\sin\\alpha) \\\\ \\rho(\\sin\\theta \\cos\\alpha + \\cos \\theta \\sin\\alpha) \\end{bmatrix} = \\begin{bmatrix} \\rho \\cos(\\theta + \\alpha) \\\\ \\rho \\sin(\\theta + \\alpha) \\end{bmatrix} \\] reflection 1.1.2 Matrix Multiplication as Linear Transformation They are matrices whose multiplication effect do that fall into specific geometric categories like scaling, dilation, and rotation. They do, in some sense, exert the same type of influence on vectors through multiplication. It turns out that another way to look at \\(A_{m \\times n} \\, x _{ n \\times 1} = b_{m \\times 1}\\), besides linear combination of column vectors, is to think of the matrix \\(A\\) as an force that “acts” on a vector \\(x\\) in \\(\\mathbb{R^n}\\) by multiplication to produce a new vector called \\(b\\) in \\(\\mathbb{\\mathbb{R^m}}\\). A transformation \\(T\\) from \\(\\mathbb{R^n}\\) to \\(\\mathbb{R^m}\\) is a rule that assigns each vector {x} in \\(\\mathbb{R^n}\\) a vector \\(T(x)\\) in \\(\\mathbb{R^m}\\), which is called the image of {x} (under the action of \\(T\\)). It can be show that such transformations induced by multiplying a matrix is a type of linear transformation, because it satisfies all required properties to be linear: \\[ \\begin{aligned} \\text{vector addition} \\quad A(\\bar{u} + \\bar{v}) &amp;= A\\bar{u} + A\\bar{v} \\\\ \\text{scalar multiplication} \\quad A(c\\bar{u}) &amp;= cA\\bar{u} \\end{aligned} \\] Theorem 1.2 (left multiplication as linear transformation) There is a one to one relationship between a linear transformation and a matrix. Let \\(T: \\mathbb{R^n} \\rightarrow \\mathbb{R^m}\\) be a linear transformation. Then there exists a unique matrix \\(A\\) such that: \\[ T(x) = Ax \\quad \\text{for all} \\; x \\; \\text{in} \\; \\mathbb{R^n} \\] In fact, \\(A\\) is a \\(m \\times n\\) matrix whose \\(j\\)th column is the vector \\(T(\\bar{e_j})\\), where \\(\\bar{e_j}\\) is the \\(j\\)th basis of \\(\\mathbb{R^n}\\) Proof \\[ \\bar{x} = x_1\\bar{e_1} + \\dots + x_n{\\bar{e_n}} \\] And because \\(T(\\bar{x})\\) is a linear transformation: \\[ \\begin{split} T(\\bar{x}) &amp;= x_1T(\\bar{e}_1) + \\dots + x_nT(\\bar{e}_n) \\\\ &amp;= [T(\\bar{e}_1) \\, \\cdots \\, T(\\bar{e}_n)]\\bar{x} \\\\ &amp;= (A\\bar{x})_{m \\times 1} \\end{split} \\] In other words, the transformation is specified once we know what all basis in \\(\\mathbb{R^n}\\) become in \\(\\mathbb{R^m}\\). The matrix \\(A\\) is called the standard matrix for the linear transformation \\(T\\). Definition 1.1 (A mapping is onto \\(\\mathbb{R^m}\\)) A mapping \\(T: \\mathbb{R^n} \\rightarrow \\mathbb{R^m}\\) is said to be onto if each \\(\\bar{b}\\) in \\(\\mathbb{R^m}\\) is the image of at least one \\(\\bar{x}\\) in Equivalently, \\(T\\) is onto \\(\\mathbb{R^m}\\) means that there exists at least one solution of \\(T(\\bar{x}) = \\bar{b}\\) for any \\(\\bar{b}\\) in \\(\\mathbb{R}^m\\) Definition 1.2 (one-to-one mapping) A mapping T: \\(\\mathbb{R^n} \\rightarrow \\mathbb{R^m}\\) is said to be one-to-one if each \\(\\bar{b}\\) in \\(\\mathbb{R^m}\\) is the image of at most one \\(\\bar{x}\\) in \\(\\mathbb{R^n}\\) Equivalently, \\(T\\) is one-to-one if, for each \\(\\bar{b}\\) in \\(\\mathbb{R^m}\\), the equation \\(T(\\bar{x}) = \\bar{b}\\) has either a unique solution or none at all. Theorem 1.3 Let \\(\\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) be a linear transformation, and \\(A\\) the standard matrix. Then \\(T\\) maps \\(mathbb{R}^n\\) onto \\(\\mathbb{R}^m\\) if only if columns of \\(A\\) span \\(\\mathbb{R}^m\\) \\(R\\) is one-to-one if and only if columns of \\(A\\) are linearly independent 1.1.3 Selector Matrix Let the standard basis in \\(\\mathcal{R}^n\\) be \\[ \\begin{aligned} \\bar{e}_1^T &amp;= [1, 0, \\cdots, ..., 0] \\\\ \\bar{e}_2^T &amp;= [0, 1, \\cdots, ..., 0] \\\\ &amp; \\vdots \\\\ \\bar{e}_n^T &amp;= [0, 0, \\cdots, ..., 1] \\end{aligned} \\] Then an \\(m \\times n\\) matrix \\(A\\) formed by a subset of such vectors are called a selector matrix. In other words, an \\(m \\times n\\) selector matrix \\(A\\) is one in which each row is a standard unit vector \\[ A = \\begin{bmatrix} \\bar{e}_{k_1}^T \\\\ \\vdots \\\\ \\bar{e}_{k_m}^T \\end{bmatrix} \\] where \\(1 \\le k_1, ..., k_m \\le n\\). When a selector matrix multiplies a vector \\(\\bar{x}\\), it returned the \\(k_i\\)th entry of \\(\\bar{x}\\) on the \\(i\\)th entry of \\(y = A\\bar{x}\\). For example, we can construct a \\(4 \\times n\\) selector matrix \\[ A = \\begin{bmatrix} \\bar{e}_2^T \\\\ \\bar{e}_1^T \\\\ \\bar{e}_5^T \\\\ \\bar{e}_8^T \\end{bmatrix} \\] When multiplying \\(\\bar{x}\\) \\[ \\begin{bmatrix} \\bar{e}_2^T \\\\ \\bar{e}_1^T \\\\ \\bar{e}_5^T \\\\ \\bar{e}_8^T \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} x_2 \\\\ x_1 \\\\ x_5 \\\\ x_8 \\end{bmatrix} \\] The identity matrix, and the reverser matrix \\[ A = \\begin{bmatrix} \\bar{e}_n^T \\\\ \\vdots \\\\ \\bar{e}_1^T \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\end{bmatrix} \\] are one of the selector matrices family. Selector matrices have important applications in down-sampling, image cropping, and permutation. (P.144, VSLM) 1.1.4 Discrete Convolution The (discrete) convolution of an \\(n\\)-vector \\(a\\) \\(a\\) and \\(m\\)-vector \\(b\\) is a \\((n + m - 1)\\)-vector, denoted \\(a * b\\), with entries \\[ c_k = \\sum_{i + j = k + 1}a_ib_j, \\quad k = 1, ..., n + m - 1 \\] For example, the convolution of \\(n = 5\\) and \\(m = 3\\) is \\[ \\begin{aligned} c_1 &amp;= a_1b_1 \\\\ c_2 &amp;= a_1b_2 + a_2b_1 \\\\ c_3 &amp;= a_1b_3 + a_2b_2 + a_3b_1 \\\\ c_4 &amp;= a_2b_3 + a_3b_2 + a_4b_1 \\\\ c_5 &amp;= a_3b_3 + a_4b_2 + a_5b_1 \\\\ c_6 &amp;= a_4b_3 + a_5b_2 \\\\ c_7 &amp;= a_5b_3 \\end{aligned} \\] A important application of convolution arises in polynomial multiplication. If \\(a\\) and \\(b\\) represent coefficients of two polynomials \\[ p(x) = a_1 + a_2x + \\cdots + a_nx^{n-1}, \\quad q(x) = b_1 + b_2x + \\cdots + b_mx^{m - 1} \\] then the coefficients of the product polynomial \\(p(x)q(x)\\) are represented by \\(c = a * b\\) \\[ p(x)q(x) = c_1 + c_2x + \\cdots + c_{n + m - 1}x^{n + m - 2} \\] With this interpretation using polynomial multiplication, we can quickly verify some properties of convolution symmetric \\(a * b = b * a\\) associative \\((a * b) * c = a * (b * c)\\) \\(a * b = 0\\) if and only if one of \\(a\\) and \\(b\\) are zero vectors. How is convolution related to matrix multiplication. A basic fact is that for a fixed \\(a\\), \\(a * b\\) is a linear function of \\(b\\), and likewise for \\(b\\). This mean \\(a * b\\) can be expressed as a matrix-vector product \\[ a * b = T(b)a = T(a)b \\] According to the relationship between matrix multiplication and linear transformation, we conclude that \\(T(b)\\) and \\(T(a)\\) are matrices that are determined and constructed by the elemetns of \\(b\\) and \\(a\\) respectively. Refer to the previous example, we have \\[ T(b) = \\begin{bmatrix} b_1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ b_2 &amp; b_1 &amp; 0 &amp; 0 &amp; 0 \\\\ b_3 &amp; b_2 &amp; b_1 &amp; 0 &amp; 0 \\\\ 0 &amp; b_3 &amp; b_2 &amp; b_1 &amp;0 \\\\ 0 &amp; 0 &amp; b_3 &amp; b_2 &amp; b_1 \\\\ 0 &amp; 0 &amp; 0 &amp; b_3 &amp; b_2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; b_3 \\end{bmatrix}, \\quad T(a) = \\begin{bmatrix} a_1 &amp; 0 &amp; 0 \\\\ a_2 &amp; a_1 &amp; 0 \\\\ a_3 &amp; a_2 &amp; a_1 \\\\ a_4 &amp; a_3 &amp; a_2 \\\\ a_5 &amp; a_4 &amp; a_3 \\\\ 0 &amp; a_5 &amp; a_4 \\\\ 0 &amp; 0 &amp; a_5 \\end{bmatrix} \\] \\(T(a)\\) and \\(T(b)\\) both have the same values along any diagonal, a property characterizing the so-called Toeplitz matrices. Convolution has computational applications in areas like time series smoothing, first order differences and audio filtering. (P. 150, VMLS) 1.2 Elemetary matrix and row operations An elementary matrix is one that is obtained by performing a single elementary row operation on an identity matrix \\(I\\). Each elementary matrix \\(E\\) is invertible. The inverse of \\(E\\) is the elementary matrix of the same type that transforms \\(E\\) back into \\(I\\). Left multiplication by a elementary matrix has a nice illustration. There are 3 primary types of elementary matrices (example for \\(3 \\times 3\\)): \\[ E_1 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ -4 &amp; 0 &amp; 1 \\end{bmatrix} ,\\; E_2 = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} ,\\; E_3 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 5 \\end{bmatrix} \\] \\(E_1, E_2, E_3\\) represents 3 types of elementary row operations applicable to a \\(3 \\times 3\\) matrix, Row addition, a scalar multiple of the \\(i\\)th row is added to the \\(j\\)th row Row interchange, the \\(i\\)th row and the \\(j\\)th row of the matrix are interchanged Row scaling, the \\(i\\)th row is multiplied by a scalar. \\[ \\begin{aligned} E_1A &amp;= \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ - 4a_{11} + a_{31} &amp; -4a_{12} + a_{32} &amp; -4a_{13} + a_{33} \\end{bmatrix} \\\\ \\\\ E_2A &amp;= \\begin{bmatrix} a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{bmatrix} \\\\ \\\\ E_3A &amp;= \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ 5a_{31} &amp; 5a_{32} &amp; 5a_{33} \\\\ \\end{bmatrix} \\end{aligned} \\] Thus, any row operation on \\(A\\) is equivalent to left multiply a corresponding elementary matrix \\(E\\). Since row operation are invertible, elementary matrices are invertible. This gives a general way of finding the inverse matrix of \\(A\\). Theorem 1.4 (an algorithm for finding inverse matrices) Row reduce the augmented matrix \\([A \\;\\; I]\\), when \\(A\\) becomes \\(I\\), \\(I\\) becomes \\(A^{-1}\\). Otherwise \\(A^{-1}\\) is not invertible. 1.3 LU Factorization https://fml-fam.github.io/blog/2020/07/03/matrix-factorizations-for-data-analysis/ A factorization of matrix \\(A\\) is an equation that expresses \\(A\\) as a product of two or more matrices. Definition 1.3 (LU factorization) Suppose \\(A\\) can be reduced to an echelon form \\(U\\) using only row addition and row scaling, there exist a set of unit lower trangular matrices \\(E_1, \\dots, E_p\\) such that \\[ E_p \\cdots E_1A = U \\] Then \\[ A = (E_p \\cdots E_1)^{-1}U = LU \\] where \\(u\\) is a the upper triangular row echelon form (or upper trapezoidal), and \\(L\\) an lower triangular matrix \\[ L = (E_p \\cdots E_1)^{-1} \\] LU decomposition expresses a matrix (don’t have to be square or invertible) as the product of a square lower triangular matrix \\(L\\) and a rectangular upper triangular matrix \\(U\\). From the definition, we know that row operations on \\(A\\) must only be confined to row addition and row scaling, but not row interchange. Otherwise \\((E_p \\cdots E_1)^{-1}\\) cannot be lower triangular. The most common needs for row exchanges in row reduction is when \\(a_{11}\\) is 0. For example, the following matrix does not have a LU factorization because it first requires exchanging two rows to produce row echelon form1 \\[ \\begin{bmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{bmatrix} \\] To give a former definition of LU factorization, one can show that if all principal submatrices are non-singular2, then the factorization exists. Note that this condition is not necessary, the following matrix has a zero in (2, 2) position violating the rule, but it can still be expressed in terms of LU \\[ \\begin{bmatrix} 2 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 0 &amp; 0 \\\\ 1 &amp; -\\frac{1}{2} &amp; 0 \\\\ 1 &amp; \\frac{1}{2} &amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; \\frac{1}{2} &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] If an LU factorization exists, we shall notice that the LU form is “asymmetric” on the diagonal, in the sense that L has all \\(1\\)s and \\(U\\) has the pivots. We can factor out another diagonal matrix \\(D\\) to also have all \\(1\\)s on \\(U\\)’s diagonal \\[ U = \\begin{bmatrix} d_{1} \\\\ &amp; d_{2} \\\\ &amp; &amp; \\ddots \\\\ &amp; &amp; &amp; d_n \\end{bmatrix} \\begin{bmatrix} 1 &amp; u_{12} / d_1 &amp; \\cdots &amp; u_{1n}/d_1 \\\\ &amp; 1 &amp; \\cdots &amp; u_{2n}/d_2 \\\\ &amp; &amp; \\ddots &amp; \\vdots \\\\ &amp; &amp; &amp; 1 \\end{bmatrix} \\] Then the \\(LU\\) factorization becomes \\(A = LDU\\), where \\(L\\) and \\(U\\) have ones on its diagonal and \\(D\\) is the diagonal matrix of pivots. In cases where row exchanges are needed to produce the echelon form, we can express the elimination process in terms of a permutation matrix \\(P\\) and the other set of row addition operations, defined by \\(L_1, ..., L_{k}\\) \\[ PL_{k}\\cdots L_1A = U \\] Note that permutation matrix \\(P\\) satisfies \\(PP^T = I\\). Multiplying both sides with \\(P^T\\) and the inverses of \\(L_1, ..., L_{k}\\), we obtain \\[ A = \\underbrace{ L_1^{-1} \\cdots L_k^{-1}}_{L}P^TU \\] One can try to polish this form by obtain a decomposition in which the permutation matrix occurs before the lower triangular matrix \\(L\\) (\\(L\\) and \\(P\\) are not the same after reordering) \\[ A = P^TLU \\] It’s also common to write this decomposition as \\(PA = LU\\). 1.4 Determinants In the \\(2 \\times 2\\) case, the determinant of \\(A\\) is simply \\[ \\begin{vmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{vmatrix} = ad - bc \\] When \\(A\\) is \\(3 \\times 3\\), we still get a not-too-messy explicit formula to compute its determinant \\[ \\begin{vmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{vmatrix} = a_{11}a_{22} a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21} a_{32} - a_{13}a_{22}a_{31} - a_{12}a_{21} a_{33} - a_{11}a_{23}a_{32} \\] This is the sum of products on the foward diagonal minus the sum of products on the backward diagonal. 1.4.1 Cofactor Expansion Cofactor expansion is a more practical way to compute determinant of bigger matrices. The \\((i, j)\\text{-cofactor}\\) of \\(A\\) is a number \\(C_{ij}\\) in \\(\\mathbb{R}\\) given by \\[ C_{ij} = (-1)^{i + j} \\det A_{ij} \\] where \\(A_{ij}\\) denotes the submatrix formed by deleting the \\(i\\)h row and \\(j\\)th column of \\(A\\). Theorem 1.5 (cofactor expansion) The determinant of an \\(n \\times n\\) matrix is given by a cofactor expasion across any row or column. For example, the expansion across the \\(i\\)th row is: \\[ \\det A = a_{i1}C_{i1} + a_{i2}C_{i2} + \\cdots + a_{in}C_{in} \\] and cross the \\(j\\)th column is \\[ \\det A = a_{1j}C_{1j} + a_{2j}C_{2j} + \\cdots + a_{nj}C_{nj} \\] 1.4.2 Geometric Interpretation of Determinant Given matrix \\(A_{n \\times n}\\) \\[ \\begin{bmatrix} a_1^{T} \\\\ a_2^{T} \\\\ \\vdots \\\\ a_n^{T} \\end{bmatrix} \\] where \\(a_1, ..., a_n\\) are row vectors of A. Then \\(|\\det A|\\) is the volume of parallelotope constrained by \\(a_1, ..., a_n\\). When \\(A\\) is \\(2\\times2\\), \\(|\\det A|\\) is simply the area of the parallelogram defined by two side \\(a_1, a_2\\) 1.4.3 Properties of Determinant A list of arithmetic properties of determinants, A is an \\(n\\times n\\) matrix: \\(\\det(A^T) = \\det(A)\\) \\(\\det(kA) = k^n \\det(A)\\) \\(\\det(AB) = \\det(A)\\det(B)\\) (although \\(AB \\not = BA\\) in general), it follows that \\(\\det(A^n) = \\det(A)^n\\) \\(\\det(A^{-1}) = 1 / \\det(A)\\), if \\(A\\) is invertible determinant is equal to the product of eigenvalues (counting multiplicity) \\(\\det(A) = \\prod_{i=1}^n{\\lambda_i}\\) If the \\(i\\)-th row (column) in A is a sum of the \\(i\\)-th row (column) of a matrix \\(B\\) and the \\(i\\)-th row (column) of a matrix \\(C\\) and all other rows in \\(B\\) and \\(C\\) are equal to the corresponding rows in \\(A\\) (that is, \\(B\\) and \\(C\\) differ in one row only), then \\(\\det(A)=\\det(B)+\\det(C)\\). This can be proven by cofactor expansion across the \\(i\\)th row. The same applies to columns. Row operations on \\(A\\) has the following effect on \\(\\det A\\) if we multiply a single row in \\(A\\) by a scalar \\(k \\in \\mathbb{R}\\), then the determinant of the new matrix is \\(k\\det A\\) if we exchange two rows \\(a_i^T\\) and \\(a_j^T\\) of \\(A\\), determinant becomes \\(-\\det A\\) Add a multiple of one row to another row has no effect on determinant Note that all row operations don’t change whether or not a determinant is 0, only change it by a non-zero factor or change its sign. The first two effects can be easily understood in connection with geometric meaning of determinant. What third property means is illustrated by the following example. Represent \\(B\\) and \\(C\\) with row vectors \\[ B = \\begin{bmatrix} a_1^T \\\\ \\vdots \\\\ a_i^T \\\\ \\vdots \\\\ a_j^T \\\\ \\vdots \\\\ a_n^T \\end{bmatrix} , C = \\begin{bmatrix} a_1^T \\\\ \\vdots \\\\ ka_j^T \\\\ \\vdots \\\\ a_j^T \\\\ \\vdots \\\\ a_n^T \\end{bmatrix} \\] And define \\(A\\) as \\[ A = \\begin{bmatrix} a_1^T \\\\ \\vdots \\\\ a_i^T + ka_j^T \\\\ \\vdots \\\\ a_j^T \\\\ \\vdots \\\\ a_n^T \\end{bmatrix} \\] It is obvious that here \\(A\\) is on purpose the result of \\(B\\) after performing row addition (add a multiple of the \\(j\\)th row to the \\(i\\)th row). And by property 6 \\(\\det(A) = \\det(B) + \\det(C)\\). Also in this case, \\(C\\) has determinant \\(0\\), and this property indeed proves that row addition does not change determinant. In more general cases, \\(|C|\\) definitely does not have to be zero. 1.4.4 Cramer’s Rule Given an \\(n \\times n\\) matrix \\(A\\) and \\(\\bar{b}\\) in \\(\\mathbb{\\mathbb{R^n}}\\), denote \\(A_i(\\bar{b})\\) as the matrix derived by \\(A\\) by replacing column \\(i\\) by vector \\(\\bar{b}\\): \\[ A_i(\\bar{b}) = [\\bar{a}_1 \\cdots \\underbrace{\\bar{b}}_{\\text{column} \\,i} \\cdots \\bar{a}_n] \\] Theorem 1.6 (Cramer’s rule) Let \\(A\\) be an invertible \\(n \\times n\\) matrix. For any \\(\\bar{b}\\) in \\(\\mathbb{R^n}\\), the unique solution \\(\\bar{x}\\) of \\(A\\bar{x} = \\bar{b}\\) has entries given by: \\[ x_i = \\frac{\\det A_i(\\bar{b})}{\\det A} \\] 1.5 Trace The trace of square matrix \\(A\\) is the sum of its diagonal entries \\(\\sum_{i = 1}^{n}A_{ii}\\). The trace has the following properties: \\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\) \\(\\text{tr}(kA) = k\\,\\text{tr}(A)\\), \\(k\\) is a scalar \\(\\text{tr}(A^T) = \\text{tr}(A)\\) For \\(A\\), \\(B\\) such that \\(AB\\) is square, \\(\\text{tr}(AB) = \\text{tr}(BA)\\) Trace of product of multiple matrices is invariant to cyclic permutations, \\(\\text{tr}(ABC) = \\text{tr}(BCA) = \\text{tr}(CAB)\\). Note that the reordering cannot be done arbitrarily, fro example \\(\\text{tr}(ABC) \\not= \\text{tr}(ACG)\\) in general. Trace is equal to the sum of its eigenvalues (repeated according to multiplicity) \\(\\text{tr}(A) = \\sum_{i = 1}^n{\\lambda_i}\\) \\(\\text{tr}(\\bar{a}\\bar{a}^T) = \\bar{a}^T\\bar{a}\\) 1.6 Matrix Inversion Note that the inverse of a matrix is only defined for square matrces, so is determinants in Section 1.4. In practice \\(A^{-1}\\) is seldom computed, because computing both \\(A^{-1}\\) and \\(A^{-1}\\bar{b}\\) to solve linear equations takes about 3 times as many arithmetic operations as solving \\(A\\bar{x} = \\bar{b}\\) by row reduction. Assume that \\(A\\) and \\(B\\) are both non-singular Theorem 1.7 If A and B are both invertible matrces, we have \\[ (A^{-1})^{-1} = A \\] \\[ (AB)^{-1} = B^{-1}A^{-1} \\] \\[ (A^T)^{-1} = (A^{-1})^T \\] In Section 1.4, we derive an algorithm of finding inverse matrices by row reductions on the augmented matrix \\([A \\; | \\; I]\\). However, Cramer’s rule 1.6 leads to a general formula of calculating \\(A^{-1}\\), if it exists. The \\(j\\)th column of \\(A^{-1}\\) is a vector \\(\\bar{x}\\) that satisfies: \\[ A\\bar{x} = \\bar{e}_j \\] By Cramer’s rule \\[ \\{(i,j) \\text{ entry of } A^{-1}\\} = x_i = \\frac{\\det A_i{(\\bar{e}_j)}}{\\det A} \\] A cofactor expansion 1.5 down column \\(i\\) of \\(A_i{(\\bar{e}_j)}\\) shows that: \\[ \\det A_i{(\\bar{e}_j)} = (-1)^{i + j}\\det A_{ji} = C_{ji} \\] where \\(C_{ji}\\) is a cofactor of \\(A\\). Note that the (\\(i\\), \\(j\\))th entry of \\(A^{-1}\\) is the cofactor \\(C_{ji}\\) divided by \\(\\det A\\) (the subscript is reversed). Thus \\[\\begin{equation} \\tag{1.1} A^{-1} = \\frac{1}{\\det A} \\begin{bmatrix} C_{11} &amp; C_{21} &amp; \\cdots &amp; C_{n1} \\\\ C_{12} &amp; C_{22} &amp; \\cdots &amp; C_{n2} \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ C_{1n} &amp; C_{2n} &amp; \\cdots &amp; C_{nn} \\\\ \\end{bmatrix} \\end{equation}\\] The right side of Eq (1.1) is called the adjugate of \\(A\\), often denoted by \\(\\text{adj}\\, A\\). Theorem 1.8 (An Inverse Formula) Let \\(A\\) be an invertible \\(n \\times n\\) matrix. Then \\[ A^{-1} = \\frac{1}{\\det A}\\text{adj}\\, A \\] Interestingly, theorem 4.3 in later chapters shows that for invertible matrix \\(A\\), its inverse \\(A^{-1}\\) can be represented as a polynomial of \\(A\\). 1.6.1 The Matrix Inversion Lemma Lemma 1.1 (The Matrix Inversion Lemma) Let \\(A\\) be an invertible \\(n \\times n\\) square matrix, and \\(\\bar{v}\\) and \\(\\bar{u}\\) be d-dimensional vectors. Then, \\(A + \\bar{u}\\bar{v}^T\\) is invertible if and only if \\(\\bar{v}^TA\\bar{u} \\not = -1\\). In such a case, the inverse formula is given by \\[ (A + \\bar{u}\\bar{v}^T) = A^{-1} - \\frac{A^{-1}\\bar{u}\\bar{v}^TA^{-1}}{1 + \\bar{v}^TA^{-1}\\bar{u}} \\] Theorem 1.9 (Woodbury Identity) Let \\(A\\) be an invertible \\(n \\times n\\) square matrix, and \\(U, V\\) nonzero \\(n \\times k\\) matrices for some small values of \\(k\\). Then, \\(A + UV^T\\) is invertible if and only if the \\(k \\times k\\) amtrix \\((I +V^TA^{-1}U)\\) is invertible. In such a case, the inverse formula is given by \\[ (A + UV)^{-1} = A^{-1} - A^{-1}U(I + V^TA^{-1}U)^{-1}V^TA^{-1} \\] It’s easy to find that the Woodbury Identity is an extension of the matrix inversion lemma, where \\(\\bar{u} \\rightarrow U\\) and \\(\\bar{v} \\rightarrow V\\). Proposition 1.1 For square matrix \\(P\\) (not assuming invertible), if \\(I + P\\) is invertible, then \\((I + P)^{-1}\\) satisfies: \\[ (I + P)^{-1} = I - (I + P)^{-1}P \\] As a quick check, right and left multiply \\((I + P)\\) to see if we get an identity. \\[ \\begin{split} (I + P)^{-1}(I + P) &amp;= I \\\\ &amp;= \\Big(I - (I + P)^{-1}P \\Big )(I + P) \\\\ &amp;= I + P -(1 + P)^{-1}P(I + P) \\\\ &amp;= (I + P)( I - (I + P)^{-1}P) \\\\ &amp;= (I + P)(I + P)^{-1}\\\\ &amp;= I \\end{split} \\] and left multiply \\[ \\begin{split} (I + P)(I + P)^{-1} &amp;= I \\\\ &amp;= (I + P)(I - (I + P)^{-1}P) \\\\ &amp;= I + P - P\\\\ &amp;= I \\end{split} \\] Proposition 1.2 (Push Through Identity) Let \\(U\\) and \\(V\\) be \\(m \\times n\\) matrices, we have \\[ U^T(I + VU^T)^{-1} = (I + U^TV)^{-1}U^T \\] The above result shows the following for any \\(m \\times n\\) matrix \\(A\\) and any scalar \\(\\lambda &gt; 0\\) \\[ A^T(\\lambda I + AA^T)^{-1} = (\\lambda I + A^TA)^{-1}A^T \\] 1.7 Complexity of Matrix Computation This chapter concludes with a summary of complexity (number of flops needed) in common matrix computations. An \\(m \\times n\\) matrix is often represented by an \\(m \\times n\\) array of floating numbers in a computer, requiring \\(8mn\\) bytes (8 bytes for one element). In the case of a sparse matrix, we can store only the nonzero entries with row index \\(i\\) (integer), column index \\(j\\) (integer) and its value \\(A_{ij}\\) (floating point). Let \\(\\text{nnz}(A)\\) be the number of nonzero entries, since we only need 4 bytes to store a integer, storing a sparse matrix requires roughly \\(16\\text{nnz}(A)\\) bytes. Let \\(A\\) be an \\(m \\times n\\) matrix, and other matrices / vectors be of conformable size for arithmetic operations. The complexity (approximate number of flops) of common matrix operations are listed below Operation Expression Complexity Explanation matrix addition \\(A + B\\) \\(mn\\) and \\(\\min(\\text{nnz}(A), \\text{nnz}(B))\\) if one of \\(A\\) and \\(B\\) is sparse For any entry \\(i,j\\) for which one of \\(A_{ij}\\) and \\(B_{ij}\\) is zero, no arithmetic operations are needed to find \\((A + B)_{ij}\\) scalar multiplication \\(kA\\) \\(mn\\) \\(\\text{nnz}(A)\\) when \\(A\\) is sparse matrix-vector multiplication \\(A\\bar{x}\\) \\(m(2n - 1)\\) equivalent to \\(m\\) inner products between rows of \\(A\\) and \\(\\bar{x}\\) matrix-vector multiplication when \\(A\\) is sparse between \\(\\text{nnz}(A)\\) and \\(2\\text{nnz}(A) - 1\\) \\(A\\bar{x}\\) First we need \\(\\text{nnz}(A)\\) multiplications, and certain number of additions. We need most additions when nonzero entries are arranged next to each other, and least when they are separated in different columns and rows (for example for diagonal matrix \\(A\\), we need no additions at all) matrix-matrix multiplication \\(A_{m \\times n}B_{n \\times p}\\) \\(2mnp\\) \\(2n - 1\\) for each inner product between rows of \\(A\\) and columns of \\(B\\), with \\(m \\times p\\) inner products in total, which is roughly \\(2mnp\\) flops sparse matrix multiplication \\(AB\\) no more than \\(2\\min\\{\\text{nnz}(A)p, \\text{nnz}(B)m\\}\\) Suppose \\(A\\) is sparse \\(m \\times n\\), and \\(B\\) is normal \\(n \\times p\\). Then the inner product of the \\(i\\)th row \\(\\bar{a}_i^T\\) and the \\(j\\)th column of \\(B\\) requires no more than \\(2\\text{nnz}(\\bar{a}_i^T)\\) flops. That adds up to \\(2\\text{nnz}(A)p\\) in total. If \\(B\\) is sparse then the number \\(2\\text{nnz}(B)m\\) flops. In cases they are both spares, the required flops will be no more than the minimum of these two quantities transposition \\(A^T\\) \\(0\\) Computing \\(A^T\\) needs only copying, but no flops Convolution \\(a * b = T(a)b = T(b)a\\) \\(2mn\\) One interesting discovery in the complexity of matrix product is that order matters. Different order of computing the same expression can result in different complexity. We first consider the vector product \\(ab^Tc\\), where they are all \\(n\\)-th vectors. If we first evaluate \\(ab^T\\), the cost is \\(n^2\\) flops, and the cost of matrix-vector product \\((ab^T)c\\) is \\(2n^2\\) flops. The total cost is \\(3n^2\\) flops. If we first evaluate \\(b^Tc\\) and then multiply by \\(a\\), the total cost is \\(3n\\) flops. Also, the first order requires storing an intermediate \\(n \\times n\\) matrix, while the latter only stores a scalar. Next, consider the product of three matrices \\[ D = A_{m \\times n}B_{n \\times p}C_{p \\times q} \\] The first method is \\((AB)C\\). The complexity of \\(AB\\) and \\((AB)C\\) are \\(2mnp\\) and \\(2mpq\\). for a total of \\(2mp(n + q)\\) flops. The second method is \\(A(BC)\\). We first compute product \\(BC\\) (\\(2npq\\) flops) and then form \\(D = A(BC)\\), for a total of \\(2nq(m + p)\\) flops. The two complexity are equal when \\[ \\frac{1}{n} + \\frac{1}{q} = \\frac{1}{m} + \\frac{1}{p} \\] When \\(m = p\\) and \\(n = q\\): \\[ \\begin{aligned} \\text{first method}&amp;: \\mathcal{O}(m^2n) \\\\ \\text{second method}&amp;: \\mathcal{O}(mn^2) \\end{aligned} \\] Therefore the complexity can vary dramatically according to the order of computing, when there is a great difference between \\(m\\) and \\(n\\). In particular, a non-singular matrix with \\(a_{11} = 0\\) cannot have LU decomposition↩︎ square matrices whose diagonal entries are those of the original matrix↩︎ "],
["vector-spaces.html", "Chapter 2 Vector Spaces 2.1 Vector Space 2.2 Metric Spaces, Normed Spaces, Inner Product Spaces 2.3 Subspaces 2.4 Fundamental Theorem of Linear Algebra 2.5 Rank 2.6 Bases and Coordinate Systems 2.7 Complexity of Vector Computations", " Chapter 2 Vector Spaces Vector spaces, metric spaces, normed spaces, and inner product spaces are places where computations in linear algebra happen. These spaces are defined more or less to generalize properties of Euclidean space. 2.1 Vector Space A vector space \\(V\\) is a nonempty set, also called linear spaces, the elements of which are called vectors,. A vector space comes with two operations predefined: vectors can be added together, and vectors can be multiplied by real numbers called scalars. For all vectors \\(\\bar{u}, \\bar{v}\\) and \\(\\bar{w}\\), and all scalars \\(c\\) and \\(d\\) in \\(V\\), the following axioms of vector space must hold: The sum of \\(\\bar{u}\\) and \\(\\bar{v}\\) is in \\(V\\) The scalar multiple of \\(\\bar{u}\\) by c, denoted by \\(c\\bar{u}\\), is in \\(V\\) There exists additive identity (denoted by \\(\\bar{0}\\)) such that \\(\\bar{x} + \\bar{0} = \\bar{x}\\). Similarly, multiplicative identity (written \\(\\bar{1}\\)) means \\(\\bar{1}\\bar{x} = \\bar{x}\\) There exists an additive inverse (written \\(-\\bar{x}\\)) such that \\(-\\bar{x} + \\bar{x} = \\bar{0}\\) Communitivity: \\(\\bar{x} + \\bar{y} = \\bar{y} + \\bar{x}\\) Associativity: \\((\\bar{x} + \\bar{y}) + \\bar{z} = \\bar{x} + (\\bar{y} + \\bar{z})\\), and \\(\\alpha(\\beta\\bar{x}) = (\\alpha\\beta)\\bar{x}\\) Distributivity: \\(\\alpha(\\bar{x} + \\bar{y}) = \\alpha\\bar{x} + \\alpha\\bar{y}\\) and \\((\\alpha + \\beta)\\bar{x} = \\alpha\\bar{x} + \\beta\\bar{x}\\) A set of vectors \\(\\bar{v}_1, ..., \\bar{v}_n \\in V\\) are set to be linearly independent if the following equation has only a traivial solution \\[ \\begin{align*} c_1\\bar{v}_1 + \\cdots + c_n\\bar{v}_n &amp;= 0 &amp;&amp; \\text{implies } c_1 = \\cdots = c_n = 0 \\end{align*} \\] The span of \\(\\bar{v}_1, ..., \\bar{v}_n\\) is the set of all vectors that can be expressed as a linear combination of them. 2.1.1 Euclidean Space Euclidean space is the quintessential vector space, denoted by \\(\\mathbb{R}^n\\). The two must-have operations of vector spaces are valid in \\(\\mathbb{R}^n\\) \\[ \\bar{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}, \\;\\; \\bar{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, \\;\\; \\alpha \\in \\mathbb{R} \\\\ \\bar{x} + \\bar{y} = \\begin{bmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{bmatrix}, \\;\\; \\alpha\\bar{x} = \\begin{bmatrix} \\alpha x_1 \\\\ \\vdots \\\\ \\alpha x_n \\end{bmatrix} \\] Euclidean spaces have other structures defined in addition to the plainest vector space. We can calculate dot product, length, distance by \\[ \\begin{aligned} \\text{dot product between } \\bar{x} \\text{ and } \\bar{y}: \\bar{x} \\cdot\\bar{y} &amp;= \\sum_{i=1}^{n}{x_iy_i} \\\\ \\text{length of } \\bar{x}: \\|\\bar{x}\\| &amp;= \\sqrt{\\bar{x} \\cdot \\bar{x}} \\\\ \\text{distance between } \\bar{x} \\text{ and } \\bar{y}: \\text{dist}(\\bar{x}, \\bar{y}) &amp;= \\|\\bar{x} - \\bar{y}\\| \\end{aligned} \\] 2.2 Metric Spaces, Normed Spaces, Inner Product Spaces 2.2.1 Metric and Norm Metric spaces, normed spaces and inner product spaces capture important properties of Euclidean space in a more general way (distance, length, angle). Although metric spaces are not required to be vector spaces, it is always assumed in linear algebra that this is the case. For this reason, metric spaces are short for “metric linear space”. Normed spaces and inner product spaces are defined to be extensions of metric linear spaces, so that they must be vector spaces. Metrics generalize the notion of distance from Euclidean space. A metric space is a set together with a metric on the set (metric spaces don’t have to be vector spaces). The metric is a function that defines a concept of distance \\(\\in \\mathbb{R}\\) between any two members of the set. A metric must satisfies the following properties: \\(d(\\bar{x}, \\bar{y}) \\ge 0\\), with equality if and only if \\(\\bar{x} = \\bar{y}\\). Distances are non-negative, and the only point at zero distance from \\(x\\) is \\(x\\) itself \\(d(\\bar{x}, \\bar{y}) = d(\\bar{y}, \\bar{x})\\). The distance is a symmetric function. \\(d(\\bar{x}, \\bar{z}) \\le d(\\bar{x}, \\bar{y}) + d(\\bar{y}, \\bar{z})\\). Distance satisfies triangular inequality. Norms generalize the notion of length from Euclidean space. A norm on a real vector space \\(X\\) is a function: \\(\\|\\cdot\\|: V \\rightarrow \\mathbb{R}\\) that satisfies: \\(\\|\\bar{x}\\| \\ge 0\\) for all \\(\\bar{x} \\in X\\), with equality if and only if \\(\\bar{x} = \\bar{0}\\) (nonnegative) \\(\\|\\lambda \\bar{x}\\| = \\lambda \\|\\bar{x}\\|\\), for all \\(\\bar{x} \\in X\\) and \\(\\lambda \\in \\mathbb{R}\\) (homogeneous) \\(\\|\\bar{x} + \\bar{y}\\| \\le \\|\\bar{x}\\| + \\|\\bar{y}\\|\\) (triangular inequality) A normed space is a metric space with the metric \\[ d(\\bar{x}, \\bar{y}) = \\|\\bar{x} - \\bar{y}\\| \\] So a normed space is a special case of metric spaces, a metric spcae may not necessarily has a norm associated with it. One can verify that \\(d(\\bar{x}, \\bar{y}) = \\|\\bar{x} - \\bar{y}\\|\\) satisfies all properties of a metric. The most common function for norms on \\(\\mathbb{R}^n\\) are listed below, with \\(\\bar{x} = [x_1, x_2, ..., x_n]\\). \\[ \\begin{align*} \\text{1-norm}: \\|\\bar{x}\\|_1 &amp;= \\sum_{i=1}^{n}{|x_i|}\\\\ \\text{2-norm}: \\|\\bar{x}\\|_2 &amp;= \\sqrt{\\sum_{i=1}^{n}{x_i}^2} \\\\ \\text{p-norm}: \\|\\bar{x}\\|_p &amp;= (\\sum_{i=1}^{n}{|x_i|}^p)^{\\frac{1}{p}} \\quad (p \\ge 1) \\\\ \\text{maximum norm}: \\|\\bar{x}\\|_{\\infty} &amp;= \\max\\{|x_1|, |x_2|, ..., |x_n|\\} \\end{align*} \\] 1-norm is also called the Manhattan norm. 2-norm is the Euclidean norm, the subscript \\(2\\) can be left out in \\(\\mathbb{R}^n\\). p-norm is a generalization of 1-norm and 2-norm, requiring \\(p &gt; 1\\). When \\(p\\) turns infinity, \\(\\|\\bar{x}\\|_{\\infty}\\) is called the maximum norm. The VMLS book (Boyd and Vandenberghe 2018) introduces some other topics related to norm. Root-mean-square value. The root-mean-square (RMS) value of an \\(n\\)-vector \\(\\bar{x}\\) is defined as \\[ \\boldsymbol{\\text{rms}}(x) = \\sqrt{\\frac{x_1^2 + \\cdots + x_n^2}{n}} = \\frac{\\| \\bar{x}\\|}{\\sqrt{n}} \\] RMS is useful when comparing norms of vectors with different dimensions, because the value is independent of \\(n\\), and it tells what a “typical” value of \\(|x_i|\\) is. The norm of a vector with all entries being the same value of \\(\\alpha\\) will be \\(\\sqrt{n}|\\alpha|\\), yet its rms value will be \\(|\\alpha|\\). Norm of a sum. The norm of the sum of two vectors \\(\\bar{x}\\) and \\(\\bar{y}\\) is \\[ \\|\\bar{x} + \\bar{y}\\| = \\sqrt{\\|\\bar{x}\\|^2 + 2\\bar{x}^T\\bar{y} + \\|\\bar{y} \\|^2} \\] This can be proven by expanding \\((\\bar{x} + \\bar{y})^T(\\bar{x} + \\bar{y})\\) Norm of block vectors. The norm-squared of a stacked vector is the sum of norm-square values of its subvectors. For example, if we stack 3 vectors \\(\\bar{a}, \\bar{b}, \\bar{c}\\) to form a longer vector \\(\\bar{d}\\) \\[ \\bar{d} = \\begin{bmatrix} \\bar{a} \\\\ \\bar{b} \\\\ \\bar{c} \\end{bmatrix} \\] We have \\[ \\| d\\|^2 = \\bar{a}^T\\bar{a} + \\bar{b}^T\\bar{b} + \\bar{c}^T\\bar{c} = \\|\\bar{a}\\|^2 + \\|\\bar{b}\\|^2 + \\|\\bar{c}\\|^2 \\] Another way of writing this is \\[ \\|(\\bar{a}, \\bar{b}, \\bar{c})\\| = \\sqrt{ \\|\\bar{a}\\|^2 + \\|\\bar{b}\\|^2 + \\|\\bar{c}\\|^2} = \\|(\\|\\bar{a} \\|, \\|\\bar{b} \\|, \\|\\bar{c}\\|)\\| \\] Chebyshev inequality \\[ \\frac{\\|\\bar{x}\\|^2}{k} \\ge a^2 \\] Note that \\(\\text{rms}(\\bar{x}) = \\|\\bar{x}\\| / \\sqrt{n}\\), so that \\(\\|\\bar{x}\\|^2 = n \\, \\text{rms}(\\bar{x})^2\\). Then we have \\[ \\Big(\\frac{\\text{rms}(\\bar{x})}{a}\\Big)^2 \\ge \\frac{k}{n} \\] Average, RMS value, and standard deviation. The mean and standard deviation of vector \\(\\bar{x}\\) can be written in the form \\[ \\begin{aligned} \\mu &amp;= \\frac{1}{n}\\bar{1}^T\\bar{x} \\\\ \\sigma &amp;= \\frac{\\|\\bar{x} - \\mu\\bar{1}\\| }{\\sqrt{n}} \\end{aligned} \\] Then we have the following relationship \\[ \\text{rms}(\\bar{x})^2 = \\mu^2 + \\sigma^2 \\] Proof \\[ \\begin{split} \\sigma^2 &amp; = \\frac{1}{n} \\|\\bar{x} - \\frac{1}{n}\\bar{1}^T\\bar{x}\\bar{1}\\|^2 \\\\ &amp;= \\frac{1}{n}[\\bar{x}^T\\bar{x} + \\Big((\\frac{1}{n}\\bar{1}^T\\bar{x})\\bar{1}\\Big)^T\\Big((\\frac{1}{n}\\bar{1}^T\\bar{x})\\bar{1}\\Big) - 2\\bar{x}^T(\\frac{1}{n}\\bar{1}^T\\bar{x})\\bar{1}] \\qquad \\mbox{norm of sum} \\\\ &amp;= \\frac{1}{n}[\\bar{x}^T\\bar{x} + \\frac{1}{n}(\\bar{1}^T\\bar{x})^2 - \\frac{2}{n}(\\bar{1}^T\\bar{x})^2] \\\\ &amp;= \\frac{1}{n}\\bar{x}^T\\bar{x} - \\frac{1}{n^2}(\\bar{1}^T\\bar{x})^2 \\\\ &amp;= \\text{rms}(\\bar{x}) - \\mu^2 \\end{split} \\] 2.2.2 Inner Produc, Outer Product, Cross Product An inner product on a real vector space \\(X\\) is a function \\(\\langle \\cdot, \\cdot\\rangle: X \\times X \\rightarrow \\mathbb{R}\\) satisfying \\(\\langle \\bar{x}, \\bar{y} \\rangle \\ge 0\\), with equality if and only if \\(x = \\bar{0}\\) \\(\\langle \\bar{x}, \\bar{y} \\rangle = \\langle \\bar{y}, \\bar{x} \\rangle\\) \\(\\langle \\bar{x} + \\bar{y}, \\bar{z}\\rangle = \\langle \\bar{x}, \\bar{z}\\rangle + \\langle \\bar{y}, \\bar{z}\\rangle\\) and \\(\\langle \\lambda \\bar{x}, \\bar{y}\\rangle = \\lambda \\langle \\bar{x}, \\bar{y} \\rangle\\) A vector space equipped with such inner product is called a inner product space. Note that all inner product spaces are normed spaces, because a inner product induce a norm on a vector space: \\[ \\langle \\bar{x}, \\bar{x} \\rangle = \\|\\bar{x}\\|^2 \\] The standard inner product defined on \\(\\mathbb{R}^{n}\\) is the dot product, given by \\[ \\langle \\bar{x}, \\bar{y} \\rangle = \\sum_{i=1}^{n}{x_iy_i} = \\bar{x}^T\\bar{y} \\] The abstract spaces—metric spaces, normed spaces, and inner product spaces—are all examples of what are more generally called “topological spaces” (linear topological space if they are assumed to be vector spaces first). These spaces have been given in order of increasing structure. That is, every inner product space is a normed space, and in turn, every normed space is a metric space. The outer product of an \\(m\\) vector \\(\\bar{a}\\) and an \\(\\bar{b}\\) is given by \\(\\bar{a}\\bar{b}^T\\), which is an rank 1 \\(m \\times n\\) matrix \\[ \\bar{a}\\bar{b}^T = \\begin{bmatrix} a_1b_1 &amp; a_1b_2 &amp; \\cdots &amp; a_1b_n \\\\ a_2b_1 &amp; a_2b_2 &amp; \\cdots &amp; a_2b_n \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ a_nb_1 &amp; a_nb_2 &amp; \\cdots &amp; a_nb_n \\end{bmatrix} \\] The outer product is not symmetric, i.e. (\\(\\bar{a}\\bar{b}^T \\not = \\bar{b}\\bar{a}^T\\)) If we express an \\(m \\times n\\) matrix \\(A\\) in terms of its columns \\(a_1, ..., a_n\\) and the \\(n \\times p\\) matrix \\(B\\) in terms of its rows \\(b_1, ..., b_p\\) \\[ A = \\begin{bmatrix} a_1 &amp; \\cdots &amp; a_n \\end{bmatrix}, \\quad B = \\begin{bmatrix} b_1^T \\\\ \\vdots \\\\ b_n^T \\end{bmatrix} \\] The the matrix multiplication \\(AB\\) can be expressed as the sum of outer products between \\(a_i\\) and \\(b_i\\) \\[ AB = a_1b_1^T + \\cdots + a_nb_n^T \\] This echoes theorem 1.1, where matrix multiplication is computed via column row expansion. Next, we bring in the concept of cross product which may not be so significant in the linear algebra section, but it will shine in multivariate integral calculus. The cross product between \\(\\bar{u}\\) and \\(\\bar{v}\\) is a vector perpendicular to \\(\\bar{u}\\), \\(\\bar{v}\\) and the plane formed by them, with length equal to \\[ \\|\\bar{u} \\times \\bar{v} \\| = \\|\\bar{u}\\| \\|\\bar{v}\\| \\sin \\theta \\] Let \\(\\bar{n}\\) denote the unit vector perpendicular to the plane containing \\(\\bar{u}\\) and \\(\\bar{v}\\), the cross product can be written as \\[ \\bar{u} \\times \\bar{v} = \\|\\bar{u}\\| \\|\\bar{v}\\| \\sin (\\theta)\\bar{n} \\] Though this formula is rarely helpful we because the main usage of cross product is finding the direction \\(\\bar{n}\\). In most cases, we only consider cross product in 3D spaces, although it can be generalized to higher dimensions. Since the sines of \\(0\\) and \\(\\pi\\), it makes sense that the cross product of two parallel vectors is zero. The cross product obeys the following laws \\(r\\bar{u} \\times s\\bar{v} = rs (\\bar{u} \\times \\bar{v})\\) order matters: \\(\\bar{u} \\times \\bar{v} = -\\bar{v} \\times \\bar{u}\\) Distributive: \\(\\bar{u} \\times (\\bar{v} + \\bar{w})= \\bar{u} \\times \\bar{v} + \\bar{u} \\times \\bar{w}\\) Not associative: \\(\\bar{u} \\times (\\bar{v} \\times \\bar{w}) = (\\bar{u} \\cdot \\bar{w})\\bar{v} - (\\bar{u} \\cdot \\bar{v})\\bar{w}\\) To get a general formula of cross product, we consider the cross product among 3 standard basis \\(\\boldsymbol{i} = [1, 0, 0], \\boldsymbol{j} = [0, 1, 0], \\boldsymbol{k} = [0, 0, 1]\\) Figure 2.1: Figure 12.30 FROM Thomas Calculus, 14th edition \\[ \\begin{aligned} \\boldsymbol{i} \\times \\boldsymbol{j} &amp;= \\boldsymbol{k} \\\\ \\boldsymbol{j} \\times \\boldsymbol{k} &amp;= \\boldsymbol{i} \\\\ \\boldsymbol{k} \\times \\boldsymbol{i} &amp;= \\boldsymbol{j} \\end{aligned} \\] and \\[ \\boldsymbol{i} \\times \\boldsymbol{i} = \\boldsymbol{j} \\times \\boldsymbol{j} = \\boldsymbol{k} \\times \\boldsymbol{k} = 0 \\] Then the cross product between \\(\\bar{u}\\) and \\(\\bar{v}\\) can be written as \\[ \\begin{split} \\bar{u} \\times \\bar{v} &amp;= (u_1\\boldsymbol{i} + u_2\\boldsymbol{j} + u_3\\boldsymbol{k}) \\times (v_1\\boldsymbol{i} + v_2\\boldsymbol{j} + v_3\\boldsymbol{k}) \\\\ &amp;= (u_1v_1)\\boldsymbol{i} \\times \\boldsymbol{i} + (u_1v_2)\\boldsymbol{i} \\times \\boldsymbol{j} + (u_1v_3) \\boldsymbol{i} \\times \\boldsymbol{k} \\\\ &amp;{\\quad} + (u_2v_1)\\boldsymbol{j} \\times \\boldsymbol{i} + (u_2v_2)\\boldsymbol{j} \\times \\boldsymbol{j} + (u_2v_3)\\boldsymbol{j} \\times \\boldsymbol{k} \\\\ &amp;{\\quad} + (u_3v_1)\\boldsymbol{k} \\times \\boldsymbol{i} + (u_3v_2)\\boldsymbol{k} \\times \\boldsymbol{j} + (u_3v_3)\\boldsymbol{k} \\times \\boldsymbol{k} \\\\ &amp;= (u_2v_3 - u_3v_2)\\boldsymbol{i} - (u_1v_3 - u_3v_1)\\boldsymbol{j} + (u_1v_2 - u_2v_1)\\boldsymbol{k} \\\\ &amp;= \\begin{bmatrix} u_2v_3 - u_3v_2 \\\\ u_1v_3 - u_3v_1 \\\\ u_1v_2 - u_2v_1 \\end{bmatrix} \\end{split} \\] This result is same as the determinant of the following matrix (think of \\(\\boldsymbol{i}, \\boldsymbol{j}, \\boldsymbol{k}\\)) as scalars during the computation) \\[ \\bar{u} \\times \\bar{v} = \\begin{vmatrix} \\boldsymbol{i} &amp; \\boldsymbol{j} &amp; \\boldsymbol{k} \\\\ u_1 &amp; u_2 &amp; u_3 \\\\ v_1 &amp; v_2 &amp; v_3 \\end{vmatrix} \\] The main use for cross product will be finding a vector which is perpendicular to two given vectors. We can make use of the magnitude as well. Recall the magnitude of the cross product \\[ \\| \\bar{u} \\times \\bar{v} \\| = \\|\\bar{u}\\| \\|v\\|\\sin\\theta \\] This is the area of the following parallelogram defined by \\(\\bar{u}\\) and \\(\\bar{v}\\) Figure 2.2: Figure 12.31 FROM Thomas Calculus, 14th edition So let’s review what the dot product gives us: its a vector perpendicular to the plane containing \\(\\bar{u}\\) and \\(\\bar{v}\\), the length of that vector is equal to the area of the parallelogram formed by \\(\\bar{u}\\) and \\(\\bar{v}\\). As one application of the cross product, we have the following theorem. Theorem 2.1 (Volume of a Parallelepiped) The volume of a parallelepiped with adjacent edges given by vectors \\(\\bar{u}, \\bar{v}\\) and \\(\\bar{w}\\) is the absolute value of the triple scalar product \\[ V = |\\bar{u} \\cdot (\\bar{v} \\times \\bar{w})| \\] Figure 2.3: Figure 2.59 from https://openstax.org/books/calculus-volume-3/pages/2-4-the-cross-product Proof The area of the base of the parallelepiped is given by \\(\\|\\bar{v} \\times \\bar{w}\\|\\), the height is \\(\\bar{u}\\) projected on to the resulting vector from the dot product. So we have \\[ \\begin{split} V &amp;= \\|\\text{proj}_{\\bar{v} \\times \\bar{w}} \\bar{u} \\|\\|\\bar{v} \\times \\bar{w}\\| \\\\ &amp;= |\\frac{\\bar{u} \\cdot (\\bar{v} \\times \\bar{w})}{\\| \\bar{v} \\times \\bar{w} \\|} |\\| \\bar{v} \\times \\bar{w} \\| \\\\ &amp;= |\\bar{u} \\cdot (\\bar{v} \\times \\bar{w})| \\end{split} \\] Looking back at the formula for \\(\\bar{v} \\times \\bar{w}\\), we can see that this term is the absolute value of the following determinant according to cofactor expansion \\[ \\begin{vmatrix} \\bar{u}^T \\\\ \\bar{v}^T \\\\ \\bar{w}^T \\\\ \\end{vmatrix} \\] 2.2.3 Restricted Definition of Inner Products in \\(R^n\\) Sometimes it suffice only to generalize the dot product, with the definition of inner product, in Euclidean space \\(\\mathbb{R}^n\\) instead of other inner product spaces. For example, many engineering applications measure similarity between vectors using the dot product after stretching the two vectors in some directions, with linear transformation \\(A\\). Therefore, we can given a restricted definition of inner product that is meant to be used in \\(\\mathbb{R}^n\\). Definition 2.1 (Restricted definition of inner product) The generalized dot product \\(\\langle \\bar{x}, \\bar{y}\\rangle\\) in \\(\\mathbb{R}^n\\) between two vectors, is the dot product between \\(A\\bar{x}\\) and \\(A\\bar{y}\\), for some \\(n \\times n\\) positive definite matrix \\(S\\). The inner product \\(\\langle \\bar{x}, \\bar{y}\\rangle\\) can also be expressed using the Gram matrix \\(S = A^TA\\) \\[ \\langle \\bar{x}, \\bar{y}\\rangle =\\bar{x}^TS\\bar{y} = \\bar{x}(A^TA)\\bar{y} \\] It’s easy to see that when \\(S\\) is the identity matrix, the inner product is the dot product. \\(S\\) being positive semidefinite ensures \\(\\bar{x}^TS\\bar{y}\\) satisfies axioms of inner product. This definition of inner product also induces angles and distances with respect to transformation \\(A\\) \\[ \\cos_A{(\\bar{x}, \\bar{y})} = \\frac{\\langle \\bar{x}, \\bar{y}\\rangle}{\\sqrt{\\langle \\bar{x}, \\bar{x}\\rangle}\\sqrt{\\langle \\bar{y}, \\bar{y}\\rangle}} = \\frac{\\bar{x}S\\bar{y}}{\\sqrt{\\bar{x}S\\bar{x}}\\sqrt{\\bar{y}S\\bar{y}}} = \\frac{(A\\bar{x})^T(A\\bar{y})}{\\|A\\bar{x}\\|_2\\|A\\bar{y}\\|_2} \\\\ \\text{dist}(\\bar{x}, \\bar{y}) = \\sqrt{\\langle \\bar{x} - \\bar{y}, \\bar{x} - \\bar{y} \\rangle} = \\sqrt{(\\bar{x} - \\bar{y})^TS(\\bar{x} - \\bar{y})} = \\|A\\bar{x} - A\\bar{y} \\|_2 \\] 2.3 Subspaces If \\(A\\) is a vector space, then \\(S\\) is a subspace of \\(A\\) (denoted \\(S \\subseteq A\\)) if \\(\\bar{0} \\in S\\) \\(S\\) is closed under addition: if \\(\\bar{x}, \\bar{y} \\in S\\), then \\(\\bar{x} + \\bar{y} \\in S\\) \\(S\\) is closed under scalar multiplication if \\(\\bar{x} \\in S, \\alpha \\in \\mathbb{R}\\) then \\(\\alpha\\bar{x} \\in S\\) According to this definition, and vector space is always a subspace of itself. If \\(U\\) and \\(W\\) are all subspaces of \\(V\\), then the sum of these two subspaces are defined as \\[ U + W = \\{\\bar{u} + \\bar{v} \\;| \\; \\bar{w} \\in U, \\bar{w} \\in W \\} \\] If \\(U\\) and \\(W\\) are perpendicular, in other words, \\(U \\cap W = \\bar{0}\\). Then \\(U + W\\) are said to be a direct sum and written \\(U \\oplus W\\). Dimensions of the sum of subspaces has the following property \\[ \\text{dim}(U + W) = \\text{dim}(U) + \\text{dim}(W) - \\text{dim}(U \\cap W) \\] It follows that if \\(U\\) is perpendicular to \\(W\\), \\(W\\), \\[ \\text{dim}(U \\oplus W) = \\text{dim}(V) = \\text{dim}(U) + \\text{dim}(W) \\] 2.4 Fundamental Theorem of Linear Algebra The columnspace (also called range) of matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) is the span of the columns of \\(A\\) \\[ \\mathcal{R}(A) = \\{\\bar{v} \\in \\mathbb{R}^m\\;|\\; \\bar{v} = A\\bar{x}, \\, \\bar{x} \\in \\mathbb{R}^n\\} \\] Similarly, the rowspace of \\(A\\) is the span of its rows \\(\\mathcal{R}(A^T)\\). The nullspace (also called kernel) of \\(A\\) is the set of solutions to \\(A\\bar{v} = \\bar{0}\\) \\[ \\mathcal{N}(A) = \\{\\bar{x} \\in \\mathbb{R}^n\\;|\\;A\\bar{x} = \\bar{0}\\} \\] And the left null space is all \\(\\bar{x}\\) that satisfies \\(A^T\\bar{x} = \\bar{0}\\). The word “left” in this context stems from the fact that \\(A^T\\bar{x}= \\bar{0}\\) is equivalent to \\(\\bar{x}^TA=\\bar{0}\\) where y “acts” on A from the left. The relationship between these four subspaces present the fundamental theorem of linear algebra Theorem 2.2 (The fundamental theorem of Linear Algebra) \\(\\mathcal{R}(A) = \\mathcal{N}(A^T)^{\\perp}\\), and \\(\\dim(\\mathcal{R}(A)) + \\dim(\\mathcal{N}(A^T)) = m\\) \\(\\mathcal{R}(A^T) = \\mathcal{N}(A)^{\\perp}\\), and \\(\\dim(\\mathcal{R}(A^T)) + \\dim(\\mathcal{N}(A)) = n\\) If the rank (defined next section) of \\(A \\in \\mathbb{R}^{m \\times n}\\) is \\(r\\) \\(m - r\\) is the dimension of the left null space of \\(A\\) \\(n - r\\) is the dimension of the null space of \\(A\\) 2.5 Rank The rank of a vector space is its dimension. Definition 2.2 The rank of a matrix is equal to the rank of its column space, which is the same as the rank of its column space. I often think of rank of \\(A\\) as the total volume of information that the matrix can offer. From the definition of matrix rank, we know that \\(A\\)’s row rank (the dimension of \\(\\mathcal{R}(A^T)\\))equals its column rank \\(\\mathcal{R}(A)\\). A way to verify this are presented below. All matrices can be reduced into a (possibly rectangular) diagonal matrix with elementary row and column operations. First we can row reduce the matrix into row echelon form, then use column operations to convert positions above the pivot to zero. Thus, any \\(A \\in \\mathbb{R}^{m \\times n}\\) can be expressed in the following form \\[ RAC = \\Lambda \\tag{1} \\] where \\(R\\) is the product of the elementary matrices that perform row opertations, and \\(C\\) for column operations. Since \\(C\\) is invertible, we can write \\[ RA = \\Lambda C^{-1} \\tag{2} \\] From (1) we know that row rank of \\(A\\) is identical to that the number of non-zero entries in \\(\\Lambda\\), on the ground that row operations on \\(A\\) does not change its row rank, and \\(C^{-1}\\) only scale diagonal entries of \\(\\Lambda\\) to some multiple. Similarly, \\(AC = R^{-1}\\Lambda\\) shows that column rank of \\(A\\) is the same as the number of non-zero diagonal entries in \\(\\Lambda\\). Therefore, row rank \\(=\\) column rank. 2.5.1 Effect of Operations on Matrix Rank Let \\(A, B \\in \\mathbb{R}^{m \\times n}\\) have ranks \\(a\\) and \\(b\\) Corollary 2.1 \\(|a - b| \\le r(A + B) \\le a + b\\) \\(r(AB) &lt; \\min(a, b)\\) Proof For (1), rows / columns of \\(A + B\\) can be expressed as linear combinations of rows / columns of \\(A\\) and \\(B\\). For (2), Each column of \\(AB\\) is a linear combination of columns of A, and each row is a linear combination of rows of \\(B\\). Therefore, \\(r(AB)\\) can not exceed either rank of \\(A\\) or \\(B\\). There is also a corollary on the lower bound of \\(r(AB)\\), which is \\(a + b - n\\). Note that \\(n\\) is the shared dimension. I have not found a concise proof about this, but this property leads to a interesting result: when one of \\(A\\) and \\(B\\) are square and full rank, \\(\\min(r(AB)) = \\max(r(AB))\\) Corollary 2.2 Multiplying \\(A\\) with a square matrix \\(B\\) of full rank does not change the rank of \\(A\\). If \\(A\\) and \\(B\\) are both singular, then \\(AB\\) is non-singular if and only if \\(A\\) and \\(B\\) are both non-singular Proof Suppose \\(B\\) is \\(n \\times n\\), the minimum rank of \\(AB\\) is \\(a + n - n = a\\), and the maximum rank is \\(\\min(a, n) = a\\). Thus, multiplying by a full rank matrix preserves rank: \\(r(AB) = r(A)\\). (4) follows naturally after (3). 2.5.2 Gram Matrix Proposition 2.1 (Gram matrix) The matrix \\(A^TA\\) is said to be the Gram matrix of column space of \\(A_{m \\times n}\\). The columns of \\(A\\) are linearly independent if and only if \\(A^TA\\) is invertible. Proof When \\(A^TA\\) is invertible, it has rank \\(n\\). Therefore, each of the factors of \\(A^TA\\) has at least rank \\(n\\), and this means columns of \\(A\\) are linearly independent (since \\(r(A) \\le \\min(m, n)\\)). Similarly, \\(AA^T\\) are called the left Gram matrix of rowspace of \\(A\\). And \\(AA^T\\) is invertible if and only rows of \\(A\\) are linearly independent. Proposition 2.2 For any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(A\\), \\(A^TA\\) and \\(AA^T\\) always have the same rank. \\[ r(A) = r(A^TA) = r(AA^T) \\] Proof For \\(r(A) = r(A^TA)\\), suppose \\(r(A) = r\\), then \\(\\dim(\\mathcal{N}(A)) = n - \\dim(\\mathcal{R}(A^T)) = n - r\\). Note that for any \\(\\bar{x}\\) that satisfies \\(A\\bar{x} = 0\\), we have \\(A^TA\\bar{x} = 0\\). It follows that \\(A\\) and \\(A^TA\\) have the same null space, \\(\\mathcal{N}(A) = \\mathcal{N}(A^TA)\\). Since \\(A^TA \\in \\mathbb{R}^{n \\times n}\\), we have \\(r(A^TA) = \\dim(\\mathcal{R}((A^TA)^T)) = n - \\dim(\\mathcal{N}(A^TA)) = r\\). For \\(r(A) = R(AA^T)\\), note that \\(r(A) = r(A^T)\\), and that \\(\\mathcal{N}(A^T) = \\mathcal{N}(AA^T)\\), then the conclusion presents itself. From the SVD perspective(Corollary 4.1 and Section 5.2), one can show that \\(A^TA\\) and \\(AA^T\\) have the same set of nonzero eigenvalues, and \\(r(A)\\) is the same as the number of nonzero eigenvalues of \\(A^A\\) or \\(AA^T\\), so that \\(r(A) = r(A^TA) = r(AA^T)\\). 2.6 Bases and Coordinate Systems Definition 2.3 (basis of a vector space) Let \\(V\\) be a subspace of \\(\\mathbb{R}^n\\). An indexed set of vectors \\(\\mathcal{B} = \\{\\bar{a}_1, ..., \\bar{a}_r\\}\\) is said to be a basis of \\(V\\), if \\(\\mathcal{B}\\) spans \\(V\\) (these \\(r\\) vectors are linearly independent) Note that \\(\\bar{a}_1, ..., \\bar{a}_r\\) are n-dimensional vectors themselves, though \\(V\\), the subspace they span, is a hyperplane that has dimension strictly less than \\(n\\) when \\(r &lt; n\\). Definition 2.4 (coordinates) The coordinates of \\(\\bar{v}\\) relative to basis \\(\\mathcal{B}\\) (or the \\(\\mathcal{B}\\)-coordinates of \\(\\bar{v}\\), denoted by \\([\\bar{v}]_{\\mathcal{B}}\\)) are the weights \\(x_1, ..., x_r\\) such that \\(\\bar{x} = x_1\\bar{a}_1 + \\cdots + x_r\\bar{a}_n\\) Theorem 2.3 (Unique coordinate vector) Let \\(\\mathcal{B} = \\{\\bar{a}_1, ..., \\bar{a}_n\\}\\) be a basis of \\(V\\). Then for any vector \\(\\bar{v} \\in V\\), there exists a unique coordinate vector \\(\\bar{x} = [x_1, ..., x_r]\\) such that \\[ \\bar{v} = x_1\\bar{a}_1 + \\cdots + x_r\\bar{a}_r \\] Suppose \\(\\bar{v}\\) has another representation \\[ \\bar{v} = y_1\\bar{a}_1 + \\cdots + y_r\\bar{a}_r \\] Then, subtracting we have \\[ \\bar{0} = \\bar{x} - \\bar{x} = (x_1 - y_1)\\bar{a}_1 + \\cdots + (x_r - y_r)\\bar{a}_r \\] Since \\(\\{\\bar{a}_1, ..., \\bar{a}_r\\}\\) is a linearly independent set, we have \\(x_i = y_i\\) for \\(1 \\le i \\le r\\). Thus, the coordinates \\(\\bar{x}\\) of any vector in \\(V\\) in terms of a certain basis is always unique. Note that the coordinate vector of the n-dimensional vector \\(\\bar{v}\\) contains \\(r\\) components instead of \\(n\\), each of which corresponds to a weight associated with one basis in \\(\\mathcal{B}\\) The default basis in \\(\\mathbb{R}^n\\), is the collection of \\(n\\) n-dimensional vectors \\(\\{\\bar{e}_1, ..., \\bar{e}_n\\}\\). Each of \\(\\bar{e}_i\\) contains a \\(1\\) in the \\(i\\)th entry and a value of \\(0\\) in all other entries. \\[ \\bar{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} ,\\, \\bar{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\\\ \\end{bmatrix} , \\, \\cdots ,\\, \\bar{e}_n = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\] The next question is, given \\(\\mathcal{B} = \\{\\bar{a}_1, ..., \\bar{a}_r\\}\\), how can we find the unique coordinates of \\(\\bar{v}\\), which is by default expressed in terms of the standard basis. We start by a special case where \\(\\bar{a}_1, ..., \\bar{a}_r\\) forms a orthonormal basis of \\(V\\), then the coordinates are simply the dot products of \\(\\bar{x}\\) with these vectors. By taking the dot product of both sides of \\(\\bar{v} = \\sum x_i\\bar{a}_i\\) with each \\(\\bar{a}_i\\) and using \\(\\bar{a}_i^T\\bar{a}_i = 1, \\, \\bar{a}_i^T\\bar{a}_j = 0\\), we can show that \\(x_i = \\bar{a}_i^T\\bar{v}\\). This is a frequently used result that, if \\(\\bar{a}_i\\) is a unit vector, \\(\\bar{a}_i^T\\bar{v}\\) will project \\(\\bar{v}\\) onto \\(\\bar{a}_i\\) and return a coordinate component of \\(\\bar{v}\\) in terms of \\(\\bar{a}_i\\). Moreover, we get the whole coordinate vector of \\(\\bar{v}\\) with respect to \\([\\bar{a}_1 \\, \\cdots \\bar{a}_r]\\) by \\[ \\begin{aligned} A &amp;= [\\bar{a}_1 \\, \\cdots \\bar{a}_r] \\\\ A^T\\bar{v} &amp;= \\begin{bmatrix} \\bar{a}_1^T\\bar{v} \\\\ \\vdots \\\\ \\bar{a}_r\\bar{v} \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_r \\\\ \\end{bmatrix} \\end{aligned} \\] When the set \\(\\bar{a}_1, ..., \\bar{a}_r\\) are not orthonormal, a general strategy is solving the system of equations \\(A\\bar{x} = \\bar{v}\\). This boils down to a problem extensively discussed in Chapter 6. If \\(V\\) is simply \\(\\mathbb{R}^n\\), then \\(A\\) is square and invertible, then \\(\\bar{x}\\) is found by \\(A^{-1}\\bar{v}\\). Difficult cases are \\(A\\) is not square, or \\(\\bar{v}\\) does not lie in the hyperplane defined by \\(\\bar{a}_1, ..., \\bar{a}_r\\) and therefore defies valid coordinates. Finding \\(\\bar{x}\\) now becomes a classic least square problem (Section 6.2.1). This results in the following (the best fit solution) \\[ \\bar{x} = (A^TA)^{-1}A^T\\bar{v} \\] 2.6.1 Change of Basis Previous discussions involve different representations of \\(\\bar{x}\\) with respect to two basis, one of which is standard basis in \\(\\mathbb{R}^n\\). This section deals with transformations between two non-standard basis. Now suppose \\(A = [\\bar{a}_1, ..., \\bar{a}_n]\\) and \\(B = [\\bar{b}_1, ..., \\bar{b}_n]\\) are both bases for \\(\\mathbb{R}^n\\). And the vector \\(\\bar{x}\\) has coordinates \\(\\bar{x}_a, \\bar{x}_b\\) with respect to \\(A\\) and \\(B\\) respectively. The goal is to find an \\(n \\times n\\) matrix that transforms one coordinate to another \\[ \\bar{x}_b = P_{a \\rightarrow b}\\,\\bar{x} \\] To find \\(P_{a \\rightarrow b}\\), we use the fact that \\(A\\bar{x}_b\\) and \\(B\\bar{x}_a\\) represents the same vector \\(x\\). We have the following \\[ A\\bar{x}_a = B \\bar{x}_b = \\bar{x} \\] Since \\(B\\) is invertible, we have \\[ \\bar{x}_b = \\underbrace{B^{-1}A}_{P_{a \\rightarrow b}}\\bar{x}_a \\] \\(B^{-1}A\\) is called the change-of-basis/coordinate matrix from \\(\\mathcal{A}\\) to \\(\\mathcal{B}\\), because it converts coordinates from one system to another. When \\(B\\) is large, finding its inverse can be challenging. Though, when \\(\\{\\bar{b}_1, ..., \\bar{b}_n\\}\\) is an orthonormal basis, the transformation matrix simplify to \\(B^TA\\). Additionally, if \\(A\\) corresponds to the standard basis, the transformation matrix is \\(B^T\\), as shown before. Such change-of-basis transformation can be performed between basis sets that define some other r-dimensional hyperplane \\(V\\) rather than \\(\\mathbb{R}^n\\). Moreover, the two basis set could even represent different hyperplane in \\(\\mathbb{R}^n\\). We demonstrate such a case below. Let \\(A = [\\bar{a}_1 \\, \\cdots \\, \\bar{a}_r]\\) and \\(B = [\\bar{b}_1 \\, \\cdots \\, \\bar{b}_r]\\) be two matrices whose columns are bases from different hyperplanes in \\(\\mathbb{R}^n\\), and let \\(\\bar{v}\\) lie in \\(A\\)’s hyperplane. In this case, \\(A\\bar{x}_a \\not = B\\bar{x}_b\\) because \\(B\\bar{x}_b\\) is only a best-fit solution of \\(\\bar{v}\\). This is again a least square problem. We have the following \\[ B^T(A\\bar{x}_a - B\\bar{x}_b) = \\bar{0} \\] Therefore \\[ \\bar{x}_b = \\underbrace{(B^TB)^{-1}B^TA}_{P_{a \\rightarrow b}}\\bar{x}_a \\] Change of basis has various applications in machine learning, such as discrete wavelet transform (P.60, Ch2, Aggarwal (2020)). In time series analysis, a time series of length \\(n\\) would result in a vector from \\(\\mathbb{R}^{n}\\), and \\(n\\) can be extremely large. Since consecutive sample points tend to be similar, we may take larger interest in a few variations across time, rather than the whole long vector. The Haar wavelet transformation performs a basis transformation with a view to extracting important variations, making the original time series vector reasonably sparse. 2.7 Complexity of Vector Computations "],
["orthogonality.html", "Chapter 3 Orthogonality 3.1 Orthogonal Decomposition 3.2 Orthonormal Sets and Orthogonal Matrices 3.3 Projection and idempotent matrices 3.4 Gram-Schmidt Process 3.5 QR Factorization 3.6 Complexity", " Chapter 3 Orthogonality 3.1 Orthogonal Decomposition 3.1.1 Orthogonal Complements if vector \\(\\bar{v}\\) is orthogonal to every vector in a subspace \\(W\\) of \\(\\mathbb{R^n}\\), then \\(\\bar{v}\\) is said to be orthogonal to \\(W\\). The subspace that contains the set of vectors that are orthogonal to \\(W\\) is called the orthogonal complement, denoted by \\(W^{\\perp}\\). \\[ W^{\\perp} = \\{\\bar{v} \\in W^{\\perp} | \\;\\bar{v} \\perp \\bar{x} \\; \\text{for all} \\; \\bar{x} \\in W\\} \\] This corresponds to discussions in Section 2.4, where \\[ \\mathcal{R}(A^T) = \\mathcal{N}(A) \\\\ \\mathcal{R}(A) = \\mathcal{N}{(A^T)} \\] Theorem 1.2 If \\(W\\) is a subspace of \\(\\mathbb{R}^n\\), \\(W^{\\perp}\\) is also a subspace of \\(\\mathbb{R}^n\\). It’s easy to verify that \\(W^{\\perp}\\) is closed under scalar multiplication, and under vector addition, and that any vector in \\(W\\) has \\(n\\) components. So that \\(W^{\\perp}\\) is a subspace of \\(\\mathbb{R}^n\\) 3.1.2 Orthogonal Sets and Orthogonal Basis An orthogonal set is a set of vectors \\(\\{\\bar{u}_1, \\dots, \\bar{u}_p\\}\\) in \\(\\mathbb{R^n}\\), in which each pair of distinct vectors is orthogonal: \\(\\bar{u}_i^{T} \\bar{u}_j = 0 \\quad i\\not = j\\). Note that the set do not necessarily span the whole \\(\\mathbb{R^n}\\), but a subspace \\(W\\). Since vectors in orthogonal sets is mutually perpendicular, they must also be linearly independent and could form a basis for a subspace \\(W\\). In such case, they are called orthogonal basis. There is a particular advantage in using orthogonal basis rather than other basis, because we can find a easy representation of any vector in \\(W\\). Theorem 3.1 For each \\(\\bar{y}\\) in \\(W\\), there exists a linear combination \\[ y = c_1\\bar{u}_1 + \\cdots + c_p\\bar{u}_p \\] and \\[ c_i = \\frac{\\bar{y} \\cdot \\bar{u}_i}{\\bar{u}_i \\cdot \\bar{u}_i} \\quad i = 1, \\cdots, p \\] where \\(\\{\\bar{u}_1, \\dots, \\bar{u}_p\\}\\) is an orthogonal basis. Proof \\[ \\begin{split} \\bar{u}_1 \\cdot \\bar{y} &amp;= \\bar{u}_1 \\cdot (c_1\\bar{u}_1 + \\cdots + c_p\\bar{u}_p) \\\\ &amp;= c_1 \\bar{u}_1 \\cdot \\bar{u}_1 \\end{split} \\] So: \\[ c_1 = \\frac{\\bar{u}_1 \\cdot \\bar{y}}{\\bar{u}_1 \\cdot \\bar{u}_1} \\] Derivations for other \\(c_i\\) is similar. 3.1.3 Orthogonal Decomposition Orthogonal decomposition split \\(\\bar{y}\\) in \\(\\mathbb{R^n}\\) into two vectors, one in \\(W\\) and one in its orthogonal compliment \\(W^{\\perp}\\). Theorem 3.2 Let \\(\\mathbb{R}^n\\) be a inner product space and \\(W\\) and subspace of \\(\\mathbb{R}^n\\). Then every \\(\\bar{v}\\) in \\(W\\) can be written uniquely in the form \\[ \\bar{v} = \\bar{v}_w + \\bar{v}_{\\perp} \\] where \\(\\bar{v}_w \\in W\\) and \\(\\bar{v}_{\\perp} \\in W^{\\perp}\\) Proof Let \\(\\bar{u}_1, ..., \\bar{u}_m\\) be a orthonormal basis for \\(W\\), there exists linear combination according to Section 3.1.2 \\[ \\bar{v}_w = (\\bar{v} \\cdot \\bar{u}_1)\\bar{u}_1 + \\cdots + (\\bar{v} \\cdot \\bar{u}_m)\\bar{u}_m \\] and \\[ \\bar{v}_{\\perp} = \\bar{v} - \\bar{v}_w \\] It is clear that \\(\\bar{v}_W \\in W\\). And we can also show that \\(\\bar{v}_{\\perp}\\) is perpendicular to \\(W\\) \\[ \\begin{split} \\bar{v}_{\\perp} \\cdot \\bar{u}_i &amp;= [\\bar{v}- (\\bar{v} \\cdot \\bar{u}_1)\\bar{u}_1 - \\cdots - (\\bar{v} \\cdot \\bar{u}_m)\\bar{u}_m] \\cdot \\bar{u}_i \\\\ &amp;= (\\bar{v} \\cdot \\bar{u}_1) - [(\\bar{v} \\cdot \\bar{u}_i)\\bar{u}_i \\cdot \\bar{u}_i] \\\\ &amp;= 0 \\end{split} \\] which implies \\(\\bar{v}_{\\perp} \\in W^{\\perp}\\). To prove that \\(\\bar{v}_w\\) and \\(\\bar{v}_{\\perp}\\) are unique (does not depend on the choice of basis), let \\(\\bar{u}_1&#39;, ..., \\bar{u}_m&#39;\\) be another orthonormal basis for \\(W\\), and define \\(\\bar{v}_w&#39;\\) and \\(\\bar{v}_{\\perp}&#39;\\) similarly we want to get \\(\\bar{v}_w&#39; = \\bar{v}_w\\) and \\(\\bar{v}_{\\perp}&#39; = \\bar{v}_{\\perp}\\). By definition \\[ \\bar{v}_w + \\bar{v}_{\\perp} = \\bar{v} = \\bar{v}_w&#39; + \\bar{v}_{\\perp}&#39; \\] so \\[ \\underbrace{\\bar{v}_w - \\bar{v}_w&#39;}_{\\in W} = \\underbrace{\\bar{v}_{\\perp}&#39; - \\bar{v}_{\\perp}}_{\\in W^{\\perp}} \\] From the orthogonality of these subspaces, we have \\[ 0 = (\\bar{v}_w - \\bar{v}_w&#39;) \\cdot (\\bar{v}_{\\perp}&#39; - \\bar{v}_{\\perp}) = (\\bar{v}_w - \\bar{v}_w&#39;) \\cdot (\\bar{v}_w - \\bar{v}_w&#39;) = \\|\\bar{v}_w - \\bar{v}_w&#39;\\|^2 \\] Similarly we have \\(\\|\\bar{v}_{\\perp}&#39; - \\bar{v}_{\\perp}\\|^2 = 0\\). The existence and uniqueness of the decomposition above mean that \\[ \\mathbb{R}^n = W \\oplus W^{\\perp} \\] whenever \\(W\\) is a subspace. 3.2 Orthonormal Sets and Orthogonal Matrices An orthogonal set whose components are all unit vectors is said to be orthonormal sets. According this definition, we can easily create an orthonormal set using the original orthogonal set after perform scaling. If \\(\\{\\bar{u}_1, \\dots, \\bar{u}_p\\}\\) forms an orthogonal set in \\(\\mathbb{R^n}\\), then an orthonormal set will be \\(\\{\\bar{q}_1, \\dots, \\bar{q}_p\\}\\) \\[ \\begin{aligned} \\bar{q}_1 &amp;= \\frac{\\bar{u}_1}{\\|\\bar{u}_1\\|} \\\\ \\bar{q}_2 &amp;= \\frac{\\bar{u}_2}{\\|\\bar{u}_2\\|} \\\\ \\vdots \\\\ \\bar{q}_p &amp;= \\frac{\\bar{u}_p}{\\|\\bar{u}_p\\|} \\end{aligned} \\] 3.2.1 Orthogonal Matrices An orthogonal matrix is a square matrix \\(Q\\) whose inverse is its transpose: \\[ \\tag{3.1} QQ^T = Q^TQ = I \\] Another way of defining it is that an orthogonal matrix has both orthonormal columns and orthonormal rows. Orthogonal matrices have a nice property that they preserve inner products: \\[ (Q\\bar{x})^T(Q\\bar{y}) = \\bar{x}^TQ^TQ\\bar{y} = \\bar{x}^TI\\bar{y} = \\bar{x}^T\\bar{y} \\] A direct result is that \\(Q\\) preserves L2 norms \\[ \\|Q\\bar{x}\\|_2 = \\sqrt{(Q\\bar{x})^T(Q\\bar{x})} = \\sqrt{\\bar{x}^T\\bar{x}} = \\|\\bar{x}\\|_2 \\] Therefore multiplication by an orthogonal matrix can be considered as a transformation that preserves length, but may rotate or reflect the vector about the origin. Note that \\(Q\\) may not necessarily be a square matrix to satisfy \\(Q^TQ = I\\). For exmaple \\(Q \\in \\mathbb{R}^{m \\times n}, n &lt; m\\), but its columns and rows can still be orthonormal, then \\(QQ^T = I\\). But in most cases the term orthogonal implies a square matrix \\(Q\\). 3.2.2 Best Approximation Theorem 3.3 (The Best Approximation) Given \\(\\bar{y}\\) be any vector in \\(\\mathbb{R^n}\\), with its subspace \\(W\\), let \\(\\hat{\\bar{y}}\\) be the orthogonal projection of \\(\\bar{y}\\) onto \\(W\\). Then \\(\\hat{\\bar{y}}\\) is the closest point in \\(W\\) to \\(\\bar{y}\\) in the sense that \\[ \\|\\bar{y} - \\hat{\\bar{y}}\\| \\le \\|\\bar{y} - \\bar{v}\\| \\] Proof Take \\(\\bar{v}\\) distinct from \\(\\hat{\\bar{y}}\\) in \\(W\\), we know that \\(\\bar{y} - \\hat{\\bar{y}}\\) is perpendicular to \\(\\bar{v}\\). According to Pythoagorean theorem, we have Figure 2.2: figure from page p352, ch6 (Lay 2006) \\[ \\|\\bar{y}- \\bar{v}\\|^2 = \\|\\bar{\\hat{y}} - \\bar{v}\\|^2 + \\|\\bar{y} -\\bar{\\hat{y}}\\|^2 \\] When \\(\\bar{v}\\) is distinct from \\(\\bar{\\hat{y}}\\), \\(\\|\\bar{\\hat{y}} - \\bar{v}\\|^2\\) is non-negative, so the error term of choosing \\(\\bar{v}\\) is always larger than that of the orthogonal projection \\(\\bar{\\hat{y}}\\). 3.3 Projection and idempotent matrices \\[ \\begin{split} P_S\\bar{v} &amp;= (\\bar{v} \\cdot \\bar{u}_1)\\bar{u}_1 + \\cdots + (\\bar{v} \\cdot \\bar{u}_m)\\bar{u}_m \\\\ &amp;= \\bar{v}^T\\bar{u}_1\\bar{u}_1 + \\cdots + \\bar{v}^T\\bar{u}_m\\bar{u}_m\\\\ &amp;= (\\bar{u}_1\\bar{u}_1^T)\\bar{v} + \\cdots + (\\bar{u}_m\\bar{u}_m^T)\\bar{v} \\\\ &amp;= (\\sum_{i=1}^{M}{\\bar{u}_i\\bar{u}_i^T})\\bar{v}\\\\ &amp;= \\begin{bmatrix} \\bar{u}_1 &amp; \\cdots &amp; \\bar{u}_m \\end{bmatrix} \\begin{bmatrix} \\bar{u}_1^T \\\\ \\vdots \\\\ \\bar{u}_m^T \\end{bmatrix}\\bar{v} \\\\ &amp;= UU^T\\bar{v} \\end{split} \\] In practical problems, it is more convenient to use matrix at hand rather than producing an orthonormal basis. Corollary 3.1 Idempotent matrices have eigenvalues either \\(1\\) or \\(0\\). Another way to derive projection matrices with matrix calculus \\[ \\begin{split} \\|A\\bar{x} - \\bar{b}\\|_2^2 &amp;= (A\\bar{x} - \\bar{b})^T(A\\bar{x} - \\bar{b}) \\\\ &amp;= \\bar{x}^TA^TA\\bar{x} - 2\\bar{b}^TA\\bar{x} + \\bar{b}^T\\bar{b} \\end{split} \\] \\[ \\begin{split} \\nabla_{x}(\\bar{x}^TA^TA\\bar{x} - 2A\\bar{x}\\bar{b} + \\bar{b}^T\\bar{b}) &amp;=\\nabla_x(\\bar{x}^TA^TA\\bar{x}) - \\nabla_x{2\\bar{b}^TA\\bar{x}} + \\nabla_x{\\bar{b}^T\\bar{b}} \\\\ &amp;= 2(A^TA)\\bar{x} - 2A^T\\bar{b} \\end{split} \\] \\[ \\bar{x} = (A^TA)^{-1}A^T\\bar{b} \\] \\[ A\\bar{x} = A(A^TA)^{-1}A^T\\bar{b} = \\hat{\\bar{b}} \\] 3.4 Gram-Schmidt Process Then Gram-Schmidt process is a simple algorithm that transforms a set of linearly independent vectors into orthogonal or orthonormal basis for a subspace. In its essence, it is a sequential projection of \\(\\bar{x}_{i}\\) onto the space spanned by the previously created orthogonal set \\(\\{\\bar{v}_{1}, ..., \\bar{v}_{i-1}\\}\\), and take the term in the orthogonal compliment to be \\(\\bar{v}_{i}\\). Theorem 3.4 (the Gram-Schmidt process) Given a basis \\(\\{\\bar{x}_1, ..., \\bar{x}_p\\}\\) for a nonzero subspace \\(W\\) of \\(\\mathbb{R}^n\\), define \\[ \\begin{aligned} \\bar{u}_1 &amp;= \\bar{x}_1 \\\\ \\bar{u}_2 &amp;= \\bar{x}_2 - \\frac{\\bar{x}_2 \\cdot \\bar{u}_1}{\\bar{u}_1 \\cdot \\bar{u}_1}\\bar{u}_1 \\\\ \\bar{u}_3 &amp;= \\bar{x}_3 - \\frac{\\bar{x}_3 \\cdot \\bar{u}_1}{\\bar{u}_1 \\cdot \\bar{u}_1}\\bar{u}_1 - \\frac{\\bar{x}_3 \\cdot \\bar{u}_2}{\\bar{u}_2 \\cdot \\bar{u}_2}\\bar{u}_2 \\\\ &amp; \\vdots \\\\ \\bar{u}_p &amp;= \\bar{x}_p - \\frac{\\bar{x}_p \\cdot \\bar{u}_1}{\\bar{u}_1 \\cdot \\bar{u}_1}\\bar{u}_1 - \\frac{\\bar{x}_p \\cdot \\bar{u}_2}{\\bar{u}_2 \\cdot \\bar{u}_2}\\bar{u}_2 - \\cdots - \\frac{\\bar{x}_p \\cdot \\bar{u}_{p-1}}{\\bar{u}_{p-1} \\cdot \\bar{u}_{p-1}}\\bar{u}_{p-1} \\end{aligned} \\] Then \\(\\{\\bar{u}_1, ..., \\bar{u}_p\\}\\) is an orthogonal basis for \\(W\\). In addition \\[ \\text{Span}\\{\\bar{u}_1, ..., \\bar{u}_p\\} = \\text{Span}\\{\\bar{x}_1, ..., \\bar{x}_p\\} \\] To make \\(\\{\\bar{u}_1, ..., \\bar{u}_p\\}\\) an orthonormal basis, there is simply one more step of normalization \\[ \\{\\bar{q}_i = \\frac{\\bar{u}_i}{\\|\\bar{u}_i\\|}, \\;i = 1, ... p\\} \\] If the vectors \\(\\bar{x}_1, \\cdots, \\bar{x}_{j-1}\\) are linearly independent, but \\(\\bar{a}_1, ..., \\bar{a}_j\\) are linearly independent. We can come up with an algorithm for Gram-Schmidt process that terminates automatically when it produced most orthonormal basis vectors. Gram–Schmidt algorithm given \\(n\\)-vectors \\(\\bar{x}_1, ..., \\bar{x}_p\\). Let \\(u_1 = \\bar{x}_1\\) and \\(q_1 = \\frac{\\bar{u}_1}{\\|\\bar{u}_1 \\|}\\) (assuming \\(x_1\\) is not zero), for \\(i = 2, ..., k\\) Orthogonalization: \\(u_i = \\bar{x}_i - (\\bar{q}_1 \\cdot \\bar{x}_i)\\bar{q}_1 - \\cdots - (\\bar{q}_{i-1} \\cdot \\bar{x}_i)\\bar{q}_{i-1}\\) Test for linear independence. If \\(\\bar{u}_i = \\bar{0}\\), quit and return \\(\\bar{q}_i, ..., \\bar{q}_{i- 1}\\) as the orthonormal basis for \\(\\mathbb{R}^{i - 1}\\) Normalization: \\(\\bar{q}_i = \\frac{\\bar{u}_i}{\\|\\bar{u}_i \\|}\\) The Gram-Schmidt basis does not expose any specific properties of a vector with the help of its coordinates. The discrete cosine transform uses a basis with trigonometric properties in order to expose periodicity in a time series. (see Section 2.7.3 of LAOML (Aggarwal 2020)) 3.5 QR Factorization For \\(A \\in \\mathbb{R}^{m \\times n}\\) with linearly independent columns \\(\\bar{x}_1, ..., \\bar{x}_n\\), apply the Gram-Schmidt process to \\(\\bar{x}_1, ..., \\bar{x}_n\\) amounts to factorizing \\(A\\). Theorem 3.5 (QR factorization) if \\(A\\) is an \\(m \\times n\\) matrix with full column rank, then \\(A\\) can be factored as \\(A = QR\\), where \\(Q\\) is an \\(m \\times n\\) matrix whose columns form an orthonormal basis of \\(\\text{Col}\\;A\\) and \\(R\\) is an \\(n \\times n\\) upper triangular invertible matrix with positive entries on its diagonal. Proof Because \\(A_{m \\times n}\\) is full column rank, we can transform its column vector \\(\\{\\bar{a}_{1}, ..., \\bar{a}_{n}\\}\\) into a new set of orthonormal basis \\(\\{\\bar{q}_{1}, ..., \\bar{q}_{n}\\}\\) with Gram-Schmidt process. Let \\[ Q = [\\bar{q}_{1} \\;\\; \\cdots \\;\\; \\bar{q}_{n}] \\] To find \\(R\\), let’s consider how the Gram-Schmidt process works. We create \\(\\{\\bar{q}_{1}, \\cdots, \\bar{q}_{n}\\}\\) by (for simplicity exclude the vector symbol) \\[ \\begin{aligned} u_1 &amp;= a_1 &amp;&amp; q_1 = \\frac{u_1}{\\|u_1\\|} \\\\ u_2 &amp;= a_2 - (a_2 \\cdot u_1)u_1 &amp;&amp; q_2 = \\frac{u_2}{\\|u_2 \\|} \\\\ u_3 &amp;= a_3 - (a_3 \\cdot u_1)u_1 - (a_3 \\cdot u_2)u_2 &amp;&amp; q_3 = \\frac{q_3}{\\|q_3\\|} \\\\ \\vdots \\\\ u_n &amp;= a_n - \\sum_{i = 1}^{n}(a_n \\cdot u_i)u_i &amp;&amp; q_n = \\frac{u_n}{\\|u_n\\|} \\end{aligned} \\] Then solve for \\(a_i\\) over the newly produced orthonormal basis. Recall the geometric interpretation of Gram-Schmidt Process, the representation of \\(a_1\\) depends only on \\(q_1\\), \\(a_2\\) depends only on \\(q_1\\) and \\(q_2\\), \\(a_3\\) depends only on \\(q_1, q_2, q_3\\) and so on \\[ \\begin{aligned} a_1 &amp;= (q_1 \\cdot a_1) q_1 \\\\ a_2 &amp;= (q_1 \\cdot a_2) q_1 + (q_2 \\cdot a_2)q_2 \\\\ a_3 &amp;= (q_1 \\cdot a_3)q_1 + (q_2 \\cdot a_3)q_2 + (q_3 \\cdot a_3)q_3 \\\\ \\vdots \\\\ a_n &amp;= \\sum_{i=1}^{n}(q_i \\cdot a_n)q_i \\end{aligned} \\] The set equations above is essentially expressing \\(a_i\\) by their coordinates associated with the orthonormal basis \\(q_i\\). As a summary, for any column of \\(A\\), there exists a set of constant \\(r_{1k}, ..., r_{kk}\\) where \\(r_{ik} = q_i \\cdot a_k\\), such that \\[ \\bar{x}_k = r_{1k}\\bar{q}_{1} + \\cdots + r_{kk}\\bar{q}_{k} + 0 \\cdot\\bar{q}_{k+1} + \\cdots + 0 \\cdot \\bar{q}_{n} \\] Therefore \\[ A = [\\bar{x}_{1} \\;\\; \\bar{x}_{2} \\;\\; \\cdots \\;\\; \\bar{x}_{n}] = [\\bar{q}_{1} \\;\\; \\bar{q}_{2} \\;\\; \\cdots \\;\\; \\bar{q}_{n}] \\begin{bmatrix} r_{11} &amp; r_{12} &amp; \\cdots &amp; r_{1n} \\\\ 0 &amp; r_{22} &amp; \\cdots &amp; r_{2n} \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\ 0 &amp; 0 &amp; \\cdots &amp; r_{nn} \\end{bmatrix} = QR \\] We could assume that \\(r_{kk} \\ge 0\\). (if \\(r_{kk} &lt; 0\\), multiply both \\(r_{kk}\\) and \\(\\bar{u}_k\\) by \\(-1\\)) 3.6 Complexity Compleity of the "],
["eigenthings-and-quadratic-forms.html", "Chapter 4 Eigenthings and Quadratic Forms 4.1 Eigenvectors and Eigenvalues 4.2 Diagnolization and Similar Matrices 4.3 Symmetric Matrices 4.4 Quadratic Forms 4.5 Cholesky Factorization 4.6 Rayleigh Quotients", " Chapter 4 Eigenthings and Quadratic Forms 4.1 Eigenvectors and Eigenvalues Definition 4.1 (Eigenvectors and eigenvalues) An eigenvector of an \\(n \\times n\\) matrix \\(A\\) is a nonzero vector \\(\\bar{x}\\) such that \\(A\\bar{x} = \\lambda\\bar{x}\\). \\(\\lambda\\) is the eigenvalue of \\(A\\) if there is a nontrivial solution \\(\\bar{x}\\) of \\(A\\bar{x} = \\lambda \\bar{x}\\); such an \\(\\bar{x}\\) is called an eigenvector corresponding to \\(\\lambda\\) To find eigenvalues and corresponding eigenvectors of \\(A\\), we look at the equation \\[ (A - \\lambda I)\\bar{x}= 0 \\] Since eigenvector \\(\\bar{x}\\) must be nonzero, \\((A - \\lambda I)\\) is a singular matrix \\[\\begin{equation} \\tag{4.1} \\det (A - \\lambda I) = 0 \\end{equation}\\] Eq (4.1) is called the characteristic equation of matrix \\(A\\). This is a scalar equation containing information about eigenvalues and eigenvectors of a square matrix \\(A\\). Theorem 1.2 Eigenvalues of a trangular matrix are its diagonal entries. Proof Consider the \\(3 \\times 3\\) case. If \\(A\\) is upper triangular, then \\(A - \\lambda I\\) has the form \\[ \\begin{bmatrix} a_{11} - \\lambda &amp; a_{12} &amp; a_{13} \\\\ 0 &amp; a_{22} - \\lambda &amp; a_{23} \\\\ 0 &amp; 0 &amp; a_{33} - \\lambda \\end{bmatrix} \\] So the roots of characteristic are \\(a_{11}, a_{22}, a_{33}\\) respectively. There are some useful results about how eigenvalues change after various manipulations. For any \\(k, b \\in \\mathbb{R}\\), \\(\\bar{x}\\) is an eigenvector of \\(kA + bI\\) with eigenvalue \\(k\\lambda + b\\) If \\(A\\) is invertible, then \\(\\bar{x}\\) is an eigenvector of \\(A^{-1}\\) with eigenvalue \\(1/\\lambda\\) \\(A^{k}\\bar{x} = \\lambda^{k}\\bar{x}\\) Proof For (1) \\[ (kA + bI)\\bar{x} = kA\\bar{x} + bI\\bar{x} = k \\lambda\\bar{x} + b\\bar{x} = (k\\lambda + b)\\bar{x} \\] For(2) \\[ \\bar{x} = A^{-1}A\\bar{x} = A^{-1}\\lambda \\bar{x} = \\lambda A^{-1}\\bar{x} \\] The next theorem is important in terms of diagonalization and spectral decomposition Theorem 4.1 For distinct eigenvalues \\(\\lambda_1, \\cdots, \\lambda_r\\) of an \\(n \\times n\\) matrix A, their corresponding eigenvectors \\(\\bar{v_1}, ..., \\bar{v_r}\\) are linearly independent. Proof Suppose for r distinct eigenvalue \\(\\lambda_1, \\cdots, \\lambda_r\\), the set \\(\\{\\bar{v_1}, ..., \\bar{v_r}\\}\\) is not linearly independent, and \\(p\\) is the least index such that \\(\\bar{v}_{p+1}\\) is a linear combination of the preceding vectors. Then there exists scalars \\(c_1, \\cdots, c_p\\) such that \\[ c_1\\bar{v}_1 + \\cdots + c_p\\bar{v}_p = \\bar{v}_{p+1} \\tag{1} \\] Left multiply by \\(A\\), and note we have \\(A\\bar{v}_i = \\lambda_i\\bar{v}_i\\) for \\(i = 1, ..., n\\) \\[ c_1\\lambda_1\\bar{v}_1 + \\cdots + c_p\\lambda_p\\bar{v}_p = \\lambda_{p+1}\\bar{v}_{p+1} \\tag{2} \\] Multiplying both sides of (2) by \\(\\lambda_{p+1}\\) and subtracting (2) from the result \\[ c_1(\\lambda_1 - \\lambda_{p+1})\\bar{v}_1 +\\cdots + c_p(\\lambda_p - \\lambda_{p+1})\\bar{v}_p = 0 \\tag{3} \\] Since \\(\\bar{v}_1, ..., \\bar{v}_p\\) are linearly independent, weights in (3) must be all zero. Since \\(\\lambda_1, \\cdots, \\lambda_p\\) are distinct, hence \\(c_i = 0, \\, i = 1, ..., p\\). But then (5) says that eigenvector \\(\\bar{v}_{p+1}\\) is zero vector, which contradicts definition 4.1 Corollary 4.1 Let \\(A \\in \\mathbb{R}^{m \\times n}\\) \\(A^TA\\) and \\(AA^T\\) has the same set of nonzero eigenvalues. Proof Let \\(\\lambda\\) be a nonzero eigenvalue of \\(A^TA\\) and \\(\\bar{x}\\) its eigenvector \\[ \\begin{split} (A^TA)\\bar{x} &amp;= \\lambda\\bar{x} \\\\ \\end{split} \\] Left multiply by \\(A\\) \\[ AA^T(A\\bar{x}) = \\lambda (A\\bar{x}) \\] We will have to verify that \\(A\\bar{x}\\) is no zero vector before concluding \\(\\lambda\\) is also an eigenvector of \\(AA^T\\). Suppose \\(A\\bar{x} = 0\\), then \\(A^TA\\bar{x} =\\lambda\\bar{x} = 0\\). Since \\(\\bar{x}\\) is a eigenvector which is nonzero, \\(\\lambda = 0\\), which contradicts our former statement. Thus, any nonzero eigenvalue of \\(A^TA\\) is also an eigenvalue of \\(AA^T\\). \\(A^TA\\) and \\(AA^T\\) are known as Gram matrix and left Gram matrix in corollary 2.1 4.1.1 Additional Properties of Eigenvalues and Eigenvectors Let \\(A \\in \\mathbb{R}^{n \\times n}\\) with eigenvalues \\(\\lambda_1, ..., \\lambda_n\\). Here are some additional properties of this matrix and its eigenvalues: The trace of \\(A\\) is the sum of all eigenvalues \\[ \\text{tr}(A) = \\sum_{i=1}^{n}{\\lambda_i} \\] The determinant of \\(A\\) is the product of all its eigenvalues. \\[ \\det(A) = \\prod_{i=1}^{n}{\\lambda_i} \\] The eigenvalues of \\(k\\)th power of \\(A\\), i.e. \\(A^k\\), is \\(\\lambda_1^k, ..., \\lambda_n^k\\) If \\(A\\) is invertible, then eigenvalues of \\(A^{-1}\\) are \\(\\frac{1}{\\lambda_1}, ..., \\frac{1}{\\lambda_n}\\) For a polynomial function \\(P\\) the eigenvalues of \\(P(A)\\) are \\(P(\\lambda_1), ..., P(\\lambda_n)\\) 4.1.2 Left Eigenvectors and Right Eigenvectors \\[ \\bar{x}A = \\lambda\\bar{x} \\] 4.2 Diagnolization and Similar Matrices Definition 4.2 (Diagonalization thoerem) An \\(n \\times n\\) matrix \\(A\\) is diagnolizable if and only if A has \\(n\\) independent linearly independent eigenvectors. In such case, in \\(A = P \\Lambda P^{-1}\\), the diagonal entries of \\(D\\) are eigenvalues that correpond, respectively, to the eigenvectors of in \\(P\\) In other words, \\(A\\) is diagnolizable if and only if there are enough eigenvectors in form a basis of \\(R^n\\), called an eigenvector basis of \\(R^n\\) Proof \\[ \\begin{split} AP &amp;= A[\\bar{v}_1 \\cdots \\bar{v}_n] \\\\ &amp;= [A\\bar{v}_1 \\cdots A\\bar{v}_n] \\\\ &amp;= [\\lambda_1\\bar{v}_1 \\cdots \\lambda_n\\bar{v}_n] \\end{split} \\] while on the other side of the equation: \\[ \\begin{aligned} DP &amp;= [\\bar{v}_1 \\cdots \\bar{v}_n] \\begin{bmatrix} \\lambda_1 &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; \\lambda_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\lambda_n \\end{bmatrix} \\\\ &amp;= [\\lambda_1\\bar{v}_1 \\cdots \\lambda_n\\bar{v}_n] \\end{aligned} \\] So that \\[ \\begin{aligned} AP &amp;= PD \\\\ A &amp;= P \\Lambda P^{-1} \\end{aligned} \\] Because \\(P\\) contains \\(n\\) independent columns so it’s invertible. According to theorem 4.1, an \\(n \\times n\\) matrix with \\(n\\) distinct eigenvalues is diagonalizable. This is a sufficient condition. For matrices whose eigenvalues are not distinct, there is still a change that it is diagonalizable. For any matrix \\(A_{n\\times n}\\), as long as the sum of the dimensions of the eigenspaces equals \\(n\\) then \\(P\\) is invertible. This could happen in the following two scenarios The characteristic polynomial factors completely into linear factors. This is the case when \\(A\\) has n distinct eigenvalues. The dimension of the eigenspace for each \\(\\lambda_k\\) equals the multiplicity of \\(\\lambda_k\\). Thus \\(A\\) with repeated eigenvalues can still be diagonalizable. Repeated eigenvalues create the possibility that a diagonalization might not exist. Particularly, if less than \\(r_i\\) eigenvectors exist for an eigenvalue with multiplicity \\(r_i\\), a diagonalization does not exist. Such a matrix is said to be deflective. 4.2.1 Similarity If \\(A\\) and \\(B\\) are both \\(n \\times n\\) matrices, then \\(A\\) is similar to \\(B\\) if there is an invertible matrix \\(P\\) such that \\(P^{-1}AP = B\\), or equivalently if we write \\(Q\\) for \\(P^{-1}\\), \\(Q^{-1}BQ = A\\). Changing \\(A\\) into \\(P^{-1}AP\\) is called a similarity transformation. Theorem 3.1 If \\(A\\) and \\(B\\) are similar, they have the same eigenvalues. Proof If \\(B = P^{-1}AP\\), then \\[ B - \\lambda I = P^{-1}AP - \\lambda P^{-1}P = P^{-1}(AP - \\lambda P) = P^{-1}(A - \\lambda I) P \\] so that \\[ \\det (B - \\lambda I ) = \\det(P) \\cdot \\det(A - \\lambda I ) \\cdot \\det(P^{-1}) \\] since \\(\\det(P) \\cdot \\det(P^{-1}) = \\det (I) = 1\\), we have \\[ \\det (B - \\lambda I) = \\det(A - \\lambda I) \\] As a result of their identical characteristic polynomial, \\(B\\) and \\(A\\) have the same eigenvalues. We can also show that eigenvector of \\(B\\) is \\(P\\bar{v}\\): \\[ \\begin{aligned} A\\bar{v} &amp;= \\lambda\\bar{v} \\\\ (P^{-1}BP)\\bar{v} &amp;= \\lambda\\bar{v} \\\\ P(P^{-1}BP)\\bar{v} &amp;= \\lambda P\\bar{v} \\\\ B(P\\bar{v}) = \\lambda P \\bar{v} \\end{aligned} \\] The similarity theorem leads to a interesting result. Corollary 4.2 For \\(A, B \\in \\mathbb{R}^{n \\times n}\\), \\(AB\\) and \\(BA\\) are similar matrices and therefore share the same set of eigenvalues. To prove this, we need to show that there exists a invertible matrix \\(A\\) such that \\(P^{-1}(AB)P = BA\\). Take \\(P = A\\) and the equation holds. It is easy to show that similarity is transitive: if \\(A\\) is similar to \\(B\\), \\(B\\) is similar to \\(C\\), then \\(A\\) is similar to \\(C\\). So similarity means a family of matrices with the same set of eigenvalues, the most special and simplest of which is the diagonal matrix (if this is an diagonalizable family). Some computer algorithms calculate eigenvalues of \\(A\\) in this manner: with a sequential choices of \\(P\\), the off-diagonal elements of \\(A\\) become smaller and smaller until \\(A\\) becomes a triangular matrix or diagonal matrix, whose eigenvalues are simply diagonal entries and is the same as \\(A\\). It is obvious that a diagonalizable matrix \\(A\\) is similar to diagonal matrix \\(D\\), whose diagonal entries are \\(A\\)’s eigenvalues \\(\\lambda_i\\), and \\(P = [\\bar{v}_1 \\;\\; \\cdots \\;\\; \\bar{v}_n]^{-1}\\) where \\(\\bar{v}_i, \\;i = 1,..., n\\) are eigenvectors corresponding to \\(\\lambda_i\\). But square matrix \\(A\\) can still be similar to matrices other than \\(D\\) with other choices of \\(P\\), and non-diagonal matrices can also have similar matrices of their own. In fact, every square matrix is similar to a matrix in Jordan matrix 4.2.2. Similarity is only a sufficient condition for identical eigenvalues. The matrices \\[ \\begin{bmatrix} 2 &amp; 1 \\\\ 0 &amp; 2 \\end{bmatrix} \\;\\text{and}\\; \\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 2 \\end{bmatrix} \\] are not similar even though they have the same eigenvalues. 4.2.2 Jordan Matrix For non-diagonalizable square matrix \\(A_{n \\times n}\\), the goal is to with similar transformation \\(P^{-1}AP\\) construct a matrix that is as near to a diagonal matrix as possible. Definition 1.2 The \\(n \\times n\\) matrix \\(J_{\\lambda, n}\\) with \\(\\lambda\\)s on the diagonal, \\(1\\)s on the superdiagonal and \\(0\\)s elsewhere is called a Jordan matrix. A Jordan matrix in Jordan normal form is a block matrix that has Jordan blocks down its block diagonal and is zero elsewhere An example of Jordan matrix, the appearance of \\(\\lambda_i\\) on the diagonal is equal to its multiplicity as \\(A\\)’s eigenvalue. \\[ \\begin{bmatrix} \\lambda_1 &amp; 1 &amp; \\\\ &amp; \\lambda_1 &amp; 1 &amp; \\\\ &amp; &amp; \\lambda_1 &amp; \\\\ &amp; &amp; &amp; \\lambda_2 &amp; 1 \\\\ &amp; &amp; &amp; &amp; \\lambda_2 \\\\ &amp; &amp; &amp; &amp; &amp; \\lambda_3 &amp; 1 \\\\ &amp; &amp; &amp; &amp; &amp; &amp; \\ddots \\\\ &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\lambda_n &amp; 1 \\\\ &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\lambda_n \\end{bmatrix} \\] An illustration from wikipedia, the circled area is the Jordan block. Though the purpose of this section was not the computation details of Jordan matrices, it helps to give a concrete example. Consider \\(A\\) \\[ A = \\begin{bmatrix} 5 &amp; 4 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; -1 &amp; -1 \\\\ -1 &amp; -1 &amp; 3 &amp; 0 \\\\ 1 &amp; 1 &amp; -1 &amp; 2 \\end{bmatrix} \\] Including multiplicity, the eigenvalues of \\(A\\) are \\(\\lambda = 1, 2, 4, 4\\). And for \\(\\lambda = 4\\), the eigenspace is 1 dimensional instead of 2, meaning \\(A\\) is not diagonalizable. Nonetheless, \\(A\\) is similar to the following Jordan matrix \\[ J = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 2 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 4 &amp; 1\\\\ 0 &amp; 0 &amp; 0 &amp; 4 \\end{bmatrix} \\] To obtain \\(P\\), recall that \\(P^{-1}AP = J\\). Let \\(P\\) have column vectors \\(p_i, \\; i = 1,...,4\\), then: \\[ A[\\bar{p}_1 \\; \\; \\bar{p}_2 \\;\\; \\bar{p}_3 \\;\\; \\bar{p}_4] = [\\bar{p}_1 \\; \\; \\bar{p}_2 \\;\\; \\bar{p}_3 \\;\\; \\bar{p}_4] \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 2 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 4 &amp; 1\\\\ 0 &amp; 0 &amp; 0 &amp; 4 \\end{bmatrix} = [\\bar{p}_1 \\;\\; 2\\bar{p}_2 \\;\\; 4\\bar{p}_3 \\;\\; \\bar{p}_3 + 4\\bar{p}_4] \\] We see that \\[ \\begin{aligned} (A - 1I)\\bar{p}_1 &amp;= \\bar{0} \\\\ (A - 2I)\\bar{p}_2 &amp;= \\bar{0} \\\\ (A - 4I)\\bar{p}_3 &amp;= \\bar{0} \\\\ (A - 1I)\\bar{p}_4 &amp;= \\bar{p}_3 \\end{aligned} \\] The solutions \\(\\bar{p}_i\\) are called generalized eigenvectors of \\(A\\). 4.2.3 Simultaneous Diagonalization A diagonlizable matrix family that share the same eigenvectors is called simultaneously diagonalizable. This notion is complimentary to a family of similar matrices that are diagonalizable, share eigenvalues but not eigenvectors. Definition 4.3 (Simultaneously diagonalizable) Two diagonalizable matrices \\(A\\) and \\(B\\) are said to be simultaneously diagonalizable if a \\(n \\times n\\) matrix \\(P\\) exists, such that \\(P^{-1}AP\\) and \\(P^{-1}BP\\) are diagonal matrices. In other words \\[ A = P\\Lambda_1P^{-1} \\\\ B = P\\Lambda_2P^{-1} \\] The geometric interpretation of simultaneously diagonalizable matrices is that they perform scaling in the same set of directions. Theorem 4.2 If \\(A\\) and \\(B\\) are diagonalizable matrices, they are simultaneously diagonalizable if and only if they commute, such that \\(AB\\) = \\(BA\\). This theorem is useful in identifying diagonalizable matrices with the same eigenvectors. For example, the matrices \\[ \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} \\quad \\text{and} \\quad \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} \\] are not simultaneously diagonalizable because they do not commute. 4.2.4 Cayley-Hamilton Theorem For any square matrix \\(A_{n \\times n}\\), the characteristic polynomial of \\(\\lambda\\) is defined as \\[ \\det(A - \\lambda I) \\] We can obtain a polynomial of matrix \\(A\\) by substituting \\(A\\) for \\(\\lambda\\), and \\(kI\\) for constant terms. For example, the matrix form of the polynomial \\(3\\lambda^2 + 2\\lambda + 2\\) is \\(3A^2 + 2A + 2I\\). Theorem 4.3 (Cayley-Hamilton Theorem) Let \\(f(\\lambda)\\) be the polynomial function of the characteristic polynomial \\(\\det(A - \\lambda I)\\), where \\(A\\) is a square matrix. Then \\(f(A)\\) evaluates to a zero matrix. Proof Though the Caley Hamilton theorem 4.3 applies to any square matrix \\(A\\). Our proof only address the case for diagonalizable matrices. When \\(A\\) is diagonalizable, the polynomial of \\(A\\) takes the form \\[ f(A) = Pf(\\Lambda)P^{-1} \\] Since \\(f(\\lambda) = \\det(A - \\lambda I)\\), and the diagonal entries of \\(\\Lambda\\) are the eigenvalues of \\(A\\). Evaluate \\(f(\\lambda)\\) at each diagonal entry of \\(\\Lambda\\) will be zero. Thus \\(f(A)\\) is a zero matrix. A direct result derived from the Cayley Hamilton theorem is that for every invertible matrix \\(A\\), its inverse \\(A^{-1}\\) can be represented as a polynomial of \\(A\\) with degree \\(d - 1\\). Proposition 4.1 (Polynomial representation of matrix inverse) The inverse of an invertible square matrix \\(A\\) is a polynomial of \\(A\\) with degree at most \\(d -1\\). Since the constant term in the characteristic polynomial is the product of eigenvalues, which is nonzero for non-singular matrices. If the product of all eigenvalues are \\(k\\), we can write the Cayley-Hamilton matrix polynomial \\(f(A)\\) in the form \\(f(A) = A \\cdot g(A) + kI\\), where \\(A \\cdot g(A)\\) is obtained by factoring out \\(A\\) from the d-degree matrix polynomial, leaving \\(g(A)\\) with degree of \\(d - 1\\). Since \\(f(A)\\) evaluates to zero, we have \\[ A \\underbrace{\\Big( - g(A) / k\\Big)}_{A^{-1}} = I \\] Therefore, \\(A^{-1}\\) is shown to be a polynomial of \\(A\\). 4.3 Symmetric Matrices A square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) is symmetric if \\(A = A^{T}\\), and anti-symmetric if \\(A = - A^{T}\\). It can be shown that for any \\(A \\in \\mathbb{R}^{n \\times n}\\), \\(A + A^T\\) is symmetric and \\(A - A^T\\) anti-symmetric. So any square matrix \\(A\\) can be wrote as a sum of a symmetric matrix and an anti-symmetric matrix \\[ A = \\frac{1}{2}(A + A^T) + \\frac{1}{2}(A - A^T) \\] It is common to denote the set of all symmetric matrices of size \\(n\\) as \\(\\mathbb{S}^n\\), and \\(A \\in \\mathbb{S}^n\\) means \\(A\\) is a symmetric \\(n \\times n\\) matrix. Symmetric matrices have some nice properties about diagonalization. Theorem 1.7 If \\(A\\) is symmetric, eigenvectors from distinct eigenvalues are orthogonal. Proof Let \\(\\bar{v}_1\\) and \\(\\bar{v}_2\\) be eigenvectors that correspond to distinct eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\). Compute \\[ \\begin{split} \\lambda_1\\bar{v}_1 \\cdot \\bar{v}_2 &amp;= (\\lambda_1\\bar{v}_1)^T\\bar{v}_2 \\\\ &amp;= (\\bar{v}_1^TA^T)\\bar{v}_2 \\\\ &amp;= \\bar{v}_1^T(A\\bar{v}_2) \\\\ &amp;= \\bar{v}_1^T(\\lambda_2\\bar{v}_2) \\\\ &amp;= \\lambda_2\\bar{v}_1 \\cdot \\bar{v}_2 \\end{split} \\] because \\(\\lambda_1 \\not = \\lambda_2\\), \\(\\bar{v}_1 \\cdot \\bar{v}_2 = 0\\). For symmetric matrices \\(A \\in \\mathbb{R}^{n \\times n}\\) without \\(n\\) distinct eigenvalues, it turns out that the dimension of the eigenspace for each \\(\\lambda_k\\) always equals the multiplicity of \\(\\lambda_k\\). For this reason, if \\(A\\) is a symmetric matrix we can always construct a orthonormal set \\(\\{\\bar{q}_1 \\;\\; \\cdots \\;\\; \\bar{q}_n\\}\\) from \\(\\{\\bar{v}_1 \\;\\; \\cdots \\;\\; \\bar{v}_n\\}\\) such that \\[ Q^{T} = \\begin{bmatrix} \\bar{q}_1^T \\\\ \\vdots \\\\ \\bar{q}_n^T \\end{bmatrix} = Q^{-1} \\] Recall that matrix \\(A\\) with \\(n\\) linearly independent eigenvectors is diagonalizable and can be written as \\[ A = P \\Lambda P^{-1} \\] where \\(P = [\\bar{v}_1 \\;\\; \\cdots \\;\\; \\bar{v}_n]\\) and \\(\\Lambda\\) is a diagonal matrix with eigenvalues on its diagonal entries. With symmetric matrices, \\(\\{\\bar{v}_1, \\cdots, \\bar{v}_n\\}\\) must be linearly independent and can be transformed into a orthonormal basis \\(\\{\\bar{q}_1, \\cdots, \\bar{q}_n\\}\\). With orthogonal matrix \\(Q =[\\bar{q}_1 \\;\\; \\cdots \\;\\; \\bar{q}_n]\\), we have \\[\\begin{equation} \\tag{4.2} A = Q \\Lambda Q^{T} \\end{equation}\\] Such matrix \\(A\\) is said to be orthogonally diagonalizable. We have seen that for symmetric matrix \\(A\\), Eq (4.2) always holds. We can also also verify that if \\(A\\) is orthogonally diagonalizable then it is a symmetric matrix \\[ A^T = (Q \\Lambda Q^{T})^T = (Q^T)^T\\Lambda^TQ^T= Q \\Lambda Q^{T} = A \\] Theorem 1.8 An \\(n \\times n\\) matrix \\(A\\) is orthogonally diagonalizable if an only if \\(A\\) is a symmetric matrix. 4.3.1 Spectral Decomposition For orthogonally diagonalizable matrix \\(A\\), we have \\[ A = Q \\Lambda Q^{T} = [\\bar{q}_1 \\;\\; \\cdots \\;\\; \\bar{q}_n] \\begin{bmatrix} \\lambda_1 &amp; &amp; \\\\ &amp; \\ddots \\\\ &amp; &amp; \\lambda_n \\end{bmatrix} \\begin{bmatrix} \\bar{q}_1^T \\\\ \\vdots \\\\ \\bar{q}_n \\end{bmatrix} \\] It follows that \\[\\begin{equation} \\tag{4.3} A = \\lambda_1\\bar{q}_1\\bar{q}_1^T + \\cdots + \\lambda_1\\bar{q}_n\\bar{q}_n^T \\end{equation}\\] Eq (4.3) is called the spectral decomposition, breaking \\(A\\) into pieces of rank 1 matrix. It got this name because he set of eigenvalues of a matrix \\(A\\) is sometimes called its spectrum. 4.3.2 A-Orthogonality Definition 4.4 (A-Orthogonality) Column vector \\(\\bar{v}_i\\) and \\(\\bar{v}_j\\) are said to be A-orthogonal if \\(\\bar{v}_i^TA\\bar{v}_j = 0\\) for some \\(n \\times n\\) invertible matrix \\(A\\). Similarly, a set of column vectors \\(\\bar{v}_1, ..., \\bar{v}_n\\) is A-orthogonal, if and only if \\(\\bar{v}_i^TA\\bar{v}_i = 0\\) for each pair of vectors. 4.4 Quadratic Forms Definition 4.5 (Quadratic form) A quadratic form on \\(\\mathbb{R}^n\\) is a function \\(Q\\) defined on \\(\\mathbb{R}^n\\) whose value at a vector \\(\\bar{x}\\) in \\(\\mathbb{R}^n\\) can be computed by an expression of the form \\(Q(\\bar{x}) = \\bar{x}^TA\\bar{x}\\), where \\(A \\in \\mathbb{R}^{n \\times n}\\) is a symmetric matrix. \\(A\\) is called the matrix of the quadraticc form. There exists a one-to-one mapping between symmetric matrix \\(A\\) and the quadratic form. Consider the \\(3 \\times 3\\) case: \\[ \\bar{x} = \\begin{bmatrix} x_1 \\\\ x_3 \\\\ x_3 \\\\ \\end{bmatrix} , \\;\\; A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{bmatrix} \\\\ \\] \\[ \\begin{split} \\bar{x}^TA\\bar{x} &amp;= [x_1 \\;\\; x_2 \\;\\; x_3] \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_3 \\\\ x_3 \\\\ \\end{bmatrix} \\\\ &amp;= a_{11}x_1^2 + a_{22}x_2^2 + a_{33}x_3^2 + \\\\ &amp; \\quad(a_{12} + a_{21})x_1x_2 + (a_{13} + a_{31})x_1x_3 + (a_{23} + a_{32})x_2x_3 \\end{split} \\tag{1} \\] Since \\(A\\) is symmetric, we have \\(a_{ij} = a_{ji}\\), thus \\[ \\bar{x}^TA\\bar{x} = a_{11}x_1^2 + a_{22}x_2^2 + a_{33}x_3^2 + 2a_{12}x_1x_2 + 2a_{13}x_1x_3 + 2a_{23}x_2x_3 \\tag{2} \\] This verifies that \\(\\bar{x}^TA\\bar{x}\\) when \\(A \\in \\mathbb{R}^{n \\times n}\\) is symmetric does result in a quadratic function of \\(n\\) variables. Conversely, any quadratic function of \\(n\\) variables, like shown in \\((2)\\), can be expressed in terms of \\(\\bar{x}^TA\\bar{x}\\) with unique choice of symmetric matrix \\(A \\in \\mathbb{R}^{n \\times n}\\). 4.4.1 Change of Variable If \\(\\bar{x}\\) is a variable vector in \\(\\mathbb{R}^n\\), then a change of variable is an equation of the form \\[ \\begin{aligned} \\bar{x} &amp;= P\\bar{y} \\\\ \\text{or equivalently} \\quad \\bar{y} &amp;= P^{-1}\\bar{x} \\end{aligned} \\] where \\(P\\) is any invertible matrix \\(\\in \\mathbb{R}^{n \\times n}\\) Theorem 4.4 (The Principal Axes Theorem) Let \\(A\\) be an \\(n \\times n\\) symmetric matrix. Then there exists an orthogonal change of variable, \\(\\bar{x} = Q\\bar{y}\\), this transform the quadratic form \\(\\bar{x}^TA\\bar{x}\\) into a quadratic form \\(\\bar{y}^T\\Lambda\\bar{y}\\) with no cross-product term. \\(Q\\) is constructed with \\(A\\)’s orthonormal eigenvectors \\(\\bar{q}_1, ..., \\bar{q}_n\\). According to theorem (4.2): \\[ \\bar{x}^TA\\bar{x} = (Q\\bar{y})^TA(Q\\bar{y}) = \\bar{y}^TQ^{T}AQ\\bar{y} = \\bar{y}^T \\Lambda \\bar{y} \\] The principal axes theorem 4.4 shows that if \\(A\\) is diagonalizable, quadratic form \\(\\bar{x}^TA\\bar{x}\\) can be reexpressed into the form \\(\\lambda_1y_1^2 + \\lambda_2y_2^2 + \\cdots + \\lambda_ny_n^2\\) with change of variables \\(\\bar{x} = Q\\bar{y}\\). 4.4.2 Classification of Quadratic Forms A symmetric matrix \\(A \\in \\mathbb{S}^n\\) is positive definite (PD) if for all non-zero vectors \\(\\bar{x} \\in \\mathbb{R}^n,\\; \\bar{x}^TA\\bar{x} &gt; 0\\). We can denote positive definite matrix \\(A\\) as \\(A \\succ 0\\) (or \\(A &gt; 0\\)). The set of all positive definite matrices is denoted as \\(\\mathbb{S}_{++}^n\\) A symmetric matrix \\(A \\in \\mathbb{S}^n\\) is positive semidefinite (PSD) if for all non-zero vectors \\(\\bar{x} \\in \\mathbb{R}^n,\\; \\bar{x}^TA\\bar{x} \\ge 0\\). We can denote positive definite matrix \\(A\\) as \\(A \\succeq 0\\) (or \\(A \\ge 0\\)). The set of all positive semidefinite matrices is denoted as \\(\\mathbb{S}_{+}^n\\) A symmetric matrix \\(A \\in \\mathbb{S}^n\\) is negative definite (ND), denoted by \\(A \\prec 0\\) (or \\(A &lt; 0\\)), if for all non-zero vectors \\(\\bar{x} \\in \\mathbb{R}^n,\\; \\bar{x}^TA\\bar{x} &lt; 0\\). Similarly, a symmetric matrix \\(A \\in \\mathbb{S}^n\\) is negative semidefinite (NSD), denoted by \\(A \\preceq 0\\) (or \\(A \\le 0\\)), if for all non-zero vectors \\(\\bar{x} \\in \\mathbb{R}^n,\\; \\bar{x}^TA\\bar{x} \\le 0\\). Finally, a symmetric matrix \\(A \\in \\mathbb{S}^n\\) is indefinite, if it is neither positive semidefinite or negative semidefinite. In other words, if there exists \\(\\bar{x}, \\bar{x}&#39;, \\in \\mathbb{R}^{n}\\) such taht \\(\\bar{x}^TA\\bar{x} &gt; 0\\) and \\(\\bar{x&#39;}^TA\\bar{x}&#39; &gt; 0\\) Note that when talking about \\(A\\) being PD, PSD, ND, NSD or indefinite, \\(A\\) is always assumed to be symmetric. Also, if \\(A\\) is positive definite, then \\(−A\\) is negative definite and viceversa. Likewise, if \\(A\\) is positive semidefinite then \\(−A\\) is negative semidefinite and vice versa. If \\(A\\) is indefinite, then so is \\(−A\\). From theorem 4.4, we know that the sign of eigenvalues are closely related to classifications of symmetric matrices here. Take positive definite matrices for example, the following statements of \\(A\\) are equivalent: For any \\(\\bar{x} \\in \\mathbb{R}^n, \\; \\bar{x}^TA\\bar{x} &gt; 0\\) Let \\(\\lambda_i, \\; i = 1, ..., n\\) be \\(A\\)’s eigenvalues, \\(\\lambda_i &gt; 0\\) All pivots are \\(&gt; 0\\) All leading determinants of \\(A &gt; 0\\) For the last criterion, leading determinant is the determinant of the top-left \\(k \\times k\\) lock of \\(A\\) for all \\(1 \\le k \\le n\\). There is formal theorem on this Theorem 4.5 (Sylvester’s Criterion) A symmetric matrix \\(A\\) is positive definite if and only if the determinant of the top-left \\(k \\times k\\) block of \\(A\\) is strictly positive for all \\(1 \\le k \\le n\\) Sylvester’s criterion is more complicated for positive semidefinite matrices, we have to check more matrices rather than just the top-left one. Classification of \\(A \\in \\mathbb{S}^{n}\\) by its eigenvalue can be applied in general. Theorem 4.6 (Quadratic forms and eigenvalues) Let \\(A \\in \\mathbb{S}^{n}\\). Then the quadratic form \\(\\bar{x}^TA\\bar{x}\\) and \\(A\\) is: positive definite if and only if the eigenvalues of \\(A\\) are all positive negative definite if and only if the eigenvalues of \\(A\\) are all negative indefinite if and only if \\(A\\) has both positive and negative eigenvalues Corollary 4.3 Given positive definite matrices \\(A, B \\in \\mathbb{S}^n\\) and \\(\\alpha \\in \\mathbb{R}\\), the following results remain to be positive definite. Scalar multiplication of PD matrices \\(\\alpha A\\) are PD matrices The sum of PD matrices \\(A +B\\) are PD matrices If a PD matrix is invertible, its inverse \\(A^{-1}\\) is also PD. Similar matrix of a PD matrix is PD. Corollary 4.4 Given any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(A^TA\\) and \\(AA^T\\) are always positive semidefinite matrices Proof By definition, \\(A^TA\\) is a positive semidefinite matrix if for any \\(\\bar{x} \\in \\mathbb{R}^n\\), the quadratic form \\(\\bar{x}^T(A^TA)\\bar{x} \\ge 0\\). \\[ \\begin{split} \\bar{x}^T(A^TA)\\bar{x} &amp;= (\\bar{x}^TA^T)(A\\bar{x}) \\\\ &amp;= (A\\bar{x})^T(A\\bar{x}) \\\\ &amp;= \\|A\\bar{x}\\|^2 \\end{split} \\] It turns out that the result is the square of the 2-norm of \\(A\\bar{x}\\) (nonnegative). This also tells \\(A^TA\\) is positive definite when \\(\\bar{x} \\not\\subseteq \\mathcal{N}(A)\\) Similarly, the quadratic form for \\(AA^T\\) can be refactored in to the standard norm of \\(A^T\\bar{x}\\). Corollary 4.5 \\(A^TA +\\lambda I\\) and \\(AA^T + \\lambda I\\) are always positive definite and invertible for \\(\\lambda &gt; 0\\) Proof From the previous corollary 4.4 we know that \\(A^TA\\) and \\(AA^T\\) are positive semidefinite, and that they have the same nonzero eigenvalues from corollary 4.1. According to Section 4.1.1, eigenvalues for \\(P(A)\\) are \\(P(\\lambda)\\) for polynomial function \\(P\\). Therefore, \\(A^TA +\\lambda I\\) and \\(AA^T + \\lambda I\\) share a positive set of \\(n\\) eigenvalues \\(\\lambda_1 + \\lambda, ..., \\lambda_r + \\lambda, \\lambda, ..., \\lambda\\), so they are PD and invertible. 4.4.3 Gershgorin Discs and Diagonal Dominance This section introduces one more criterion for positive definite matrices. We first present the Gershgorin Disc Theorem, which is more convenient to be expressed in terms of complex matrices. Theorem 4.7 (Gershgorin Disc Theorem) Let \\(A \\in \\mathcal{M}_n(\\mathbb{C})\\) and define the following objects \\(r_i = \\sum_{j \\not = i}|a_{ij}|\\) (the sum of the off-diagonal entries of the \\(i\\)-th row of \\(A\\)) \\(D(a_{ii}, r_i)\\) is the closed disc in the complex plane centered at \\(a_{ii}\\) with radius \\(r_i\\). Then every eigenvalue of \\(A\\) is in at least one of the \\(D(a_{ii}, r_i)\\) (called Gershigorin discs) As an illustration taken from Wikipedia, the following figure presents 4 Gershgorin discs of the matrix \\[ \\begin{bmatrix} 10 &amp; -1 &amp; 0 &amp; 1 \\\\ 0.2 &amp; 8 &amp; 0.2 &amp; 0.2 \\\\ 1 &amp; 1 &amp; 2 &amp; 1 \\\\ -1 &amp; -1 &amp; -1 &amp; -11 \\end{bmatrix} \\] In this case, all elements and eigenvalues are real. Therefore, Gershgorin discs are circles centered at the real number line. Proof Let \\(\\lambda\\) be an eigenvalue of \\(A\\) with associated eigenvector \\(\\bar{v}\\). Suppose the largest entry of \\(\\bar{v}\\) is \\(v_i\\), we scale \\(\\bar{v}\\) such that \\(v_i = 1\\) and all other elements less than \\(1\\). Note \\(\\lambda\\) and \\(i\\) are arbitrary, for example, the associated eigenvector of the 3rd eigenvalue may have the largest entry at \\(v_5\\). By the definition of matrix multiplication, we know \\([A \\bar{v}]_i = \\sum_{j = 1}^{n}a_{ij}v_j = a_{ij}v_i + \\sum_{j \\not = i}a_{ij}v_j\\). Also \\(\\lambda\\) is an eigenvalue and \\(v_i = 1\\), thus \\([A\\bar{v}]_i = \\lambda v_ia\\). Then \\[ a_{ij}\\underbrace{v_i}_{1} + \\sum_{j \\not = i}a_{ij}v_j = \\lambda\\underbrace{v_i}_{1} \\] \\[ \\lambda - a_{ii} = \\sum_{j \\not = i}a_{ij}v_j \\] Take absolute values on both sides and use the triangle equality \\[ |\\lambda - a_{ii}| = |\\sum_{j \\not = i}a_{ij}v_j| \\le \\sum_{j \\not = i} |a_{ij}||v_j| \\qquad (v_j \\le 1) \\] \\[ |\\lambda - a_{ii}| \\le \\sum_{j \\not = i} |a_{ij}| = r_i \\] In other words, for any eigenvalue \\(\\lambda\\), the distance between \\(\\lambda\\) and \\(a_{ii}\\) is less than \\(r_i\\), where \\(i\\) is the index at which \\(\\lambda\\)’s associated eigenvector take its largest value. For diagonal entries, all \\(r_i = 0\\), thus their eigenvalues are exactly diagonal entries. In cases where the off diagonal entries of \\(A\\) are very small, then \\(A\\)’s eigenvalue will be very close to diagonal entries. This leads to the notion of diagonal dominance. Theorem 4.8 (Diagonal dominant matrix) Suppose that \\(A \\in \\mathcal{M}_n{(\\mathbb{C})}\\), then \\(A\\) is called diagonally dominant if \\(|a_{ii}| \\ge \\sum_{j \\not = i}|a_{ij}|\\) for all \\(1 \\le i \\le n\\) and strictly diagonally dominant if \\(|a_{ii}| &gt; \\sum_{j \\not = i}|a_{ij}|\\) for all \\(1 \\le i \\le n\\) A strictly diagonally dominant matrix is non-singular, because all of its Gershgorin discs do not touch zero. This result is known the Levy-Desplanques theorem Corollary 4.6 (non-negative diagonal entries and diagonal dominance implies PSD) Suppose that \\(A \\in \\mathcal{M}_n{(\\mathbb{C})}\\) has non-negative diagonal entries If \\(A\\) is diagonally dominant then it is positive semidefinite If \\(A\\) is strictly dominant then it is positive definite Note that this is a one-way theorem, unlike the criterion using eigenvalues, pivots and leading matrices. A PD matrix may not be diagonally dominant. 4.5 Cholesky Factorization Lemma 4.1 A symmetric matrix \\(A \\in \\mathbb{S}^n\\) is positive semidefinite if and only if it can be expressed in the gram matrix form \\(B^TB\\) of some matrix \\(B\\). The previous corollary 4.4 shows that if \\(A = B^TB\\) then it is positive definite. Conversely, if \\(A\\) is PSD (or PD), we have \\(A = Q \\Lambda Q^T\\) where \\(\\Lambda\\)’s diagonal entries are all nonnegative. Then we can set \\(\\Lambda ^{1/2} = \\Sigma\\) and \\(B = (Q\\Sigma)^T\\). Then \\(A = Q\\Sigma^2Q^T = (Q\\Sigma)(Q\\Sigma)^T = B^TB\\). Note that we could also have stated this lemma using \\(BB^T\\) instead of \\(B^TB\\), and the proof is similar. This lemma is inspiring in that for every PD matrix \\(A \\in \\mathbb{R}^n\\), there exists factorization \\(A = BB^T\\). In fact, this factorization is not unique. We can use an orthogonal matrix \\(Q\\) to create an additional orthogonal factorization \\(A = BB^T = B(QQ^T)B^T = (BQ)(BQ)^T\\). And let \\(BQ\\) be our new \\(B\\), we get another factorization. Since the initial \\(B = P\\Lambda^{1/2}\\) is full rank, with an appropriate choice of \\(Q\\), we can turn the \\(PB\\) into an lower triangular matrix \\(L\\) such that \\(A = LL^T\\). This is essentailly a coordinate transformation in Section 2.6.1 The uniqueness of this factorization with lower triangular matrix \\(L\\) can be proved with induction. The decomposition of PD matrices into the product of an lower triangular matrix and its transpose is called the Cholesky factorization, in this factorization \\[ A = LL^T \\] To make it more clear \\[ \\begin{bmatrix} a_{11} &amp; \\cdots &amp; a_{1n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; \\cdots &amp; a_{nn} \\end{bmatrix} = \\begin{bmatrix} l_{11} &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ l_{n1} &amp; \\cdots &amp; l_{nn} \\end{bmatrix} \\begin{bmatrix} l_{11} &amp; \\cdots &amp; l_{n1} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; l_{nn} \\end{bmatrix} \\] Since \\(L\\) is lower triangular, we can solve \\(L\\) from \\(A = LL^T\\) with a system of equations that can be easily solved using back-substitution. For example, \\(l_{11} = \\sqrt{a_{11}}\\), and \\(a_{i1} / l_{11}\\). The Cholesky factorization is a special case of LU decomposition 4.6 Rayleigh Quotients Let \\(A \\in \\mathbb{S}^n\\) and \\(\\bar{x} \\in \\mathbb{R}^n\\), Rayleigh quotient is defined as \\[ R_{A}(\\bar{x}) = \\frac{\\bar{x}^TA\\bar{x}}{\\bar{x}^T\\bar{x}} \\] The Rayleigh quotient has some nice properties: scale invariance: for any vector \\(\\bar{x} \\not= 0\\) and any scalar \\(\\alpha \\not= 0\\), \\(R_{A}(\\bar{x}) = R_{A}(\\alpha\\bar{x})\\) If \\(\\bar{x}\\) is a eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\), then \\(R_{A}(\\bar{x}) = \\lambda\\) The Rayleigh quotient is bounded by the largest and smallest eigenvalue of \\(A\\), i.e. \\[ \\lambda_{\\text{min}}(A) \\le R_{A}(\\bar{x}) \\le \\lambda_{\\text{max}}(A) \\] Proof Since the Rayleigh quotient does not depend on the 2-norm of vector \\(\\bar{x}\\), we may assume a unit vector \\(\\bar{x}^T\\bar{x} = 1\\), and Rayleigh quotient simplifies to the quadratic form \\(\\bar{x}^TA\\bar{x}\\). Next, orthogonally diagonalize \\(A\\) as \\(Q \\Lambda Q\\), we know that when \\(\\bar{x} = Q \\bar{y}\\): \\[ \\bar{x}^TA\\bar{x} = \\bar{y}^T \\Lambda \\bar{y} \\tag{1} \\] Also \\[ 1= \\bar{x}^T\\bar{x} = (Q\\bar{y})^T Q\\bar{y} = \\bar{y}^TQ^TQ\\bar{y} = \\bar{y}^T\\bar{y} \\tag{2} \\] Expand \\(\\bar{y}^T \\Lambda \\bar{y}\\) in (1) we get \\[ \\bar{x}^TA\\bar{x} = \\lambda_1y_1^2 + \\lambda_2y_2^2 + \\cdots + \\lambda_ny_n^2 \\tag{3} \\] where \\(\\{\\lambda_1, ..., \\lambda_n\\}\\) are diagonal entries of \\(\\Lambda\\) and eigenvalues of \\(A\\). Let us suppose that the set \\(\\{\\lambda_1, \\lambda_2, \\cdots, \\lambda_n\\}\\) has already been ordered descendingly, so that \\(\\lambda_1 &gt; \\lambda_2 &gt; \\cdots &gt; \\lambda_n\\). We can obtain the inequality from (3) and the order of eigenvalues: \\[ \\begin{split} \\bar{x}^TA\\bar{x} &amp;= \\lambda_1y_1^2 + \\lambda_2y_2^2 + \\cdots + \\lambda_ny_n^2 \\\\ &amp;\\le \\lambda_1y_1^2 + \\underbrace{\\lambda_1y_2^2 + \\cdots + \\lambda_1y_n^2}_{\\lambda_1 \\text{ is the greatest eigenvalue}} \\\\ &amp;\\le \\lambda_1(\\bar{y}^T\\bar{y}) \\\\ &amp;= \\lambda_1 \\end{split} \\] The equation reach equality if and only if \\([y_1, y_2, \\cdots, y_n] = [1, 0, \\cdots, 0]\\). Since \\(\\bar{x} = Q\\bar{y}\\), we have \\[ \\bar{x} = \\begin{bmatrix} \\bar{q}_1 &amp; \\bar{q}_2 &amp; \\cdots &amp; \\bar{q}_n \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} = \\bar{q}_1 \\] Similarly, the minimum of the Rayleigh quotient will be \\(\\lambda_n\\), with \\(\\bar{x} = \\bar{q}_n\\). From the optimization perspective, the bound of Rayleigh quotient amounts to a constrained optimization problem \\[ \\begin{aligned} \\text{objective function} &amp;: \\bar{x}^TA\\bar{x}\\\\ \\text{subject to}&amp;: \\bar{x}^T\\bar{x} = 1 \\end{aligned} \\] The maximum and minimum of the objective function are \\(\\lambda_1\\) and \\(\\lambda_n\\), with \\(\\bar{x}\\) being \\(\\bar{q}_1\\) and \\(\\bar{q}_n\\) respectively. If we add more constraints, for example, that \\(\\bar{x}\\) should be orthogonal to \\(\\bar{q}_1\\), then \\(\\bar{x}^TA\\bar{x}\\) has maximum \\(\\lambda_2\\) attained at \\(\\bar{x} = \\lambda_2\\) Theorem 4.9 Let \\(A \\in \\mathbb{S}^n\\) with orthogonal diagonalization \\(A = Q\\Lambda Q^T\\), where the entries on the diagonal of \\(\\Lambda\\) are arranged so that \\(\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_n\\). Then for \\(k = 2, ...\\), the maximum of value of \\(\\bar{x}^T A \\bar{x}\\) subject to constraints \\[ \\bar{x}^T\\bar{x} = 1, \\;\\; \\bar{x}^T\\bar{q}_1 = 0, \\;\\; \\dots \\;\\;, \\bar{x}^T\\bar{q}_{k-1} = 0 \\] is the eigenvalue \\(\\lambda_k\\), and this maximum is attained at \\(\\bar{x} = \\bar{q}_k\\) Proof From \\(\\bar{x} = P\\bar{y}\\) we know that \\[ \\bar{x} = y_1\\bar{q}_1 + \\cdots + + y_{k-1}\\bar{q}_{k-1} + y_k\\bar{q}_k + \\cdots + y_{n}\\bar{q}_n \\] Left multiply by \\(\\bar{q}_1^T\\) \\[ \\begin{aligned} \\bar{q}_1^T\\bar{x} &amp;= y_1\\bar{q}_1^T\\bar{q}_1 + \\cdots + + y_{k-1}\\bar{q}_1^T\\bar{q}_{k-1} + y_k\\bar{q}_1^T\\bar{q}_k + \\cdots + y_{n}\\bar{q}_1^T\\bar{q}_n \\\\ &amp;= y_1\\bar{q}_1^T\\bar{q}_1 \\\\ &amp;= y_1 \\end{aligned} \\] Since \\(\\bar{q}_1^T\\bar{x} = \\bar{x}^T\\bar{q}_1 = 0\\), we have \\(y_1 = 0\\). Similarly, \\(y_2 = \\cdots = y_{k-1} = 0\\), and \\(\\bar{y}\\) becomes \\([0 \\;\\; \\cdots \\;\\; 0 \\;\\; y_{k} \\;\\; \\cdots \\;\\; y_n]\\). And the inequality now becomes: \\[ \\begin{split} \\bar{x}^TA\\bar{x} &amp;= \\lambda_1y_1^2 + \\lambda_2y_2^2 + \\cdots + \\lambda_ny_n^2 \\\\ &amp;= \\lambda_ky_k^2 + \\cdots + \\lambda_ny_n^ 2 \\\\ &amp;\\le \\lambda_ky_k^2 + \\cdots + \\lambda_ky_n^2 \\\\ &amp;\\le \\lambda_k(\\bar{y}^T\\bar{y}) \\\\ &amp;= \\lambda_k \\end{split} \\] It’s easy to see that \\(\\bar{x}^TA\\bar{x}\\) gets its maximum \\(\\lambda_k\\) when \\(y_k = 0\\) and other weights being zero. So the solution \\(\\bar{x}\\) can be solved as \\[ \\begin{split} \\bar{x} &amp;= \\begin{bmatrix} \\bar{q}_1 &amp; \\cdots &amp; \\bar{q}_k &amp; \\bar{q}_{k+1} &amp; \\cdots &amp;\\bar{q}_n \\end{bmatrix} \\begin{bmatrix} 0 \\\\ \\vdots \\\\ \\underbrace{1}_{k\\text{th weight}} \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\\\ &amp;= \\bar{q}_k \\end{split} \\] "],
["singular-value-decomposition.html", "Chapter 5 Singular Value Decomposition 5.1 Singular Values 5.2 SVD 5.3 Matrix Norms 5.4 Low Rank Approximation", " Chapter 5 Singular Value Decomposition 5.1 Singular Values The singular value decomposition illustrates a way of decomposing any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) into the form \\(U \\Sigma V^T\\), where \\(U = [\\bar{u}_1 \\;\\; \\cdots \\;\\; \\bar{u}_n]\\) and \\(V = [\\bar{v}_1 \\;\\; \\cdots \\;\\; \\bar{v}_n]\\) are both orthogonal matrices, and \\(\\Sigma\\) a diagonal matrix with entries being the square root of the eigenvalues of \\(A^TA\\) (perhaps plus some zeros). Before proceeding to the theorem, let’s explore the motivating idea behind SVD. For (square) diagonalizable matrix \\(A \\in \\mathbb{S}^{n}\\), the absolute value of the eigenvalues measure the amounts that \\(A\\) stretches or shrinks eigenvectors, consider the ratio between the length of \\(\\bar{x}\\) before and after left multiplied by \\(A\\) \\[ \\frac{\\|A\\bar{x}\\|}{\\|\\bar{x}\\|} = \\frac{\\|\\lambda\\bar{x}\\|}{\\|\\bar{x}\\|} = \\frac{\\lambda\\|\\bar{x}\\|}{\\|\\bar{x}\\|} = \\lambda \\] If \\(\\lambda_1\\) is the greatest eigenvalue, then the corresponding eigenvector \\(\\bar{v}_1\\) identifies the direction in which \\(A\\)’s stretching effect is greatest. So, the question is, can we identify a similar ratio and direction for rectangular matrices \\(A \\in \\mathbb{R}^{m \\times n}\\), even though they does not have eigenvalues and eigenvectors? The answer is yes. Note that maximize \\(\\frac{\\|A\\bar{x}\\|}{\\|\\bar{x}\\|}\\) (now \\(\\bar{x}\\) is any vector \\(\\in \\mathbb{R}^n\\)) is equivalent to maximize \\(\\frac{\\|A\\bar{x}\\|^2}{\\|\\bar{x}\\|^2}\\) \\[ \\begin{split} \\frac{\\|A\\bar{x}\\|^2}{\\|\\bar{x}\\|^2} &amp;= \\frac{(A\\bar{x})^T(A\\bar{x})}{\\bar{x}^T\\bar{x}} \\\\ &amp;= \\frac{\\bar{x}^T(A^TA)\\bar{x}}{\\bar{x}^T\\bar{x}} \\end{split} \\] Since \\(A^TA\\) is symmetric, this is the form of a Rayleigh quotients 4.6! We know that the largest possible value is of this quotient \\(\\lambda_1\\), the greatest eigenvalue of \\(A^TA\\), with \\(\\bar{x} = \\bar{v}_1\\), among the orthonormal set \\(\\{\\bar{v}_1, \\cdots, \\bar{v}_n\\}\\). Note that here \\(V = [\\bar{v}_1 \\;\\; \\cdots \\;\\; \\bar{v}_n]\\) is already a orthogonal matrix, previously denoted by \\(Q\\). To sum up, the greatest possible stretching ratio of \\(A \\in \\mathbb{R}^{m \\times n}\\) on a vector \\(\\bar{x} \\in \\mathbb{R}^n\\) is \\(\\sqrt{\\lambda_1}\\). Generally, let \\(\\{\\bar{v}_1, \\cdots, \\bar{v}_n\\}\\) be a orthonormal basis for \\(\\mathbb{R}^n\\) consisting of eigenvectors of \\(A^TA\\), and \\(\\lambda_1, ..., \\lambda_n\\) be the eigenvalues of \\(A^TA\\), for \\(i = 1, \\cdots, n\\) \\[ \\|A\\bar{v}_i\\| ^ 2 = \\bar{v}_i^T(A^TA)\\bar{v}_i = \\lambda_i\\bar{v}_i^T\\bar{v}_i = \\lambda_i \\] From corollary 4.4, we know that \\(A^TA\\) are positive semidefinite matrices. Thus, \\(\\lambda_i \\ge 0, \\, i = 1, ..., n\\), and we can find their square root \\(\\sigma_i = \\sqrt{\\lambda_i}\\). Definition 5.1 (Singular values) The singular values of \\(A\\) are the square roots of the eigenvalues of \\(A^TA\\), denoted by \\(\\sigma_1, ..., \\sigma_n\\). That is, \\(\\sigma_i = \\sqrt{\\lambda_i}\\), and they are often arranged in descending order so that \\(\\lambda_1 \\ge \\cdots \\ge \\lambda_n\\). Geometrically, singular values of \\(A\\) are the length of the vectors \\(A\\bar{v}_1, ..., A\\bar{v}_n\\), where \\(\\{\\bar{v}_1, ..., \\bar{v}_n\\}\\) is the orthonormal basis of \\(A^TA\\)’s eigenspace. Theorem 5.1 Proceeding from previous definitons of singular values, and suppose \\(A\\) has at least one nonzero singular values. Then \\(\\{A\\bar{v}_1, ..., A\\bar{v}_r\\}\\) is an orthogonal basis for \\(\\mathcal{R}(A)\\), and \\(\\text{rank} \\;A = r\\) Proof First, let’s examine that \\(\\{A\\bar{v}_1, ..., A\\bar{v}_r\\}\\) is a orthogonal basis: any pair of two distinct vectors \\(A\\bar{v}_i, A\\bar{v}_j, \\; i,j = 1, ..., r\\) are orthogonal to each other \\[ \\begin{split} (A\\bar{v}_i)^T(A\\bar{v}_j) &amp;= \\bar{v}_i^TA^TA\\bar{v}_j \\\\ &amp;= \\bar{v}_i^T(\\lambda_j\\bar{v}_j) \\\\ &amp;= 0 \\end{split} \\] Next, we show that any vector in \\(\\mathcal{R}(A)\\) is a linear a combination of \\(\\{A\\bar{v}_1, ..., A\\bar{v}_r\\}\\). Note that \\(\\{\\bar{v}_1, ..., \\bar{v}_n\\}\\) is a orthonormal basis of \\(A^TA\\)’s eigenspace \\(\\mathbb{R}^n\\). Therefore, for any vector \\(\\bar{y} = A\\bar{x}\\) in \\(\\mathcal{R}(A)\\) , there exists \\(\\bar{x} = c_1\\bar{v}_1 + \\cdots + c_n\\bar{v}_n\\), thus \\[ \\begin{split} \\bar{y} &amp;= A\\bar{x} = A(c_1\\bar{v}_1 + \\cdots + c_n\\bar{v}_n) \\\\ &amp;= c_1 A \\bar{v}_1 + \\cdots + c_r A \\bar{v}_r + c_{r+1} A \\bar{v}_{r+1} + \\cdots + c_n A \\bar{v}_n \\end{split} \\tag{1} \\] Since \\(\\lambda_{r+1} = \\cdots = \\lambda_{n} = 0\\), \\(A\\bar{v}_{r+1}, ..., A\\bar{v}_{n}\\) have length \\(0\\): they are zero vectors. And (1) is reduced to \\[ \\bar{y} = c_1 A \\bar{v}_1 + \\cdots + c_r A\\bar{v}_r \\] Thus any \\(\\bar{y} \\in \\mathcal{R}(A)\\) is in Span\\(\\{A\\bar{v}_1, ..., A\\bar{v}_r\\}\\), and \\(\\{A\\bar{v}_1, ..., A\\bar{v}_r\\}\\) is an orthogonal basis for \\(\\mathcal{R}(A)\\). This also shows that the column rank of \\(A\\) is equal to its number of nonzero singular values. 5.2 SVD Let’s begin SVD by the \\(m \\times n\\) diagonal matrix \\(\\Sigma\\) of the form \\[ \\Sigma = \\begin{bmatrix} \\sigma_1 \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\sigma_r \\\\ &amp; &amp; &amp; 0 \\\\ &amp; &amp; &amp; &amp; \\ddots \\\\ &amp; &amp; &amp; &amp; &amp; 0 \\\\ \\end{bmatrix} \\tag{1} \\] There are \\(r\\) nonzero entries on the diagonal, being \\(A\\)’s nonzero singular values, and the left positions are filled by \\(0\\) to form a \\(m \\times n\\) matrix. If \\(r\\) equals \\(m\\) or \\(n\\) or both, some or all of the zero blocks do not appear. Theorem 5.2 (The Singular Value Decomposition) Let \\(A \\in \\mathbb{R}^{m \\times n}\\) with rank \\(r\\). There exists an diagonal matrix \\(\\mathbb{\\Sigma} \\in \\mathbb{R}^{m \\times n}\\) as in (1) for which the first \\(r\\ \\times r\\) block is a diagonal matrix with the first \\(r\\) singular values of \\(A\\) on its diagonal, and there exist \\(U \\in \\mathbb{R}^{m \\times m}\\) and \\(V \\in \\mathbb{R}^{n \\times n}\\) such that \\[ A = U \\Sigma V^T \\] Proof Since \\(A\\) has \\(r\\) nonzero singular values which measure the length of \\(A\\bar{v}_i, \\; i = 1, ...n\\), there exists orthogonal basis \\(\\{A\\bar{v}_1, ..., A\\bar{v}_r\\}\\) for \\(\\mathcal{R}(A)\\), we can further normalize the set to produce the orthonormal set \\(\\bar{u}_1, ..., \\bar{u}_r\\): \\[ \\bar{u}_i = \\frac{A\\bar{v}_i}{\\sigma_i}, \\;\\; i = 1, ..., r \\] Now we can extend \\(\\{\\bar{u}_1, ..., \\bar{u}_r\\}\\) to an orthonormal basis \\(\\{\\bar{u}_1, ..., \\bar{u}_m\\}\\) of \\(\\mathbb{R}^m\\), and let \\[ U = [\\bar{u}_1 \\;\\; \\cdots \\;\\; \\bar{u}_m], \\quad V = [\\bar{v}_1 \\;\\; \\cdots \\;\\; \\bar{v}_n] \\] and \\(\\Sigma\\) be as be as in (1) above. Write out \\[ \\begin{split} U\\Sigma &amp;= [\\bar{u}_1 \\;\\; \\cdots \\;\\; \\bar{u}_r \\;\\; \\cdots \\;\\; \\bar{u}_m]_{m \\times m} \\begin{bmatrix} \\sigma_1 \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\sigma_r \\\\ &amp; &amp; &amp; 0 \\\\ &amp; &amp; &amp; &amp; \\ddots \\\\ &amp; &amp; &amp; &amp; &amp; 0 \\\\ \\end{bmatrix}_{m\\times n} \\\\ &amp;= [\\sigma_1\\bar{u}_1 \\;\\; \\cdots \\;\\; \\sigma_r\\bar{u}_r \\;\\; \\bar{0} \\;\\; \\cdots \\;\\; \\bar{0}] \\\\ &amp; = [A\\bar{v}_1 \\;\\; \\cdots \\;\\; A\\bar{v}_r \\;\\; A\\bar{v}_{r+1} \\;\\; \\cdots \\;\\; A \\bar{v}_n] \\\\ &amp;= A_{m \\times n}V_{n \\times n} \\end{split} \\] And because \\(V\\) is orthogonal \\[ A = U \\Sigma V^{-1} = U \\Sigma V^{T} \\] \\(\\bar{u}_i\\) and \\(\\bar{v}_i\\) are called left eigenvector and right eigenvector of \\(A\\) respectively. It’s easy to verify that the spectral decomposition 4.3.1 is a special case of SVD when \\(A \\in \\mathbb{R}^{n}, \\;\\; m = n\\). In that case, \\(\\Sigma\\) is a square matrix and \\(U\\) is equal to \\(V\\). When \\(\\Sigma\\) contains rows or columns of zeros (i.e, \\(r &lt; \\min(m, n)\\)), we can write SVD in a more compact form. Divide \\(U, \\Sigma, V\\) into submatrices \\[ U = [U_r \\;\\; U_{m-r}], \\quad \\text{where } U_r = [\\bar{u}_1 \\;\\; \\cdots \\;\\; \\bar{u}_r] \\\\ V = [V_r \\;\\; V_{m-r}], \\quad \\text{where } V_r = [\\bar{v}_1 \\;\\; \\cdots \\;\\; \\bar{v}_r] \\\\ \\Sigma = \\begin{bmatrix} D &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} \\quad \\text{where } D = \\begin{bmatrix} \\lambda_1 \\\\ &amp; \\ddots \\\\ &amp; &amp; \\lambda_r \\end{bmatrix} \\] The partitioned matrix multiplication shows that \\[ A = [U_r \\;\\; U_{m-r}] \\begin{bmatrix} D &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} \\begin{bmatrix} V_r^T \\\\ V_{n-r}^T \\end{bmatrix} = U_rDV_{r}^T \\] This more compact form is called the reduced form of SVD. Another way to write this is \\[ A = \\sum_{i=1}^{t}{\\sigma_i}\\bar{u}_i\\bar{v}_i \\] Right multiply the non-compact form \\(A = U\\Sigma V^T\\) by \\(A^T\\) , we get the spectral decomposition of symmetric matrix \\(AA^T\\). \\[ AA^T = (U \\Sigma V^T)(U \\Sigma V^T)^T = U \\Sigma \\Sigma^T VV^TU^T = U (\\Sigma\\Sigma^T) U^T \\tag{1} \\] Therefore, \\([\\bar{u}_1 \\;\\; \\cdots \\;\\; \\bar{u}_n]\\) are revealed as the orthonormal basis for \\(AA^T\\)’s eigenspace, as \\([\\bar{v}_1 \\;\\; \\cdots \\;\\; \\bar{v}_n]\\) are for \\(A^TA\\). Formula (1) echoes the fact that \\(A^TA\\) and \\(AA^T\\) have the same set of nonzero eigenvalues, because \\(\\Sigma\\Sigma^T\\) produces nonzero set \\(\\lambda_1, ..., \\lambda_r\\). In fact, if were to ask for a direction in which \\(A^T\\) has its greatest stretching effect instead of \\(A\\), we would still result in the equivalent decomposition \\(A^T = V\\Sigma U^T\\), with \\(\\bar{v}_i = \\frac{A\\bar{u}_i}{\\sigma_i}\\). It’s also easy to test that \\(\\{A\\bar{u}_1, ..., A\\bar{u}_r\\}\\) produces an orthogonal basis for \\(\\mathcal{R}(A^T)\\) . The process is analogous to theorem 5.1 where \\(\\{A\\bar{v}_1, ..., A\\bar{v}_r\\}\\) are shown to span \\(\\mathcal{R}(A)\\). For any vector \\(\\bar{y}\\) in \\(\\mathcal{R}(A)\\), we have \\[ \\begin{align*} \\bar{y} &amp;= A^T\\bar{x} \\\\ &amp;= A^T(c_1\\bar{u}_1 + \\cdots + c_1\\bar{u}_n) \\\\ &amp;= c_1A\\bar{u}_1 + \\cdots + c_rA\\bar{u}_r + \\bar{0} + \\cdots + \\bar{0} &amp;&amp; (\\text{because }A\\bar{u}_i = \\sigma_i\\bar{v}_i)\\\\ &amp;= c_1A\\bar{u}_1 + \\cdots + c_rA\\bar{u}_r \\end{align*} \\] Thus, SVD can be thought of an connection between two spectral decomposition \\[ A^TA = V (\\Sigma^T\\Sigma)V^T \\\\ AA^T = U (\\Sigma\\Sigma^T) U^T \\] This shed light on the relationship between SVD and the fundamental theorem of linear algebra 2.2 Subspace Columns \\(\\mathcal{R}(A)\\) the first \\(r\\) columns of \\(U\\) \\(\\mathcal{R}(A^T)\\) the first \\(r\\) columns of \\(V\\) \\(\\mathcal{N}(A)\\) the last \\(n - r\\) columns of \\(V\\) \\(\\mathcal{N}(A^T)\\) the last \\(m - r\\) columns of \\(U\\) 5.3 Matrix Norms Let \\(A\\) and \\(B\\) be matrices conformable for the operations below, a matrix norm should at first satisfy 3 axioms of norm: \\(\\|A\\| \\ge 0\\) for all \\(x \\in X\\), with equality if and only if all elements of \\(A\\) is zero (nonnegative) \\(\\|\\alpha A\\| = |\\alpha|\\,\\|A\\|\\) (homogeneous) \\(\\|A + B\\| &lt; \\|A\\| + \\|B\\|\\) (triangular inequality) Additionally, in the case of square matrices (thus, m = n), some (but not all) matrix norms satisfy the following condition, which is related to the fact that matrices are more than just vectors \\(\\|AB\\| &lt; \\|A\\|\\,\\|B\\|\\) for \\(A, B \\in \\mathbb{R}^{n \\times n}\\) A matrix norm that satisfies this additional property is called a submultiplicative norm. There are 2 main categories of matrix norms. induced norms (defined in terms of vector norms) entry-wise norms (treat \\(A_{m \\times n}\\) like a long vector with \\(m \\times n\\) elements) 5.3.1 Induced Norms Induced norms define matrix norms in terms of vectors, also called operator norm since \\(A\\) acts like an operator in this definition. Note that matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) maps a vector \\(\\bar{x} \\not = 0\\) from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\). In particular, if the p-norm is used for both \\(\\mathbb{R}^n\\) and \\(\\mathbb{R}^m\\), then the induced norm is \\[ \\|A\\|_p = \\max \\frac{\\|A\\bar{x}\\|_p}{\\|\\bar{x}\\|_p} \\] The subscript \\(p\\) can be misleading, because the appropriate name for this matrix norm may not be “p-norm”, but rather “induced norm when p-norm is used in both spaces”. The p-norm of a matrix meaning usually means entry-wise p-norms or the Scatten p-norms defined in subsequent sections. In the special cases where \\(p = 1, 2, ..., \\infty\\), \\(\\|A\\|_p\\) is the maximum absolute column sums, largest singular value, and the maximum absolute row sums \\[ \\begin{aligned} \\|A\\|_1 &amp;= \\max \\sum_{i=1}^{m}{|A_{ij}|} \\\\ \\|A\\|_2 &amp;= \\sigma_1 \\\\ \\|A\\|_{\\infty} &amp;= \\max \\sum_{j=1}^{n}{|A_{ij}|} \\end{aligned} \\] For symmetric matrix A, we have \\[ \\|A\\|_1 = \\|A\\|_{\\infty} \\] and \\[ \\|A\\|_2 = \\lambda_1 \\] The induced 2-norm are also called the spectral norm. By definition, the following inequality holds for induced matrix norms \\[ \\|A\\bar{x}\\|_P \\le \\|A\\|_p\\|\\bar{x}\\|_p \\] Proposition 5.1 Induced matrix norms satisfies the additional submultiplicative property in that \\[ \\|AB\\|_p \\le \\|A\\|_p\\|B\\|_p \\] Proof For any \\(\\bar{x} \\in \\mathbb{R}^n\\) \\[ \\|AB\\bar{x}\\|_p \\le \\|A\\|_p\\|B\\bar{x}\\|_p \\le \\|A\\|_p\\|B\\|_p\\|\\bar{x}\\|_p \\] si \\[ \\|AB\\|_p = \\max \\frac{\\|A\\bar{x}\\|_p}{\\|\\bar{x}\\|_p} \\le \\max \\frac{\\|A\\|_p\\|B\\|_p\\|\\bar{x}\\|_p}{\\|\\bar{x}\\|_p} = \\|A\\|_p\\|B\\|_p \\] 5.3.2 Entry-wise Norm Entry-wise norms treat an \\(m \\times n\\) matrix as a long vector of size \\(m \\times n\\), denoted by \\(\\text{vec}(A)\\). For example, using the p-norm for vectors, we get \\[ \\|A\\|_{p,p} = \\|\\text{vec}(A)\\|_p = \\Bigg(\\sum_{j=1}^n\\sum_{i=1}^{m}{|a_{ij}|}^p \\Bigg)^{\\frac{1}{p}} \\] More generally, the p,q norm is defined by \\[ \\|A\\|_{p, q} = \\Bigg ( \\sum_{j=1}^n \\Big (\\sum_{i=1}^{n}{|a_{ij}|}^p \\Big)^{\\frac{q}{p}} \\Bigg)^{\\frac{1}{q}} \\] Another important member norm of this norm family is the Frobenius norm, or the F-norm. \\[ \\| A\\|_F = \\sqrt{\\sum_{j=1}^n\\sum_{j=1}^{m}{a_{ij}^2}} = \\sqrt{\\text{tr}(A^TA)} = \\sqrt{\\sum_{i=1}^{\\min(m,n)}{\\sigma_i^2}} \\] where \\(\\sigma_i\\) is the nonzero singular value of \\(A\\). Proposition 5.2 The F-norm is a submultiplicative norm. Proof Let \\(A\\) and \\(B\\) are of appropriate size such that \\[ A = \\begin{bmatrix} a_1^T \\\\ \\vdots \\\\ a_m^T \\end{bmatrix} , \\; B = \\begin{bmatrix} b_1 &amp; \\cdots &amp; b_m \\end{bmatrix} \\\\ \\quad \\\\ \\begin{aligned} \\|AB\\| &amp;= \\sqrt{\\sum_{i, j}{(a_i^Tb_j)^2}} \\\\ &amp; \\le \\sqrt{\\sum_{i, j}{\\| a_i \\|^2 \\| b_j\\|^2}} \\\\ &amp;= \\sqrt{\\sum_{i}{\\|a_i\\|^2}} \\sqrt{\\sum_{j}{\\|b_j\\|^2}} \\\\ &amp;= \\|A\\| \\|B\\| \\end{aligned} \\] The first inequality comes from the Cauchy-Schwarz inequality \\(a \\cdot b \\le \\|a\\| \\|b\\|\\) 5.3.3 Other Matrix Norms One can pick to one of two ways to generalize the F-norm. One is a generalization of the direct definition that F-norm is \\(||\\text{vec}(A)||_2\\). For \\(p \\ge 1\\), it is the Frobenius-p norm: \\[ \\| A \\|_{F_p} = \\Bigg (\\sum_{i,j}{|a_{ij}|^p} \\Bigg)^{\\frac{1}{p}} \\] The Frobenius p-norm is the ordinary Frobenius norm. Another generalization stems from the relationship between F-norm and singular values of \\(A\\). The Schatten p norm is the p-norm of the vector composed of \\(A\\)’s singular values \\[ \\|A\\|_{S_p} = \\Big( \\sum_{i = 1}^{\\min(m, n)}{\\sigma_i}^p\\Big)^{\\frac{1}{p}} \\] The most familiar choices of \\(p\\) are \\(1, 2, ..., \\infty\\). Spectral norm and F-norm can be viewed as special cases of the Schatten norm. The case \\(p = 2\\) yields the F-norm, and \\(p = \\infty\\) the spectral norm. Finally, \\(p = 1\\) yields the nuclear norm (also known as the trace norm), defined as \\[ \\|A\\|_N = \\sigma_1 + \\cdots + \\sigma_r \\] 5.3.4 Unitary Invariant Norms Definition 5.2 (unitary invariant norms) A matrix norm \\(\\|\\cdot \\|\\) is said to be unitary invariant if for all orthogonal matrices \\(U\\) and \\(V\\) of appropriate size \\[ \\|A\\| = \\|UAV\\| \\] (unitary matrices refers to orthogonal matrices with complex-valued entries, but I focus on real matrices here.) We mentioned that spectral norm, F-norm and nuclear norm are all unitary invariant norms. More than that, these 3 norms of any matrix stay the same when \\(A\\) is multiplied by an orthogonal matrix. Essentially, if a norm depends only on the singular values of a matrix, it is unitary invariant. Since for such norms: \\[ \\|A\\| = \\|\\Sigma\\| \\qquad \\text{because the norm only depend on singular values} \\] Multiply two orthogonal matrices \\(U\\) and \\(V^T\\) on each side, \\[ \\begin{aligned} \\|UA V^T\\| &amp; = \\| U \\Sigma V^T\\| \\qquad \\text{multiply by orthogonal matrix does not change norm}\\\\ \\|UAV^T\\| &amp;= \\|A\\| \\end{aligned} \\] The spectral norm (induced p-norm), F-norm and Schatten norm are all unitary invariant. Because they can all be expressed in terms of singular values of \\(A\\) \\[ \\begin{aligned} \\|A\\|_2 &amp;= \\sigma_1 \\\\ \\|A\\|_F &amp;= \\sqrt{\\sigma_1^2 + \\cdots + \\sigma_r^2} \\\\ \\|A\\|_N &amp;= \\sigma_1 + \\cdots + \\sigma_r \\end{aligned} \\] 5.4 Low Rank Approximation Theorem 5.3 (Eckart–Young–Mirsky) Let \\(\\|\\cdot\\|\\) be a unitary invariant norm. Suppose \\(A \\in \\mathbb{R}^{m \\times n}\\) where \\(m &gt; n\\), has svd \\(A = \\sum_{i = 1}^n \\sigma_i \\bar{u}_i \\bar{v}^T_i\\). Then the best rank-k approximation to \\(A\\), where \\(k \\le \\text{rank}(A)\\), is given by \\[ A_k = \\sum _{i=1}^{k}\\sigma_i \\bar{u}_i \\bar{v}_i^T \\] in the sense that for any other rank-k matrix \\(\\tilde{A}\\) \\[ \\|A - A_k \\| \\le \\|A - \\tilde{A} \\| \\] Proof for Eckart-Young theorem A measure of the quality of the approximation is given by \\[ \\frac{\\|A_k\\|^2}{\\| A\\|^2} = \\frac{\\sigma_1^2 + \\cdots + \\sigma_k^2}{\\sigma_1^2 + \\cdots + \\sigma_r^2} \\] "],
["linear-system.html", "Chapter 6 Solutions of Linear System Ax = b 6.1 When A is square 6.2 When A has independent columns or rows 6.3 Ill-Conditioned Matrices", " Chapter 6 Solutions of Linear System Ax = b A linear system \\(Ax= b\\) when \\(b = 0\\)3 may behave in any one of three possible ways: The system has no solution. This occurs when \\(b \\not \\in \\mathcal{R}(A)\\). In such cases we try to find best fits that meets certain optimization requirements. For example, to make \\(\\|Ax - b\\|\\) as small as possible. Such a solution is called the best fit. The system has a single unique solution. This occurs when \\(b \\in \\mathcal{R}(A)\\), and \\(A\\) has linearly independent columns, then there exists an unique linear combination \\(a_1x_1 + \\cdots + a_nx_n = b\\). In the special case when \\(A\\) is square, the solution is simply \\(x = A^{-1}b\\) The system has infinitely many solutions. This is the case when \\(b \\in \\mathcal{R}(A)\\) and columns of \\(A\\) are linearly dependent, with free columns. Any linear combination of current solutions would still satisfy \\(Ax = b\\). In this case, we want to make \\(\\| x \\|^2\\) as small as possible. This is called the conciseness criterion. Moreover, suppose \\(b \\not \\in \\mathcal{R}(A)\\) which means rows and columns of \\(A\\) are both linearly dependent. There are typically infinite many solutions that have the same optimal value in terms of \\(\\|Ax -b\\|\\) In the first scenario, the linear system is called inconsistent because there is no \\(x\\) that satisfies each equation in the system. Linear systems in the second and third scenario are called consistent because there is at least one solution. 6.1 When A is square 6.1.1 Invertible A The simplest case happens when \\(A\\) is square and invertible, the unque solution of linear system \\(Ax = b\\) is given by \\[ x = A^{-1}b \\] For square matrix \\(A\\), the following criteria for invertibility are equivalent columns of \\(A\\) are independent; the left null space is empty rows of \\(A\\) are independent; the null space is empty \\(|A| \\not = 0\\) eigenvalues of \\(A\\) are nonzero (singular values of \\(A\\) are nonzero) \\(A\\) is digonalizable However, direct computation of \\(A^{-1}\\) could be very hard especially when \\(A\\) is large. Cramer’s rule {1.6}, or the general formula using \\(A^*\\) mainly serve for theoretical purposes and come with prohibitive complexity. Therefore, we need more effective algorithm for finding \\(A^{-1}\\) Inverse via QR factorization. In Section 3.5, we show that a full rank square matrix \\(A\\) can be factored into an orthogonal matrix \\(Q\\) and an upper triangular matrix \\(R\\) with positive diagonal entries. Hence \\(Q\\) and \\(R\\) are invertible, we have \\[ \\begin{aligned} Ax &amp;= b \\\\ QRx &amp;= b \\\\ Rx &amp;= Q^{-1}b = Q^Tb \\end{aligned} \\] This means that to find the solution \\(x\\), we need to solve the new linear system \\(Rx = Q^Tb\\). The new coefficient matrix \\(R\\) is upper triangular, let \\(Q^Tb\\) be our new \\(b\\), we write out the equations as \\[ \\begin{aligned} R_{11}x_1 + R_{12}x_2 + \\cdots + R_{1, n - 1}x_{n-1} + R_{1n}x_n &amp;= b_1 \\\\ R_{22}x_2 + \\cdots + R_{2, n - 1}x_{n-1} + R_{2n}x_n &amp;= b_2 \\\\ &amp; \\vdots \\\\ R_{n-1, n -1}x_{n - 1} + R_{n - 1, n}x_{n} &amp;= b_{n - 1} \\\\ R_{nn}x_n &amp;= b_n \\end{aligned} \\] If we move our eyes bottom-up, we immediately know \\(x_n = b_n / R_{nn}\\). Substitute this into the second to last equation \\[ x_{n - 1} = \\frac{b_{n-1} - R_{n-1,n} b_n / R_{nn}}{R_{n-1,n-1}} \\] It follows that there exists a efficient algorithm to find the solution \\(x\\) in the order \\(x_n, x_{n - 1}, ..., x_2, x_1\\), known as back substitution. Back Substitution For the linear system \\(Rx = b\\), where \\(R\\) is an \\(n \\times n\\) upper triangular matrix with nonzero diagonal entries, the solution \\(x = [x_1, ..., x_n]\\) are given by For \\(i = n, ..., 1\\) \\[ x_i = (b_i - R_{i, i + 1}x_{i + 1} - \\cdots - R_{i, n}x_{n}) / R_{ii} \\] 6.1.2 Uninvertible A 6.2 When A has independent columns or rows 6.2.1 Lesat Squares Problems Definition 6.1 the normal equation \\[ A^TA\\bar{x} = A^T\\bar{b} \\] \\[ \\begin{aligned} A^T(\\bar{b} - A\\hat{\\bar{x}}) &amp;= \\bar{0} \\end{aligned} \\\\ A^T\\bar{b} - A^TA\\hat{\\bar{x}} = 0 \\\\ \\hat{\\bar{x}} = (A^TA)^{-1}A^T\\bar{b} \\] 6.2.2 Left and Right Inverse 6.3 Ill-Conditioned Matrices 6.3.1 Condition Number \\[ \\delta x \\] For simplicity I exclude most \\(\\bar{}\\) sign for vector notation, lower case letters typically denote a vector unless there is a need for mixed usage of scalars and vectors↩︎ "],
["special-matrices.html", "Chapter 7 Special Matrices", " Chapter 7 Special Matrices "],
["partial-derivatives.html", "Chapter 8 Partial Derivatives 8.1 Limit and Continuity 8.2 Partial Derivative 8.3 Differentials 8.4 Divergence, Curl, and Laplacian", " Chapter 8 Partial Derivatives 8.1 Limit and Continuity Limits and continuity in one dimension Limits and continuity in higher dimensions 8.2 Partial Derivative 8.2.1 Gradient and Directional Derivative We want to know the instant rate of change at \\(P_0 = (x_0, y_0)\\), when moving towards the direction \\(\\bar{u} = [u_1, u_2]\\) a tiny step of length \\(h\\). Definition 8.1 (Directional derivative) \\[ \\Big(\\frac{df}{dh} \\Big)_{\\bar{u}, P_0} = \\lim_{h \\to 0} \\frac{f(x_0 + hu_1, y_0 + hu_2) - f(x_0, y_0)}{h} \\] The concise version using vector notation is \\[ \\lim_{h \\to 0} \\frac{f(\\bar{x} + h\\bar{u}) - f(\\bar{x})}{h} \\] \\[ x = x_0 + hu_1, \\quad y = y_0 + hu_2 \\] use the chain rule \\[ \\begin{split} \\Big(\\frac{df}{dh} \\Big)_{\\bar{u}, P_0} &amp;= \\frac{\\partial f}{\\partial x} \\biggr |_{P_0} \\frac{dx}{dh} + \\frac{\\partial f}{\\partial y} \\biggr |_{P_0} \\frac{dy}{dh} \\\\ &amp;= \\frac{\\partial f}{\\partial x} \\biggr |_{P_0}u_1 + \\frac{\\partial f}{\\partial y} \\biggr |_{P_0}u_2 \\\\ &amp;= \\nabla f |_{P_0} \\cdot \\bar{u} \\end{split} \\] Expand the dot product, we get \\[ D_{\\bar{u}}f = |\\nabla f| |\\bar{u}| \\cos\\theta = |\\nabla f| \\cos\\theta \\] Theorem 8.1 (Properties of directional derivatives) That function \\(f\\) increases most rapidly in the direction of the gradient vector \\(\\nabla f\\) at \\(P\\). The directional derivative (instant increasing rate of change) in this direction is \\[ D_{\\bar{u}}f = |\\nabla f| \\cos{0} = |\\nabla f| \\] Likewise, \\(f\\) decreases most rapidly in the direction of \\(-\\nabla f\\). The directional derivative in this direction is \\[ D_{\\bar{u}}f = |\\nabla f| \\cos{\\pi} = - |\\nabla f| \\] 8.2.2 Linearization of Two-variable Functions Linearization of univariate function \\(f(x)\\) centered at \\(x = a\\) is the point-slope equation \\[ L(x) = f(a) + f&#39;(a)(x - a) \\] Now we move to functions of two variables. Suppose a differentiable function \\(f(x, y)\\) and its partial derivatives exist at a point \\(f(x_0, y_0)\\). If we move to a nearby point \\((x_0 + \\Delta x, y_0 + \\Delta y)\\), the change in \\(f\\) is \\[ \\Delta f = f(x_0 + \\Delta x, y_0 + \\Delta y) - f(x_0, y_0) \\] Like in the single variable case (Eq (8.1)), the change in \\(f\\) is \\[ f(x, y) - f(x_0, y_0) = f_x \\Delta x + f_y \\Delta y + \\varepsilon_1\\Delta x + \\varepsilon_2 \\Delta y \\] where \\(\\varepsilon_1, \\varepsilon_2 \\to 0\\) as \\(\\Delta x, \\Delta y \\to 0\\). When \\(\\Delta x, \\Delta y \\to 0\\), the error term will be even smaller and we have the linear approximation \\[ f(x, y) \\approx L(x, y) = f(x_0, y_0) + f_x (x - x_0) + f_y (y - y_0) \\] Definition 8.2 The Linearization of a function \\(f(x, y)\\) at a differentiable point \\((x_0, y_0)\\) is \\[ L(x, y ) = f(x_0, y_0) + f_x (x - x_0) + f_y (y - y_0) \\] The approximation \\[ f(x, y) \\approx L(x, y) \\] is called the standard linear approximation of \\(f\\) at \\((x_0, y_0)\\) In the two variable case, \\(L(x, y)\\) denotes a plane tangent to the surface \\(z = f(x, y)\\) at point \\((x_0, y_0)\\) 8.3 Differentials We first review the definition of differential in single variable calculus. In single variable calculus, the change in function \\(f\\) as \\(x\\) changes from \\(a\\) to \\(a + \\Delta x\\) is \\[ \\Delta y = f(a + \\Delta x) - f(a) \\] The corresponding change in the standard linearization \\(L(x)\\) from \\(a\\) to \\(a + \\Delta x\\) is \\[ \\begin{split} \\Delta L &amp;= \\underbrace{f(a) + f&#39;(a)(a + \\Delta x - a)}_{L(a + \\Delta x)} - \\underbrace{f(a)}_{L(a)} \\\\ &amp;= f&#39;(a)\\Delta x \\end{split} \\] When \\(\\Delta x\\) is a infinitely small number \\(dx\\) (but nonzero), we can use \\(\\Delta L\\) to approximate \\(\\Delta y\\). In such a case, let \\(dy\\) or \\(df\\) represents the the amount the tangent line rises or falls when \\(x\\) changes by an tiny amount \\(\\Delta x = dx\\). This amount is called a differential. In other words, the differential \\(dy\\) is the change \\(\\Delta L\\) in the linearization of \\(f\\) when \\(x = a\\) changes by an amount \\(dx\\). Now let \\(a\\) be any point \\(x\\) in \\(f\\)’s domain, the differential \\(dy\\) (or \\(df\\)) is \\[ dy = f&#39;(x)\\Delta x \\] The error between \\(\\Delta y\\) (true change) and \\(dy\\) (approximate change) at \\(x = a\\) is given by \\[ \\begin{split} \\Delta y - dy &amp;= \\underbrace{f(a + \\Delta x) - f(a)}_{\\text{true change}} - \\underbrace{f&#39;(a)\\Delta x}_{\\text{approximate change}} \\\\ &amp;= \\underbrace{\\Big(\\frac{f(a + \\Delta x) - f(a)}{\\Delta x} - f&#39;(a)\\Big)}_{\\varepsilon} \\Delta x \\\\ &amp;= \\varepsilon \\Delta x \\end{split} \\] when \\(\\Delta x \\to 0\\), the difference quotient \\[ \\frac{f(a + \\Delta x) - f(a)}{\\Delta x} \\to f&#39;(a) \\] Thus \\(\\varepsilon \\to 0\\) as \\(\\Delta x \\to 0\\), and \\(\\Delta y\\) can be reexpressed as \\[\\begin{equation} \\tag{8.1} \\Delta y = dy + \\varepsilon \\Delta x = f&#39;(a)\\Delta x + \\varepsilon \\Delta x \\end{equation}\\] In the two variable case, the change in linearization is \\[ \\Delta L = f_x \\Delta x + f_y \\Delta y \\] We take \\(dx = \\Delta x\\) and \\(dy = \\Delta y\\). We then have the following definition of the differential or total differential of \\(f\\). Definition 8.3 When moving from \\((x_0, y_0)\\) to a nearby point \\((x_0 + dx, y_0 + dy)\\), the resulting change \\[ df = f(x_0, y_0) + f_xdx + f_y dy \\] in the linearization of \\(f\\) is called the total differential of \\(f\\). 8.3.1 Continuity, Partial Derivatives and Differentiability 8.4 Divergence, Curl, and Laplacian "],
["matrix-calculus.html", "Chapter 9 Matrix Calculus 9.1 The Chain Rule 9.2 Useful Identities in Matirx Calculus", " Chapter 9 Matrix Calculus Matrix calculus refers to a number of different notations that use matrices and vectors to collect the derivative of each component of the dependent variable with respect to each component of the independent variable. \\[ \\frac{\\partial{f(x)}}{\\partial{x}} \\] Differentiate a scalar by a vector. \\[ \\frac{\\partial f(x_1, ..., x_n)}{\\partial (x_1, ..., x_n)} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} \\] One important thing here is we define the gradient of \\(f(x_1, ..., x_n)\\) to be a row vector. This is also called the numerator layout, as opposed the denominator layout where gradient is defined as a column vector. Next, we come to cases when we differentiate a vector by a vector (i.e., the gradient of a vector-valued function). The function \\(\\bar{f}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) takes a \\(n\\)-dimensional vector \\(\\bar{x} = [x_1, ..., x_n]\\) as input, and output a \\(m\\)-dimensional vector. The corresponding vector of function value is given as \\[ \\boldsymbol{f}(\\bar{x}) = \\begin{bmatrix} f_1(\\bar{x}) \\\\ \\vdots \\\\ f_m(\\bar{x}) \\end{bmatrix} \\] In this way, we think of a vector-valued function \\(\\boldsymbol{f}\\), as a vector of scalar functions \\([f_1, ..., f_m]^T, \\; f_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\). Thus, partial derivatives of \\(f_i\\) with respect vector of \\(\\bar{x}\\) is the gradient \\[ \\frac{\\partial f_i}{\\partial \\bar{x}} = \\begin{bmatrix} \\frac{\\partial f_i}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_i}{\\partial x_n} \\end{bmatrix} \\] Further, the partial derivative of \\(\\bar{f}\\) with respect to scalar \\(x_i, \\; i = 1,...,n\\) is simply a column vector of partial derivatives \\(\\frac{\\partial \\bar{f}_j}{\\partial x_i}\\) \\[ \\frac{\\partial \\boldsymbol{f}(\\bar{x})}{\\partial x_i} = \\begin{bmatrix} \\frac{\\partial f_1 }{\\partial x_i} \\\\ \\vdots \\\\ \\frac{\\partial f_m }{\\partial x_i} \\end{bmatrix} = \\begin{bmatrix} \\lim _{h \\to \\infty}\\frac{f_1(x_1, ..., x_i + h, ..., x_n)}{h} \\\\ \\vdots \\\\ \\lim _{h \\to \\infty}\\frac{f_m(x_1, ..., x_i + h, ..., x_n)}{h} \\end{bmatrix} \\] We obtain the “full” gradient of \\(\\bar{f}\\) with respect of \\(\\bar{x}\\). Definition 9.1 (Jacobian) \\[ \\begin{split} J = \\nabla_x \\bar{f} &amp;= \\begin{bmatrix} \\frac{\\partial \\bar{f}}{\\partial x_1} &amp; \\cdots &amp;\\frac{\\partial \\bar{f}}{\\partial x_n} \\end{bmatrix} &amp;= \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots &amp; &amp; \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{split} \\] 9.1 The Chain Rule 9.2 Useful Identities in Matirx Calculus Assuming \\(A\\) is a \\(m \\times n\\) matrix and \\(\\bar{w}\\) an \\(n \\times 1\\) column vector, and let others be of conformbale sizes, we adopt the numerator layout to show some commonly used identities in matrix calculus. Derivative of with respect to output size result \\(A\\bar{w}\\) \\(\\bar{w}^TA\\) \\(\\bar{w}\\) matrix \\(m \\times n\\) matrix \\(n \\times m\\) \\(A\\) \\(A^T\\) \\(\\bar{a}^T\\bar{w}\\) \\(\\bar{w}^T\\bar{a}\\) \\(\\bar{w}\\) row vector \\(1 \\times n\\) \\(\\bar{a}^T\\) \\(\\bar{b}^TB\\bar{w}\\) \\(\\bar{w}^TB^T\\bar{b}\\) \\(\\bar{w}\\) row vector \\(1 \\times n\\) \\(\\bar{b}^TB\\) \\(\\bar{w}^TA\\bar{w}\\) \\(\\bar{w}\\) row vector \\(1 \\times n\\) \\(\\bar{w}^T(A + A^T)\\) if \\(A\\) is symmetric \\(2\\bar{w}^TA\\) \\(\\bar{w}^T\\bar{w}\\) \\(\\bar{w}\\) row vector \\(1 \\times n\\) \\(2\\bar{w}^T\\) \\(\\bar{f}(X)^T\\) \\(X\\) - \\(\\Big (\\frac{\\partial \\bar{f}(X)}{\\partial X}\\Big )^T\\) The major downside of using the numerator layout is that, since we define gradient to be a row vector, we have to constantly transpose the gradient into a column vector in order to perform gradient descent updates on the column vector \\(\\bar{w}\\). In other words, we have to write the update in matrix calculus notation as follows: \\[ \\bar{w}_{i + 1} \\Leftarrow \\bar{w}_i- \\alpha \\cdot (\\nabla L)^T \\] where \\(\\nabla L\\) is the gradient of loss function \\(L\\) at \\(\\bar{w}_i\\). "],
["taylor-series.html", "Chapter 10 Taylor Series 10.1 Convergence of Taylor Series 10.2 Taylor Approximation of Multivariate Functions", " Chapter 10 Taylor Series \\[ L(x, y) = f(x_0, y_0) + f_x(x_0, y_0)(x - x_0) + f_y(x_0, y_0)(y - y_0) \\] The approximation \\[ f(x, y) \\approx L(x, y) \\] is called the standard linear approximation of \\(f\\) at \\((x_0, y_0)\\) If \\(M\\) is the upper bound of \\(|f_{xx}|\\), \\(|f_{yy}|\\) and \\(|f_{xy}|\\) \\[ \\text{error of linearization} \\le \\frac{1}{2}M(|x - x_0|^2 - |y - y_0|^2) \\] 10.1 Convergence of Taylor Series 10.2 Taylor Approximation of Multivariate Functions \\[ \\boldsymbol{f}(\\bar{x}) = \\begin{bmatrix} f_1(\\bar{x}) \\\\ \\vdots \\\\ f_m(\\bar{x}) \\end{bmatrix} \\] We begin by asking for the first order univariate taylor approximation of a certain individual component \\(f_i(\\bar{x})\\) of \\(\\boldsymbol{f}(\\bar{x})\\), near \\(\\bar{a} = [a_1, ..., a_n]\\) \\[ f_i(\\bar{x}) \\approx f_i(\\bar{a}) + \\frac{\\partial f_i}{\\partial x_1}(\\bar{a}) f_i(x_1 - a_1) + \\frac{\\partial f_i}{\\partial x_2}(\\bar{a}) f_i(x_2 - a_2) + \\cdots + \\frac{\\partial f_i}{\\partial x_n}(\\bar{a}) f_i(x_n - a_n) \\] Or use the vector form \\[ f_i(\\bar{x}) \\approx f_i(\\bar{a}) + \\nabla f_i(z)(\\bar{x} - \\bar{a}) \\] Therefore \\[ \\boldsymbol{f}(\\bar{x}) \\approx \\begin{bmatrix} f_1({\\bar{a}}) \\\\ \\vdots \\\\ f_m({\\bar{a}}) \\end{bmatrix} + \\begin{bmatrix} \\nabla f_1 (\\bar{a}) \\\\ \\vdots \\\\ \\nabla f_m (\\bar{a}) \\end{bmatrix} (\\bar{x} - \\bar{a}) = \\boldsymbol{f}(\\bar{a}) + \\nabla\\boldsymbol{f}(\\bar{a})(\\bar{x} - \\bar{a}) \\] where \\(\\nabla\\boldsymbol{f}(\\bar{a})\\) is the Jacobian (Definition 9.1) of vector-valued function \\(\\boldsymbol{f}\\) evaluated at \\(\\bar{a}\\). "],
["multiple-integral.html", "Chapter 11 Multiple Integral", " Chapter 11 Multiple Integral "],
["basics-of-probability-theory.html", "Chapter 12 Basics of Probability Theory 12.1 Probabilty Space 12.2 Counting 12.3 Conditional Probability", " Chapter 12 Basics of Probability Theory The probability section should be rather applied, without delving into deeper branch of analysis and measure theory. 12.1 Probabilty Space A probability space is a triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) where \\(\\Omega\\) is the set of all possible events in an experiment, called the sample space \\(\\mathcal{F}\\) is a subset of \\(\\Omega\\), containing some of the events usually denoted by \\(A\\). Events may be a singleton set \\({\\omega}\\) or contain more than one sample point. The complement of event \\(A\\) is another event, \\(A^c\\). As we can only define probabilities for such subsets of \\(\\Omega\\) that are tied together like a family, \\(\\mathcal{F}\\) is assumed to be a \\(\\sigma\\)-field which satisfy If \\(A \\in \\mathcal{F}\\) then \\(A^c \\in \\mathcal{F}\\) If \\(A_i \\in \\mathcal{F}, \\;i = 1,...,n\\) is a countable sequence of sets, then \\(A_1 \\cup A_2 \\cup ... \\cup A_n \\in \\mathcal{F}\\) \\(\\mathbb{P}\\) is a function that assigns probabilities to events: \\(\\mathbb{P}: \\mathcal{F} \\rightarrow [0 , 1]\\). Axioms of probability are concerned with specific properties of the function \\(\\mathbb{P}\\) $(A) 0 $ for all \\(A \\in \\mathcal{F}\\) \\(\\mathbb{P}(\\Omega) = 1\\) and \\(\\mathbb{P(\\emptyset)} = 0\\) If \\(A_1, ..., A_n\\) are disjoint sets (\\(A_i \\cup A_j = \\emptyset\\)) 12.2 Counting Additionally, there are several useful identities in combination problems. The formal algebraic proof can be cumbersome, instead, here we draw on story proofs from the “Introduction to Probability” book (K. Blitzstein and Hwang 2019). Proposition 12.1 (Vandermonde’s identity) \\[ \\binom{m + n}{k} = \\sum_{j = 0}^{k}\\binom{m}{j} \\binom{n}{k -j} \\] Suppose we want to select a size \\(k\\) committee out of \\(m\\) juniors and \\(n\\) seniors. The left side shows there are obviously \\(\\binom{m + n}{k}\\) probabilities. On the other side of the story, given there are \\(j\\) juniors in the committee, there must be \\(k - j\\) seniors in the committee. The right-hand side sums up the cases varying \\(k\\). Proposition 12.2 (Partnerships Problem) \\[ \\frac{(2n)!}{2^n \\cdot n!} = (2n -1 ) \\cdot (2n -3) \\cdots 3 \\cdot 1 \\] Both side of the identity count the ways to break \\(2n\\) people into \\(n\\) partnerships. According to the left side, we can form partnerships by lining up all \\(2n\\) people and making the first and the second a pair, then the third and the fourth a pair, etc. This overcounts by a factor \\(2^n \\cdot n!\\) since the order of partnerships does not matter, nor does the order inside each pair. Alternatively, there are \\(2n - 1\\) choices for people 1, \\(2n - 3\\) choices for people 2 (or person 3 if people 2 is paired to people 1), and so on. 12.3 Conditional Probability Definition 12.1 (Conditional probability) "],
["random-variables-and-moments.html", "Chapter 13 Random variables and moments 13.1 Properties of Expectation and Variance 13.2 Other Summaries 13.3 Moment Generating Functions", " Chapter 13 Random variables and moments 13.1 Properties of Expectation and Variance This section provides some properties of \\(E(X)\\) and \\(\\text{Var}(X)\\) commonly-used in probabilistic calculations. Suppose all expectations \\(E(\\cdot)\\) exists Non-negativity: If \\(X \\ge 0\\) then \\(E(X) \\ge 0\\) Linearity of expectation \\(E(\\alpha X) = \\alpha E(X)\\) (\\(\\alpha\\) is constant) \\(E(X + Y) = E(X) + E(Y)\\) (\\(Y\\) is also r.v., and not necessarily independent of \\(X\\)). More generally, for any r.v. \\(X_1, ..., X_n\\) \\(E(X_1 + \\cdots + X_n) =\\sum_{i=1}^n{E(X_i)}\\) Monotonicity: If \\(X \\le Y\\), then \\(E(X) \\le E(Y)\\) Non-multiplicativity:4 \\(E(XY) = E(X)E(Y) + \\text{Cov}(X, Y)\\). This means in general \\(E(XY) \\not= E(X)E(Y)\\) holds except that \\(X, Y\\) are independent (although this is not a necessary condition) law of the unconscious statistician (LOTUS). Suppose \\(g(X)\\) is a function of \\(X\\). If \\(X\\) is discrete then \\(E[g(X)] = \\sum g(x)f_X(x)\\), and if \\(X\\) is continuous \\(E[g(X)] = \\int g(x)f(x)dx\\) \\(E(X^2) = \\text{Var}(X) + [E(X)]^2\\)5 An important extension of the previous identity is used listed in the following lemma. Lemma 13.1 The fact that \\(\\text{E}(X^2) = \\text{Var}(X) + \\mu^2\\) is a special case of the following identity \\[ E(X - c)^2 = \\text{Var}(X) + (\\mu - c)^2 \\] where \\(c = 0\\) Proof Use the fact that variance is not affected when adding a constant \\[ \\begin{split} \\text{Var}(X) &amp;= \\text{Var}(X - c) \\\\ &amp;= E(X - c)^2 - (E(X - c))^2 \\\\ &amp;= E(X - c)^2 - (\\mu - c)^2 \\end{split} \\] For \\(\\text{Var}(X)\\), we have the following properties non-negative: \\(\\text{Var}(X) \\ge 0\\) with equality only if \\(X\\) follows degenerate distribution invariant to adding constant: \\(\\text{Var}(X + \\alpha) = \\text{Var}(X)\\) If all values are scaled by a constant, the variance is scaled by the square of that constant: \\(\\text{Var}(\\alpha X) = \\alpha^2\\text{Var}(X)\\) The variance of a sum of two random variables is given by \\[ \\begin{aligned} \\text{Var}(aX + bY) &amp;= a^2\\text{Var}(X) + b^2\\text{Var}(Y) + 2ab \\cdot\\text{Cov}(X, Y) \\\\ \\text{Var}(aX - bY) &amp;= a^2\\text{Var}(X) + b^2\\text{Var}(Y) - 2ab \\cdot\\text{Cov}(X, Y) \\end{aligned} \\] By extension, if \\(X\\) and \\(Y\\) are uncorrelated, \\(\\text{Var}(aX + bY) = a^2\\text{Var}(X) + b^2\\text{Var}(Y)\\) variance-covariance operations: the variance of a linear combination of r.v. is the sum of variance plus all pairs of covariances \\[ \\text{Var}(\\sum_{i=1}^{n} \\alpha_i X_i) = \\sum_{i=1}^{n} \\alpha^2 \\text{Var}(X_i) + \\sum_{i \\not = j}a_ia_j \\text{Cov}(X_i, X_j) \\] 13.1.1 Random vectors 13.2 Other Summaries Theorem 3.1 Let \\(X\\) be an r.v. with expectation \\(E(X) = \\mu\\), and let \\(m\\) be the median of \\(X\\) The value of \\(c\\) that minimizes the mean squared error \\(E(X - c)^2\\) is \\(c = \\mu\\) The value of \\(c\\) that minimizes the mean absolute error \\(E |X - c|\\) is \\(c = m\\) Proof In the case of mean squared error, we have \\(E(X - c)^2 = \\text{Var}(X) + (\\mu - c)^2\\) according to lemma 13.1. Therefore, the quantity is minimized when \\(c = \\mu\\). As for the mean absolute error, we need to show that \\(E|X - m| \\le E |X - a|\\) for any \\(a\\), which is equivalent to \\(E(|X - a| - |X - m|) \\ge 0\\). Assume \\(m &lt; a\\) without loss of generality, if \\(X \\le m\\) then \\[ |X - a| -|X - m| = a-m \\] and if \\(X &gt; m\\) \\[ |X - a| - |X - m| \\ge X - a - (X- m) = m -a \\] Let \\[ Y = |X - a| - |X - m| \\] When can split \\(E(Y)\\) into two parts based on whether the event \\(X \\le m\\) occurs \\[ \\begin{split} E(Y) &amp;= E(Y | X \\le m)P(X \\le m) +E(Y | X &gt; m)P(X \\gt m) \\\\ &amp;\\ge (a - m)P(X \\le m) + (m - a)P(X &gt; m) \\\\ &amp;= (a-m)P(X \\le m) - (a - m)(1 - P(X \\le m)) \\\\ &amp;= (a - m)(2P(X \\le m) - 1) \\end{split} \\] Since for median \\(m\\) we know \\(P(X \\le m) \\le \\frac{1}{2}\\), we get \\(E(Y) \\ge 0\\) with equality when \\(a = m\\). This means the mean absolute error \\(E|X - a|\\) is minimized when \\(a\\) is the median of \\(X\\). 13.3 Moment Generating Functions Theorem 13.1 (Moments via derivaties of MGF) The \\(n\\)th momment of r.v. \\(X\\) is the \\(n\\)th derivative of its MGF evaluated at zero \\[ E(X^n) = M^{n}(0) \\] Proof The Taylor expansion of \\(M(t)\\) about \\(0\\) is \\[ M(t) = \\sum_{n=1}^{\\infty} M^{(n)}(0) \\frac{t^n}{n!} \\] On the other hand, use the fact that the Taylor expansion of \\(e^x\\) about \\(0\\) is \\(\\sum \\frac{x^n}{n!}\\), we have \\[ \\begin{split} M(t) &amp;= E(e^{tX}) \\\\ &amp;= E(\\sum_{n = 0}^{\\infty} \\frac{X^nt^n}{n!}) \\\\ &amp;= \\sum_{n = 0}^{\\infty} E(X^n)\\frac{t^n}{n!} \\qquad \\text{due to techinical conditions of MGF}\\\\ \\end{split} \\] Matching the coefficients of the two expansions, we get \\(E(X^n) = M^{(n)}(0)\\) Note that expectation of infinite sum \\(E(\\sum_{i = 0}^{\\infty}X_i)\\) may not be equal to \\(\\sum_{n=0}^{\\infty}E(X_I)\\). In other words, linearity of expectation may not hold for infinite sums. However, the definition of MGF has a technical condition – MGF is finite on some open interval \\((-a, a)\\) containing \\(0\\) – which ensures the linearity property used in the following formula. Theorem 13.2 (One-to-one relationship between MGF and distribution) The MGF of a random variable determines its distribution. If two random variables have the same MGF, then they follow the same distribution. Theorem 13.3 (MGF and convolutions) The MGF of the convolution (sum) of two independent r.v. \\(X\\) and \\(Y\\), is the product of the individual MGFs. This is because \\[ \\begin{split} M_{X + Y}(t) &amp;= E(e^{t(X + Y)}) \\\\ &amp;= E(e^{tX} \\cdot e^{tY})\\\\ &amp;= E(e^{tX}) \\cdot E(e^{tY}) \\qquad \\text{X and Y are independent}\\\\ &amp;= M_X(t) \\cdot M_Y(t) \\end{split} \\] Proof: \\(E(XY) = E(X)E(Y) + \\text{Cov}(X, Y)\\). Start with covariance \\[\\begin{split} \\text{Cov}(X, Y) &amp;= E[(X - \\mu_x)(Y - \\mu_y)] \\\\ &amp;= E(XY - X\\mu_y - Y\\mu_x + \\mu_x\\mu_y) \\\\ &amp;= E(XY) - \\mu_yE(X) - \\mu_xE(Y) + \\mu_x\\mu_y \\\\ &amp;= E(XY) - 2\\mu_x\\mu_y + \\mu_x\\mu_y \\\\ &amp; = E(XY) - \\mu_x\\mu_y \\end{split}\\]↩︎ Proof: \\(E(X^2) = \\text{Var}(X) + \\big( E(X) \\big)\\). \\[ \\begin{split} E(X^2) &amp;= E \\big[ (X \\pm \\mu)^2 \\big] \\\\ &amp;= E(X - \\mu)^2 + \\mu^2 + 2 \\mu E(X -\\mu) \\qquad \\text{the 3rd term is zero} \\\\ &amp;= \\text{Var}(X) + [E(X)]^2 \\end{split} \\]↩︎ "],
["univariate-distributions.html", "Chapter 14 Univariate Distributions 14.1 Uniform Distribution 14.2 Normal Distribution 14.3 Binomial Distribution and Beta Distribution 14.4 Poisson Distribution 14.5 Exponential Distribution and Gamma Distribution 14.6 Beta Distribution", " Chapter 14 Univariate Distributions 14.1 Uniform Distribution \\[ f_X(x) = \\begin{cases} \\frac{1}{a - b} &amp; a \\le x \\le b \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] 14.2 Normal Distribution 14.2.1 Log Normal Distribution 14.3 Binomial Distribution and Beta Distribution 14.4 Poisson Distribution \\[ P(X = x) = \\frac{\\lambda ^x e^{-\\lambda}}{x!} \\] 14.4.1 Poisson Process 14.5 Exponential Distribution and Gamma Distribution The exponential distribution describes the length of time between occurrences, and can be derived directly from poisson distribution. Given a poisson random variable \\(X \\sim \\text{Poisson}(\\lambda)\\), we compute the probability of the time \\(T\\) to the first occurence is greater than \\(t\\) \\[ P(T &gt; t) = P(X = 0; \\lambda t) = e^{-\\lambda t} \\] Note that poisson distribution of \\(X\\) is paramterized by $$ this time. This is because in a poisson process, if events occur on a average at the rate of \\(\\lambda\\) per unit of time, then there will be on average \\(\\lambda t\\) occurrences per \\(t\\) units of time. Conversely, the probability that an event does occur during \\(t\\) units of time is given by \\[ P(T \\le t) = 1 - e^{-\\lambda t} \\] The pdf of \\(T\\) is the derivative of \\(P(T \\le t)\\) \\[ f(T) = \\lambda e^{-\\lambda t} \\] After notation changes \\(T \\rightarrow X\\), we conclude that a nonnegative random variable \\(X\\) has the exponential distribution with parameter \\(\\lambda &gt; 0\\) if it has pdf \\[ f(X) = -\\lambda e^{\\lambda x} \\] \\(X\\) is denoted \\[ X \\sim \\text{Exp}(\\lambda) \\] \\[ E(X) = \\frac{1}{\\lambda} \\\\ \\text{Var}(X) = \\frac{1}{\\lambda^2} \\] The gamma distribution can be parameterized in terms of a shape parameter \\(\\alpha\\) and an rate parameter \\(\\beta\\). When \\(\\alpha\\) is a positive integer, the Gamma distribution describes the the sum of \\(\\alpha\\) independent exponentially distributed random variables, each of which has a mean of \\(1/\\beta\\). A random variable \\(X\\) that is gamma-distributed with shape \\(\\alpha\\) and rate \\(\\beta\\) has pdf \\[ f(x) = \\frac{\\beta^\\alpha x^{\\alpha - 1}e^{-\\beta x}}{\\Gamma (\\alpha)} \\qquad \\] More on Gamma function (denominator).6 \\(X\\) is denoted \\[ X \\sim \\Gamma(\\alpha, \\beta) \\equiv \\text{Gamma}(\\alpha, \\beta) \\] 14.5.1 Properties 14.5.2 Inverse Gamma Distribution 14.6 Beta Distribution The Gamma function \\(\\Gamma\\) is one commonly used extension of the factorial function to complex numbers. For positive integer \\(n\\), \\(\\Gamma(n) = (n - 1)!\\). For complex numbers with a positive real part the gamma function is defined via a convergent improper integral \\[\\Gamma(z) = \\int_{0}^{\\infty}x^{z -1}e^{-x} dx\\]↩︎ "],
["multivariate-distributions.html", "Chapter 15 Multivariate Distributions 15.1 Multivariate Normal Distribution 15.2 Dirichlet Distributon", " Chapter 15 Multivariate Distributions 15.1 Multivariate Normal Distribution Corollary 15.1 If \\(\\boldsymbol{Y} \\sim N(\\boldsymbol{0}, \\sigma^ 2I)\\), and \\(Q\\) is an orthogonal matrix, then \\(Q^T\\boldsymbol{Y}\\) still follows multivariate normal distribution with mean \\(\\boldsymbol{0}\\) and variance-covariance matrix \\(\\sigma^2 I\\) \\[ \\mathbb{E}(Q^T\\boldsymbol{Y}) = Q^T\\mathbb{E}(\\boldsymbol{Y}) = \\boldsymbol{0} \\] \\[ \\text{Var}(Q^T\\boldsymbol{Y}) = Q^T \\text{Var}(\\boldsymbol{Y})Q = \\sigma^2 I Q^TQ = \\sigma^2I \\] 15.1.1 Chi-square Distribution Corollary 15.2 If \\(\\boldsymbol{Y} \\sim N(\\boldsymbol{0}, I)\\), and \\(A\\) is an idempotent matrix. Then \\(\\boldsymbol{Y}^TA\\boldsymbol{Y}\\) follows \\(\\chi^2_m\\), where \\(m\\) is the number of \\(A\\)’s eigenvalues equall to \\(1\\). Proof 15.2 Dirichlet Distributon "],
["markov-chain.html", "Chapter 16 Markov Chain", " Chapter 16 Markov Chain Definition 16.1 (Markov chain) Definition 16.2 (Irreducible and reducible chain) Definition 16.3 (Aperiodic and periodic chain) "],
["the-learning-problem-framework.html", "Chapter 17 The Learning Problem Framework 17.1 The PAC Learning Framework", " Chapter 17 The Learning Problem Framework 17.1 The PAC Learning Framework Let \\(\\mathcal{X}\\) be the input space containing all data points or instances \\(\\mathcal{Y}\\) be the output space "],
["basics-of-optimization.html", "Chapter 18 Basics of Optimization 18.1 Univariate Optimization 18.2 Multivariate Optimization 18.3 Convex Functions 18.4 Method of Lagrange Multiplier", " Chapter 18 Basics of Optimization 18.1 Univariate Optimization Theorem 18.1 (Second derivative test for local extrema) Suppose \\(f&#39;&#39;(x)\\) is continuous on an open interval that contains \\(x = c\\) If \\(f&#39;(c) = 0\\) and \\(f&#39;&#39;(c) &gt; 0\\), then \\(f(x)\\) has a local minimum at \\(x = c\\) If \\(f&#39;(c) = 0\\) and \\(f&#39;&#39;(c) &lt; 0\\), then \\(f(x)\\) has a local maximum at \\(x = c\\) If \\(f&#39;(c) = 0\\) and \\(f&#39;&#39;(c) = 0\\), the the test fails. \\(f(x)\\) my have a local maximum, a local minimum, or neither 18.2 Multivariate Optimization 18.3 Convex Functions Definition 18.1 (Convext set) A set \\(S\\) is said to be convex, if for every pair of points \\(\\bar{w}_1, \\bar{w}_2 \\in S\\), point \\(\\lambda\\bar{w}_1 + (1 - \\lambda)\\bar{w}_2\\) must also be in \\(S\\) for all \\(\\lambda \\in (0, 1)\\) Intuitively, convexity means if we connect \\(\\bar{w}_1, \\bar{w}_2\\) with a straight line, then all points along the line must be in set \\(S\\). For example, an open interval \\((a, b)\\) on the 1-dimensional number line is a convex set. An property of convex set \\(S\\) is that, given \\(r\\) points \\(\\bar{w}_1, ..., \\bar{w}_r\\) and \\(r\\) nonnegative numbers \\(\\lambda_1, ..., \\lambda_r\\) such that \\(\\lambda_1 + \\cdots + \\lambda_r = 1\\) ,the affine combination \\[ \\sum _{i=1}^r\\lambda_i\\bar{w}_k \\] belongs to \\(s\\). As the definition of a convex set is the case \\(r = 2\\), this property characterizes convex sets. Definition 18.2 (Convex function) Let \\(f(\\cdot)\\) be a real-valued function with a convex domain, so that if \\(f(\\bar{w}_1), f(\\bar{w}_2)\\) is defined, for any \\(\\lambda \\in (0, 1)\\), the expression \\(f\\big( \\lambda\\bar{w}_1 + (1- \\lambda)\\bar{w}_2\\big)\\) is defined. For any \\(\\lambda \\in (0, 1)\\) and \\(\\bar{w}_1, \\bar{w}_2\\) \\(f\\) is called convex if \\[ f\\big(\\lambda\\bar{w}_1 + (1 - \\lambda)\\bar{w}_2\\big) \\le \\lambda f(\\bar{w}_1) + (1-\\lambda)f(\\bar{w}_2) \\] \\(f\\) is called strictly convex if \\[ f\\big(\\lambda\\bar{w}_1 + (1 - \\lambda)\\bar{w}_2\\big) \\lt \\lambda f(\\bar{w}_1) + (1-\\lambda)f(\\bar{w}_2) \\] 18.4 Method of Lagrange Multiplier "],
["gradient-descent.html", "Chapter 19 Gradient Descent", " Chapter 19 Gradient Descent "],
["linear-models.html", "Chapter 20 Linear Models 20.1 Ordinary Least Squares 20.2 Weighted Least Squares 20.3 Partial Least Squares 20.4 Regularized Regression", " Chapter 20 Linear Models 20.1 Ordinary Least Squares 20.1.1 Least Square Estimation From theorem 3.3 we know that \\[ \\bar{\\beta} = (X^TX)^{-1}X^T\\bar{y} \\] thus: \\[ \\hat{\\bar{y}} = X\\bar{\\beta} = X (X^TX)^{-1}X^{T}\\bar{y} \\] When columns of the design matrix \\(A\\) are orthogonal, the orthogonal projection of \\(\\bar{y}\\) onto \\(\\mathcal{R}(X)\\) is given by \\[ \\hat{\\bar{y}} = \\frac{\\bar{x}_0^T\\bar{y}}{\\bar{x}_0^T\\bar{x}_0}\\bar{x}_0 + \\frac{\\bar{x}_1^T\\bar{y}}{\\bar{x}_1^T\\bar{x}_1}\\bar{x}_1 \\cdots + \\frac{\\bar{x}_p^T\\bar{y}}{\\bar{x}_p^T\\bar{x}_p}\\bar{x}_p \\] ### Maximum Likelihood Estimation The log likelihood function is given by 20.2 Weighted Least Squares 20.3 Partial Least Squares 20.4 Regularized Regression 20.4.1 Ridge Regression 20.4.2 Lasso 20.4.3 Elastic Net "],
["principal-component-analysis.html", "Chapter 21 Principal Component Analysis", " Chapter 21 Principal Component Analysis "],
["text-mining.html", "Chapter 22 Text Mining", " Chapter 22 Text Mining "],
["references.html", "References", " References Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. 2012. Learning from Data. AMLBook. Aggarwal, Charu C. 2020. Linear Algebra and Optimization for Machine Learning - A Textbook. Springer. https://doi.org/10.1007/978-3-030-40344-7. Bertsekas, D. P., and J. N. Tsitsiklis. 2008. Introduction to Probability. Athena Scientific Optimization and Computation Series. Athena Scientific. https://books.google.com.hk/books?id=yAy-PQAACAAJ. Boyd, S., and L. Vandenberghe. 2018. Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares. Cambridge University Press. https://books.google.com/books?id=IApaDwAAQBAJ. DasGupta, Anirban. 2011. Probability for Statistics and Machine Learning: Fundamentals and Advanced Topics. 1st ed. Springer Publishing Company, Incorporated. Deisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020. Mathematics for Machine Learning. Gentle, James E. 2017. Matrix Algebra: Theory, Computations, and Applications in Statistics. 2bd ed. Springer Publishing Company, Incorporated. Johnston, Nathaniel. 2020. Advanced Linear and Matrix Algebra. Springer International Publishing. https://www.springer.com/us/book/9783030528140. K. Blitzstein, Joseph, and Jessica Hwang. 2019. Introduction to Probability Second Edition. https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view. Lay, David. 2006. Linear Algebra and Its Applications. Vols. 3:CD-ROM. Pearson, Addison Wesley. Strang, Gilbert. 2006. Linear Algebra and Its Applications. Belmont, CA: Thomson, Brooks/Cole. http://www.amazon.com/Linear-Algebra-Its-Applications-Edition/dp/0030105676. Weir, M. D., G. B. Thomas, and J. Hass. 2018. Thomas’ Calculus. 14th ed. Addison-Wesley. Xie, Yihui. 2020. Bookdown: Authoring Books and Technical Documents with R Markdown. https://CRAN.R-project.org/package=bookdown. "]
]
