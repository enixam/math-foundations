[
["index.html", "Mathematical Foundations Preface", " Mathematical Foundations Qiushi Yan 2020-07-03 Preface This notebook documents my self-study on math foundations of data science, including linear algebra, probability theory, statistics and possibly optimization. Some references (books, online courses, videos, papers) include: Linear Algebra and its Applications (Lay 2006) Linear Algebra and its Applications (Strang 2006) Linear Algebra Review and Reference on Stanford’s cs229 website Gilbert Strang. 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. Spring 2018. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA. "],
["matrix-algebra.html", "Chapter 1 Matrix Algebra ", " Chapter 1 Matrix Algebra "],
["elemetary-matrix-and-row-operations.html", "1.1 Elemetary matrix and row operations", " 1.1 Elemetary matrix and row operations An elementary matrix is one that is obtained by performing a single elementary row operation on an identity matrix \\(I\\). Each elementary matrix \\(E\\) is invertible. The inverse of \\(E\\) is the elementary matrix of the same type that transforms \\(E\\) back into \\(I\\). "],
["matrix-multiplication.html", "1.2 Matrix multiplication", " 1.2 Matrix multiplication A common way of looking at matrix-vector multiplication \\(A\\boldsymbol{x}\\) is to think of as a linear combination of column vectors in \\(A\\): \\[ \\begin{aligned} A\\boldsymbol{x} &amp;= [\\boldsymbol{a}_1 \\cdots \\boldsymbol{a}_n] \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\\\ &amp;= x_1\\boldsymbol{a}_1 + \\cdots + x_n\\boldsymbol{a}_n \\end{aligned} \\] Likewise, for any \\(\\boldsymbol{x}^{T} = [x_1, \\cdots, x_n]^T\\) and matrix \\(A_{m \\times n}\\), \\(x^{T}A\\) can be thought of as a linear combination of rows in \\(A\\) to produce a new row vector: \\[ [x_1, \\cdots, x_n] \\begin{bmatrix} \\boldsymbol{a}_1^T \\\\ \\vdots \\\\ \\boldsymbol{a}_n^T \\end{bmatrix} = x_1\\boldsymbol{a}_1^T + \\dots + x_n\\boldsymbol{a}_n^T \\] For matrix-matrix multiplication \\(AB\\), besides the dot product definition we can see it as column row expansion. Theorem 1.1 Column-row expansion of \\(AB\\) if \\(A\\) is \\(m \\times n\\) and \\(B\\) is \\(n \\times p\\), then \\[ \\begin{aligned} AB &amp;= [\\text{col}_1(A) \\cdots \\text{col}_n(A)] \\begin{bmatrix} \\text{row}_1(B) \\\\ \\vdots \\\\ \\text{row}_n(B) \\end{bmatrix} \\\\ &amp;= \\text{col}_1(A)\\text{row}_1(B) + \\cdots + \\text{col}_n(A)\\text{row}_n(B) \\end{aligned} \\] Note that each \\(\\text{col}_1(A)\\text{row}_1(B)\\) is a rank 1 \\(m \\times p\\) matrix. "],
["arithmetic-properties.html", "1.3 Arithmetic properties", " 1.3 Arithmetic properties 1.3.1 Inverse of a matrix Theorem 1.2 If A and B are both invertible matrces, we have \\[ (AB)^{-1} = B^{-1}A^{-1} \\] \\[ (A^{-1})^{-1} = A \\] \\[ (A^T)^{-1} = (A^{-1})^T \\] "],
["lu-factorization.html", "1.4 LU factorization", " 1.4 LU factorization A factorization a matrix A is an equation that expresses A as a product of two or more matrices "],
["subspaces.html", "1.5 4 subspaces", " 1.5 4 subspaces "],
["matrix-multiplication-as-linear-transformation.html", "1.6 Matrix multiplication as linear transformation", " 1.6 Matrix multiplication as linear transformation The equation \\(A_{m \\times n} \\, x _{ n \\times 1} = b_{m \\times 1}\\) can arise in a way that is not directly connected with linear combination of column vectors. That is, we think of the matrix \\(A\\) as an force that “acts” on a vector \\(x\\) in \\(R^n\\) by multiplication to produce a new vector called \\(b\\) in \\(R^m\\). A transformation \\(T\\) from \\(R^n\\) to \\(R^m\\) is a rule that assigns each vector in \\(R^n\\) a vector \\(T(x)\\) in \\(R^m\\), which is called the image of (under the action of \\(T\\)). It can be show that such transformations induced by multiplying a matrix is a type of linear transformation, because it satisfies all required properties to be linear: \\[ \\begin{aligned} \\text{vector addition} \\quad A(\\boldsymbol{u} + \\boldsymbol{v}) &amp;= A\\boldsymbol{u} + A\\boldsymbol{v} \\\\ \\text{scalar multiplication} \\quad A(c\\boldsymbol{u}) &amp;= cA\\boldsymbol{u} \\end{aligned} \\] Theorem 1.3 There is a one to one relationship between a linear transformation and a matrix. Let \\(T: R^n \\rightarrow R^m\\) be a linear transformation. Then there exists a unique matrix \\(A\\) such that: \\[ T(x) = Ax \\quad \\text{for all} \\; x \\; \\text{in} \\; R^n \\] In fact, \\(A\\) is a \\(m \\times n\\) matrix whose \\(j\\)th column is the vector \\(T(\\boldsymbol{e_j})\\), where \\(\\boldsymbol{e_j}\\) is the \\(j\\)th basis of \\(R^n\\) Proof: \\[ \\boldsymbol{x} = x_1\\boldsymbol{e_1} + \\dots + x_n{\\boldsymbol{e_n}} \\] And because \\(T(\\boldsymbol{x})\\) is a linear transformation: \\[ \\begin{split} T(\\boldsymbol{x}) &amp;= x_1T(\\boldsymbol{e_1}) + \\dots + x_nT(\\boldsymbol{e_n}) \\\\ &amp;= [T(\\boldsymbol{e_1}) \\, \\cdots \\, T(\\boldsymbol{e_n})]\\boldsymbol{x} \\\\ &amp;= (A\\boldsymbol{x})_{m \\times 1} \\end{split} \\] In other words, the transformation is specified once we know what all basis in \\(R^n\\) become in \\(R^m\\). The matrix \\(A\\) is called the standard matrix for the linear transformation \\(T\\). Definition 1.1 A mapping \\(T: R^n \\rightarrow R^m\\) is said to be onto R^m if each \\(\\boldsymbol{b}\\) in \\(R^m\\) is the image of at least one \\(\\boldsymbol{x}\\) in R^n Equivalently, \\(T\\) is onto \\(R^m\\) means that there exists at least one solution of \\(T(\\boldsymbol{x}) = \\boldsymbol{b}\\) Definition 1.2 A mapping T: R^n R^m$ is said to be one-to-one ifi each \\(\\boldsymbol{b}\\) in \\(R^m\\) is the image of at most one \\(\\boldsymbol{x}\\) in \\(R^n\\) Equivalently, \\(T\\) is one-to-one if, for each \\(\\boldsymbol{b}\\) in \\(R^m\\), the equation \\(T(\\boldsymbol{x}) = \\boldsymbol{b}\\) has either a unique solution or none at all. "],
["special-matrices-and-their-properties.html", "1.7 Special matrices and their properties", " 1.7 Special matrices and their properties 1.7.1 Orthogonal matrices An orthogonal matrix is a square matrix whose inverse is its transpose: \\[ \\tag{1.1} AA^T = A^TA = I \\] Another way of defining it is that an orthogonal matrix has both orthogonal columns and orthogonal rows. "],
["eigen-values-and-quadratic-forms.html", "Chapter 2 Eigen values and quadratic forms", " Chapter 2 Eigen values and quadratic forms "],
["references.html", "References", " References Lay, David. 2006. Linear Algebra and Its Applications. Vols. 3:CD-ROM. Pearson, Addison Wesley. Strang, Gilbert. 2006. Linear Algebra and Its Applications. Belmont, CA: Thomson, Brooks/Cole. http://www.amazon.com/Linear-Algebra-Its-Applications-Edition/dp/0030105676. "]
]
