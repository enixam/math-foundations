[
["index.html", "Mathematical Notes for Machine Learning Preface", " Mathematical Notes for Machine Learning Qiushi Yan 2020-07-18 Preface This notebook collects various mathematical topics relevant to machine learning problems, including linear algebra, calculus, probability theory, statistics and (possibly) optimization. References (books, online courses, videos, papers) include: Linear Algebra Linear Algebra and its Applications (Lay 2006) Linear Algebra and Optimization for Machine Learning (Aggarwal 2020) Linear Algebra and its Applications (Strang 2006) Mathematics for Machine Learning (Deisenroth, Faisal, and Ong 2020) Linear Algebra Review and Reference on Stanford’s cs229 website Gilbert Strang. 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. Spring 2018. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA. "],
["matrix-basics.html", "Chapter 1 Matrix basics ", " Chapter 1 Matrix basics "],
["elemetary-matrix-and-row-operations.html", "1.1 Elemetary matrix and row operations", " 1.1 Elemetary matrix and row operations An elementary matrix is one that is obtained by performing a single elementary row operation on an identity matrix \\(I\\). Each elementary matrix \\(E\\) is invertible. The inverse of \\(E\\) is the elementary matrix of the same type that transforms \\(E\\) back into \\(I\\). "],
["matrix-multiplication.html", "1.2 Matrix multiplication", " 1.2 Matrix multiplication A common way of looking at matrix-vector multiplication \\(A\\boldsymbol{x}\\) is to think of as a linear combination of column vectors in \\(A\\): \\[ \\begin{aligned} A\\boldsymbol{x} &amp;= [\\boldsymbol{a}_1 \\cdots \\boldsymbol{a}_n] \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\\\ &amp;= x_1\\boldsymbol{a}_1 + \\cdots + x_n\\boldsymbol{a}_n \\end{aligned} \\] Likewise, for any \\(\\boldsymbol{x}^{T} = [x_1, \\cdots, x_n]^T\\) and matrix \\(A_{m \\times n}\\), \\(x^{T}A\\) can be thought of as a linear combination of rows in \\(A\\) to produce a new row vector: \\[ [x_1, \\cdots, x_n] \\begin{bmatrix} \\boldsymbol{a}_1^T \\\\ \\vdots \\\\ \\boldsymbol{a}_n^T \\end{bmatrix} = x_1\\boldsymbol{a}_1^T + \\dots + x_n\\boldsymbol{a}_n^T \\] For matrix-matrix multiplication \\(AB\\), besides the dot product definition we can see it as column row expansion. Theorem 1.1 (Column-row expansion of \\(AB\\)) if \\(A\\) is \\(m \\times n\\) and \\(B\\) is \\(n \\times p\\), then \\[ \\begin{aligned} AB &amp;= [\\text{col}_1(A) \\cdots \\text{col}_n(A)] \\begin{bmatrix} \\text{row}_1(B) \\\\ \\vdots \\\\ \\text{row}_n(B) \\end{bmatrix} \\\\ &amp;= \\text{col}_1(A)\\text{row}_1(B) + \\cdots + \\text{col}_n(A)\\text{row}_n(B) \\end{aligned} \\] Note that each \\(\\text{col}_1(A)\\text{row}_1(B)\\) is a rank 1 \\(m \\times p\\) matrix. "],
["lu-factorization.html", "1.3 LU factorization", " 1.3 LU factorization https://fml-fam.github.io/blog/2020/07/03/matrix-factorizations-for-data-analysis/ A factorization a matrix \\(A\\) is an equation that expresses \\(A\\) as a product of two or more matrices. Definition 1.1 (LU factorization) Suppose \\(A\\) can be reduced to an echelon form \\(U\\) using row operations that add a multiple ofo oone row to another row below it, there exist a set of unit lower trangular matrices \\(E_1, \\dots, E_p\\) such that \\[ E_p \\cdots E_1A = U \\] Then \\[ A = (E_p \\cdots E_1)^{-1}U = LU \\] where \\[ L = (E_p \\cdots E_1)^{-1} \\] "],
["determinants.html", "1.4 Determinants", " 1.4 Determinants 1.4.1 Properties of determinants A list of arithmetic properties of determinants, A is an \\(n\\times n\\) matrix: \\(\\det(A^T) = \\det(A)\\) \\(\\det(kA) = k^n \\det(A)\\) \\(\\det(AB) = \\det(A)\\det(B)\\) (although \\(AB \\not = BA\\) in general) \\(\\det(A^{-1}) = 1 / \\det(A)\\), if \\(A\\) is invertible \\(\\det(A^n) = \\det(A)^n\\) determinant is equal to the product of eigenvalues(counting multiplicity) \\(\\det(A) = \\prod_{i=1}^n{\\lambda_i}\\) 1.4.2 Cramer’s rule Given an \\(n \\times n\\) matrix \\(A\\) and \\(\\boldsymbol{b}\\) in \\(\\mathbb{\\mathbb{R^n}}\\), denote \\(A_i(\\boldsymbol{b})\\) as the matrix derived by \\(A\\) by replacing column \\(i\\) by vector \\(\\boldsymbol{b}\\): \\[ A_i(\\boldsymbol{b}) = [\\boldsymbol{a}_1 \\cdots \\underbrace{\\boldsymbol{b}}_{\\text{column} \\,i} \\cdots \\boldsymbol{a}_n] \\] Theorem 1.2 (Cramer’s rule) Let \\(A\\) be an invertible \\(n \\times n\\) matrix. For any \\(\\boldsymbol{b}\\) \\(\\mathbb{R^n}\\), the unique solution \\(\\boldsymbol{x}\\) of \\(A\\boldsymbol{x} = \\boldsymbol{b}\\) has entries given by: \\[ x_i = \\frac{\\det A_i(\\boldsymbol{b})}{\\det A} \\] "],
["trace.html", "1.5 Trace", " 1.5 Trace The trace of square matrix \\(A\\) is the sum of its diagonal entries \\(\\sum_{i = 1}^{n}A_{ii}\\). The trace has the following properties: \\(\\text{tr}(A + B) = \\text{tr}A + \\text{tr}B\\) \\(\\text{tr}(kA) = k\\text{tr}A\\), \\(k\\) is a scalar \\(\\text{tr}(A^T) = \\text{tr}(A)\\) For \\(A\\), \\(B\\) such that \\(AB\\) is square, \\(\\text{tr}(AB) = \\text{tr}(BA)\\) Trace of product of multiple matrices is invariant to cyclic permutations, \\(\\text{tr}(ABC) = \\text{tr}(BCA) = \\text{tr}(CAB)\\). Note that the reordering cannot be done arbitrarily, fro example \\(\\text{tr}(ABC) \\not= \\text{tr}(ACG)\\) in general. Trace is equal to the sum of its eigenvalues (repeated accordin gto multiplicity) \\(\\text{tr}(A) = \\sum_{i = 1}^n{\\lambda_i}\\) \\(\\text{tr}(\\boldsymbol{a}\\boldsymbol{a}^T) = \\boldsymbol{a}^T\\boldsymbol{a}\\), a is a column vector "],
["inverse-of-a-matrix.html", "1.6 Inverse of a matrix", " 1.6 Inverse of a matrix Note that the inverse of a matrix is only defined for square matrces, so is determinants in Section 1.4 Assume that \\(A\\) and \\(B\\) are both non-singular Theorem 1.3 If A and B are both invertible matrces, we have \\[ (A^{-1})^{-1} = A \\] \\[ (AB)^{-1} = B^{-1}A^{-1} \\] \\[ (A^T)^{-1} = (A^{-1})^T \\] In practice \\(A^{-1}\\) is seldom computed, because computing both \\(A^{-1}\\) and \\(A^{-1}\\boldsymbol{b}\\) to solve linear equations takes about 3 times as many arithmetic operations as solving \\(A\\boldsymbol{x} = \\boldsymbol{b}\\) by row reduction. Cramer’s rule 1.2 "],
["matrix-multiplication-as-linear-transformation.html", "1.7 Matrix multiplication as linear transformation", " 1.7 Matrix multiplication as linear transformation The equation \\(A_{m \\times n} \\, x _{ n \\times 1} = b_{m \\times 1}\\) can arise in a way that is not directly connected with linear combination of column vectors. That is, we think of the matrix \\(A\\) as an force that “acts” on a vector \\(x\\) in \\(\\mathbb{R^n}\\) by multiplication to produce a new vector called \\(b\\) in \\(\\mathbb{\\mathbb{R^m}}\\). A transformation \\(T\\) from \\(\\mathbb{R^n}\\) to \\(\\mathbb{R^m}\\) is a rule that assigns each vector in \\(\\mathbb{R^n}\\) a vector \\(T(x)\\) in \\(\\mathbb{R^m}\\), which is called the image of (under the action of \\(T\\)). It can be show that such transformations induced by multiplying a matrix is a type of linear transformation, because it satisfies all required properties to be linear: \\[ \\begin{aligned} \\text{vector addition} \\quad A(\\boldsymbol{u} + \\boldsymbol{v}) &amp;= A\\boldsymbol{u} + A\\boldsymbol{v} \\\\ \\text{scalar multiplication} \\quad A(c\\boldsymbol{u}) &amp;= cA\\boldsymbol{u} \\end{aligned} \\] Theorem 1.4 (left multiplication as linear transformation) There is a one to one relationship between a linear transformation and a matrix. Let \\(T: \\mathbb{R^n} \\rightarrow \\mathbb{R^m}\\) be a linear transformation. Then there exists a unique matrix \\(A\\) such that: \\[ T(x) = Ax \\quad \\text{for all} \\; x \\; \\text{in} \\; \\mathbb{R^n} \\] In fact, \\(A\\) is a \\(m \\times n\\) matrix whose \\(j\\)th column is the vector \\(T(\\boldsymbol{e_j})\\), where \\(\\boldsymbol{e_j}\\) is the \\(j\\)th basis of \\(\\mathbb{R^n}\\) Proof: \\[ \\boldsymbol{x} = x_1\\boldsymbol{e_1} + \\dots + x_n{\\boldsymbol{e_n}} \\] And because \\(T(\\boldsymbol{x})\\) is a linear transformation: \\[ \\begin{split} T(\\boldsymbol{x}) &amp;= x_1T(\\boldsymbol{e_1}) + \\dots + x_nT(\\boldsymbol{e_n}) \\\\ &amp;= [T(\\boldsymbol{e_1}) \\, \\cdots \\, T(\\boldsymbol{e_n})]\\boldsymbol{x} \\\\ &amp;= (A\\boldsymbol{x})_{m \\times 1} \\end{split} \\] In other words, the transformation is specified once we know what all basis in \\(\\mathbb{R^n}\\) become in \\(\\mathbb{R^m}\\). The matrix \\(A\\) is called the standard matrix for the linear transformation \\(T\\). Definition 1.2 (A mapping is onto \\(\\mathbb{R^m}\\)) A mapping \\(T: \\mathbb{R^n} \\rightarrow \\mathbb{R^m}\\) is said to be onto if each \\(\\boldsymbol{b}\\) in \\(\\mathbb{R^m}\\) is the image of at least one \\(\\boldsymbol{x}\\) in Equivalently, \\(T\\) is onto \\(\\mathbb{R^m}\\) means that there exists at least one solution of \\(T(\\boldsymbol{x}) = \\boldsymbol{b}\\) Definition 1.3 (one-to-one mapping) A mapping T: \\(\\mathbb{R^n} \\rightarrow \\mathbb{R^m}\\) is said to be one-to-one if each \\(\\boldsymbol{b}\\) in \\(\\mathbb{R^m}\\) is the image of at most one \\(\\boldsymbol{x}\\) in \\(\\mathbb{R^n}\\) Equivalently, \\(T\\) is one-to-one if, for each \\(\\boldsymbol{b}\\) in \\(\\mathbb{R^m}\\), the equation \\(T(\\boldsymbol{x}) = \\boldsymbol{b}\\) has either a unique solution or none at all. "],
["statistics-and-proabability.html", "1.8 Statistics and proabability", " 1.8 Statistics and proabability 1.8.1 Sample statistics "],
["vector-spaces.html", "Chapter 2 Vector spaces", " Chapter 2 Vector spaces \\[ \\langle \\cdot, \\cdot \\rangle \\] "],
["four-subspaces.html", "2.1 Four subspaces", " 2.1 Four subspaces "],
["eigenvalues-and-and-eigenvectors.html", "Chapter 3 Eigenvalues and and eigenvectors ", " Chapter 3 Eigenvalues and and eigenvectors "],
["diagnolization.html", "3.1 Diagnolization", " 3.1 Diagnolization Definition 3.1 Diagonalization theorem An \\(n \\ times n\\) matrix \\(A\\) is diagnolizable if and only if A has \\(n\\) independent linearly independent eigenvectors. In such case, in \\(A = PDP^{-1}\\), the diagonal entries of \\(D\\) are eigenvalues that correpond, respectively, to the eigenvectors of in \\(P\\) In other words, \\(A\\) is diagnolizable if and only if there are enough eigenvectors in form a basis of \\(R^n\\), called an eigenvector basis of \\(R^n\\) Proof \\[ \\begin{split} AP &amp;= A[\\boldsymbol{v}_1 \\cdots \\boldsymbol{v}_n] \\\\ &amp;= [A\\boldsymbol{v}_1 \\cdots A\\boldsymbol{v}_n] \\\\ &amp;= [\\lambda_1\\boldsymbol{v}_1 \\cdots \\lambda_n\\boldsymbol{v}_n] \\end{split} \\] while on the other side of the equation: \\[ \\begin{aligned} DP &amp;= [\\boldsymbol{v}_1 \\cdots \\boldsymbol{v}_n] \\begin{bmatrix} \\lambda_1 &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; \\lambda_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\lambda_n \\end{bmatrix} \\\\ &amp;= [\\lambda_1\\boldsymbol{v}_1 \\cdots \\lambda_n\\boldsymbol{v}_n] \\end{aligned} \\] So that \\[ \\begin{aligned} AP &amp;= PD \\\\ A &amp;= PDP^{-1} \\end{aligned} \\] Because \\(P\\) contains \\(n\\) independent columns so it’s invertible. "],
["eigenvectors-and-linear-transformations.html", "3.2 Eigenvectors and linear transformations", " 3.2 Eigenvectors and linear transformations "],
["orthogonality.html", "Chapter 4 Orthogonality", " Chapter 4 Orthogonality Some definitions: If \\(\\boldsymbol{u}\\) and \\(\\boldsymbol{v}\\) and both vectors in \\(\\mathbb{\\mathbb{R^n}}\\), then the number \\(\\boldsymbol{u}^T\\boldsymbol{v}\\) is called the inner product of \\(\\boldsymbol{u}\\) and \\(\\boldsymbol{v}\\), also denoted by \\(\\langle\\boldsymbol{u}, \\boldsymbol{v}\\rangle\\) or \\(\\boldsymbol{u} \\cdot \\boldsymbol{v}\\). Also, when \\(\\boldsymbol{u}\\) is perpendicular to \\(\\boldsymbol{v}\\) we have \\(\\boldsymbol{u} \\cdot \\boldsymbol{v} = 0\\). Norm is a function from a vector space to a nonnegative scalar that satisfies certain properties. The L2 norm (most common measure of length of a vector) of \\(\\boldsymbol{v}\\) is \\(||\\boldsymbol{u}||\\) defined by \\[ ||\\boldsymbol{v}||_2 = \\sqrt{v_1^2 + \\cdots + v_n^2} = \\sqrt{\\boldsymbol{v}^T\\boldsymbol{v}} \\] The subscript \\(2\\) can be left out in most cases. Moreover, the L1 norm of \\(\\boldsymbol{v}\\) is \\[ ||\\boldsymbol{v}||_1 = |v_1| + \\cdots + |v_n| \\] More generally, the Lp norm is: \\[ ||\\boldsymbol{v}||_p = (\\sum_{i = 1}^n{x_i^p})^{1/p} \\] For all these definitions, it can be shown that \\[ ||c\\boldsymbol{v}|| = |c| \\times ||\\boldsymbol{v}|| \\] The Euclidean distance between \\(\\boldsymbol{u}\\) and \\(\\boldsymbol{v}\\) is the L2 norm of the vector \\(\\boldsymbol{u} - \\boldsymbol{v}\\) \\[ \\text{dist}(\\boldsymbol{u}, \\boldsymbol{v}) = ||\\boldsymbol{u} - \\boldsymbol{v}||_2 \\] "],
["orthogonal-decomposition.html", "4.1 Orthogonal decomposition", " 4.1 Orthogonal decomposition 4.1.1 Orthogonal complements if vector \\(\\boldsymbol{v}\\) is orthogonal to every vector in a subspace \\(W\\) of \\(\\mathbb{R^n}\\), then \\(\\boldsymbol{v}\\) is said to be orthogonal to \\(W\\). The subspace that contains the set of vectors that are orthogonal to \\(W\\) is called the orthogonal complement, denoted by \\(W^{\\perp}\\). This corresponds to discussions in 2.1, where \\[ (\\text{row} A)^{\\perp} = \\text{Nul} A \\\\ (\\text{col} A)^{\\perp} = \\text{Nul} A^T \\] 4.1.2 Orthogonal sets and orthogonal basis An orthogonal set is a set of vectors \\(\\{\\boldsymbol{u_1}, \\dots, \\boldsymbol{u_p}\\}\\) in \\(\\mathbb{R^n}\\), in which each pair of distinct vectors is orthogonal: \\(\\boldsymbol{u_i}^{T} \\boldsymbol{u_j} = 0 \\quad i\\not = j\\). Note that the set do not necessarily span the whole \\(\\mathbb{R^n}\\), but a subspace \\(W\\). Since vectors in orthogonal sets is mutually perpendicular, they must also be linearly independent and could form a basis for a subspace \\(W\\). In such case, they are called orthogonal basis. There is a particular advantage in using orthogonal basis rather than other basis, because we can find a easy representation of any vector in \\(W\\). Theorem 1.1 For each \\(\\boldsymbol{y}\\) in \\(W\\), we have the following linear combination \\[ y = c_1\\boldsymbol{u_1} + \\cdots + c_p\\boldsymbol{u_p} \\] and \\[ c_i = \\frac{\\boldsymbol{y} \\cdot \\boldsymbol{u_i}}{\\boldsymbol{u_i} \\cdot \\boldsymbol{u_i}} \\quad i = 1, \\cdots, p \\] where \\(\\{\\boldsymbol{u_1}, \\dots, \\boldsymbol{u_p}\\}\\) is an orthogonal basis. Proof \\[ \\begin{split} \\boldsymbol{u_1} \\cdot \\boldsymbol{y} &amp;= \\boldsymbol{u_1} \\cdot (c_1\\boldsymbol{u_1} + \\cdots + c_p\\boldsymbol{u_p}) \\\\ &amp;= c_1 \\boldsymbol{u_1} \\cdot \\boldsymbol{u_1} \\end{split} \\] So: \\[ c_1 = \\frac{\\boldsymbol{u_1} \\cdot \\boldsymbol{y}}{\\boldsymbol{u_1} \\cdot \\boldsymbol{u_1}} \\] Derivations for other \\(c_i\\) is similar. 4.1.3 Orthogonal decomposition Orthogonal decomposition split \\(\\boldsymbol{y}\\) in \\(\\mathbb{R^n}\\) into two vectors, one in \\(W\\) and one in its orthogonal compliment \\(W^{\\perp}\\). The trick is to use \\(\\hat{\\boldsymbol{y}}\\) as \\(\\boldsymbol{y}\\)’s projection onto \\(W\\), which can be represented as illustrated in 4.1.2, and the other term, often referred to as error term in statistics, is simply \\(\\boldsymbol{y}- \\hat{\\boldsymbol{y}}\\). \\[ \\boldsymbol{y} = \\hat{\\boldsymbol{y}} + \\boldsymbol{z}= c_1\\boldsymbol{u_1} + \\cdots + c_p\\boldsymbol{u_p} + \\boldsymbol{z} \\] Where \\[ c_i = \\frac{\\boldsymbol{y} \\cdot \\boldsymbol{u}_i}{\\boldsymbol{u}_i \\cdot \\boldsymbol{u}_i}\\boldsymbol{u}_i \\quad i = 1, \\cdots, p \\] 4.1.4 Best approximation Theorem 4.1 (The Best Approximation Theorem) Given \\(\\boldsymbol{y}\\) be any vector in \\(\\mathbb{R^n}\\), with its subspace \\(W\\), let \\(\\hat{\\boldsymbol{y}}\\) be the orthogonal projection of \\(\\boldsymbol{y}\\) onto \\(W\\). Then \\(\\hat{\\boldsymbol{y}}\\) is the closest point in \\(W\\) to \\(\\boldsymbol{y}\\) in the sense that \\[ ||\\boldsymbol{y} - \\hat{\\boldsymbol{y}}|| \\le ||\\boldsymbol{y} - \\boldsymbol{v}|| \\] PROOF Take \\(\\boldsymbol{v}\\) distinct from \\(\\hat{\\boldsymbol{y}}\\) in \\(W\\), we know that \\(\\boldsymbol{y} - \\hat{\\boldsymbol{y}}\\) is perpendicular to \\(\\boldsymbol{v}\\). According to Pythoagorean theorem, we have Figure 4.1: figure from page p352, ch6 (Lay 2006) \\[ ||\\boldsymbol{y}- \\boldsymbol{v}||^2 = ||\\boldsymbol{\\hat{y}} - \\boldsymbol{v}||^2 + ||\\boldsymbol{y} -\\boldsymbol{\\hat{y}}||^2 \\] When \\(\\boldsymbol{v}\\) is distinct from \\(\\boldsymbol{\\hat{y}}\\), \\(||\\boldsymbol{\\hat{y}} - \\boldsymbol{v}||^2\\) is non-negative, so the error term of choosing \\(\\boldsymbol{v}\\) is always larger than that of the orthogonal projection \\(\\boldsymbol{\\hat{y}}\\). "],
["orthonormal-sets-and-orthogonal-matrices.html", "4.2 Orthonormal sets and orthogonal matrices", " 4.2 Orthonormal sets and orthogonal matrices An orthogonal set whose components are all unit vectors are said to be orthonormal sets. 4.2.1 Orthogonal matrices An orthogonal matrix is a square matrix whose inverse is its transpose: \\[ \\tag{4.1} AA^T = A^TA = I \\] Another way of defining it is that an orthogonal matrix has both orthogonal columns and orthogonal rows. "],
["matrix-calculus.html", "Chapter 5 Matrix calculus", " Chapter 5 Matrix calculus \\[ \\frac{\\partial{f(x)}}{\\partial{x}} \\] "],
["symmetric-matrices-and-quadratic-forms.html", "Chapter 6 Symmetric matrices and quadratic forms ", " Chapter 6 Symmetric matrices and quadratic forms "],
["svd.html", "6.1 SVD", " 6.1 SVD "],
["taylor-series-and-expansion.html", "Chapter 7 Taylor series and expansion", " Chapter 7 Taylor series and expansion "],
["linear-models.html", "Chapter 8 Linear models ", " Chapter 8 Linear models "],
["least-square-estimation.html", "8.1 Least square estimation", " 8.1 Least square estimation From theorem 4.1 we know that Definition 3.1 the normal equation \\[ A^TA\\boldsymbol{x} = A^T\\boldsymbol{b} \\] \\[ \\boldsymbol{\\beta} = (X^TX)^{-1}X^T\\boldsymbol{y} \\] thus: \\[ \\hat{\\boldsymbol{y}} = X\\boldsymbol{\\beta} = X (X^TX)^{-1}X^{T}\\boldsymbol{y} \\] \\[ \\hat{\\boldsymbol{y}} = \\frac{\\boldsymbol{x}_0^T\\boldsymbol{y}}{\\boldsymbol{x}_0^T\\boldsymbol{x}_0}\\boldsymbol{x}_0 + \\frac{\\boldsymbol{x}_1^T\\boldsymbol{y}}{\\boldsymbol{x}_1^T\\boldsymbol{x}_1}\\boldsymbol{x}_1 \\cdots + \\frac{\\boldsymbol{x}_p^T\\boldsymbol{y}}{\\boldsymbol{x}_p^T\\boldsymbol{x}_p}\\boldsymbol{x}_p \\] "],
["maximum-likelihood-estimation.html", "8.2 Maximum likelihood estimation", " 8.2 Maximum likelihood estimation "],
["principle-component-analysis.html", "Chapter 9 Principle component analysis", " Chapter 9 Principle component analysis "],
["references.html", "References", " References Aggarwal, Charu C. 2020. Linear Algebra and Optimization for Machine Learning - A Textbook. Springer. https://doi.org/10.1007/978-3-030-40344-7. Deisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020. Lay, David. 2006. Linear Algebra and Its Applications. Vols. 3:CD-ROM. Pearson, Addison Wesley. Strang, Gilbert. 2006. Linear Algebra and Its Applications. Belmont, CA: Thomson, Brooks/Cole. http://www.amazon.com/Linear-Algebra-Its-Applications-Edition/dp/0030105676. "]
]
