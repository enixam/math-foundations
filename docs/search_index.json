[
["index.html", "Mathematical Notes for Machine Learning Preface", " Mathematical Notes for Machine Learning Qiushi Yan 2020-07-22 Preface This document aims to collect notes on various mathematical topics relevant to machine learning problems, including linear algebra, calculus, probability theory, statistics and (possibly) optimization. The document is generated by the bookdown package (Xie 2020). References (books, online courses, videos, papers) include: general Mathematics for Machine Learning by Garret Thomas Mathematics for Machine Learning (Deisenroth, Faisal, and Ong 2020) Linear Algebra Linear Algebra and its Applications (Lay 2006) Linear Algebra and Optimization for Machine Learning (Aggarwal 2020) Linear Algebra and its Applications (Strang 2006) Linear Algebra Review and Reference on Stanford’s cs229 website Gilbert Strang. 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. Spring 2018. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA. Probability Thoery Probability for Statistics and Machine Learning: Fundamentals and Advanced Topics (DasGupta 2011) John Tsitsiklis, and Patrick Jaillet. RES.6-012 Introduction to Probability. Spring 2018. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA. "],
["matrix-basics.html", "Chapter 1 Matrix basics 1.1 Matrix multiplication 1.2 Elemetary matrix and row operations 1.3 LU factorization 1.4 Determinants 1.5 Trace 1.6 Inverse of a matrix 1.7 Matrix multiplication as linear transformation 1.8 Statistics and proabability", " Chapter 1 Matrix basics 1.1 Matrix multiplication A common way of looking at matrix-vector multiplication \\(A\\boldsymbol{x}\\) is to think of as a linear combination of column vectors in \\(A\\): \\[ \\begin{aligned} A\\boldsymbol{x} &amp;= [\\boldsymbol{a}_1 \\;\\; \\cdots \\;\\; \\boldsymbol{a}_n] \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\\\ &amp;= x_1\\boldsymbol{a}_1 + \\cdots + x_n\\boldsymbol{a}_n \\end{aligned} \\] Likewise, for any \\(\\boldsymbol{x}^{T} = [x_1, \\cdots, x_n]^T\\) and matrix \\(A_{m \\times n}\\), \\(x^{T}A\\) can be thought of as a linear combination of rows in \\(A\\) to produce a new row vector: \\[ [x_1 \\;\\; \\cdots \\;\\; x_n] \\begin{bmatrix} \\boldsymbol{a}_1^T \\\\ \\vdots \\\\ \\boldsymbol{a}_n^T \\end{bmatrix} = x_1\\boldsymbol{a}_1^T + \\dots + x_n\\boldsymbol{a}_n^T \\] For matrix-matrix multiplication \\(AB\\), besides the dot product definition we can see it as column row expansion. Theorem 1.1 (Column-row expansion of \\(AB\\)) if \\(A\\) is \\(m \\times n\\) and \\(B\\) is \\(n \\times p\\), then \\[ \\begin{aligned} AB &amp;= [\\text{col}_1(A) \\cdots \\text{col}_n(A)] \\begin{bmatrix} \\text{row}_1(B) \\\\ \\vdots \\\\ \\text{row}_n(B) \\end{bmatrix} \\\\ &amp;= \\text{col}_1(A)\\text{row}_1(B) + \\cdots + \\text{col}_n(A)\\text{row}_n(B) \\end{aligned} \\] Note that each \\(\\text{col}_1(A)\\text{row}_1(B)\\) is a rank 1 \\(m \\times p\\) matrix. 1.2 Elemetary matrix and row operations An elementary matrix is one that is obtained by performing a single elementary row operation on an identity matrix \\(I\\). Each elementary matrix \\(E\\) is invertible. The inverse of \\(E\\) is the elementary matrix of the same type that transforms \\(E\\) back into \\(I\\). Left multiplication by a elementary matrix has a nice illustration. There are 3 primary types of elementary matrices (example for \\(3 \\times 3\\)): \\[ E_1 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ -4 &amp; 0 &amp; 1 \\end{bmatrix} ,\\; E_2 = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} ,\\; E_3 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 5 \\end{bmatrix} \\] \\(E_1, E_2, E_3\\) represents 3 types of elementary row operations applicable to a \\(3 \\times 3\\) matrix, (1) replacement; (2) interchange and (3) scaling. We can verify this by right multiply them with an arbitrary matrix \\(A\\) \\[ \\begin{aligned} E_1A &amp;= \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ - 4a_{11} + a_{31} &amp; -4a_{12} + a_{32} &amp; -4a_{13} + a_{33} \\end{bmatrix} \\\\ \\\\ E_2A &amp;= \\begin{bmatrix} a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{bmatrix} \\\\ \\\\ E_3A &amp;= \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ 5a_{31} &amp; 5a_{32} &amp; 5a_{33} \\\\ \\end{bmatrix} \\end{aligned} \\] Thus, any row operation on \\(A\\) is equivalent to left multiply a corresponding elementary matrix \\(E\\). Since row operation are invertible, elementary matrices are invertible. This gives a general way of finding the inverse matrix of \\(A\\). Theorem 1.2 (an algorithm for finding inverse matrices) Row reduce the augmented matrix \\([A \\;\\; I]\\), when \\(A\\) becomes \\(I\\), \\(I\\) becomes \\(A^{-1}\\). Otherwise \\(A^{-1}\\) is not invertible. 1.3 LU factorization https://fml-fam.github.io/blog/2020/07/03/matrix-factorizations-for-data-analysis/ A factorization a matrix \\(A\\) is an equation that expresses \\(A\\) as a product of two or more matrices. Definition 1.1 (LU factorization) Suppose \\(A\\) can be reduced to an echelon form \\(U\\) using row operations that add a multiple ofo oone row to another row below it, there exist a set of unit lower trangular matrices \\(E_1, \\dots, E_p\\) such that \\[ E_p \\cdots E_1A = U \\] Then \\[ A = (E_p \\cdots E_1)^{-1}U = LU \\] where \\[ L = (E_p \\cdots E_1)^{-1} \\] 1.4 Determinants 1.4.1 Cofactor expansion The \\((i, j)\\text{-cofactor}\\) of \\(A\\) is a number \\(C_{ij}\\) in \\(\\mathbb{R}\\) given by \\[ C_{ij} = (-1)^{i + j} \\det A_{ij} \\] where \\(A_{ij}\\) denotes the submatrix formed by deleting the \\(i\\)h row and \\(j\\)th column of \\(A\\). Theorem 1.3 (cofactor expansion) The determinant of an \\(n \\times n\\) matrix is given by a cofactor expasion across any row or column. For example, the expansion across the \\(i\\)th row is: \\[ \\det A = a_{i1}C_{i1} + a_{i2}C_{i2} + \\cdots + a_{in}C_{in} \\] and cross the \\(j\\)th column is \\[ \\det A = a_{1j}C_{1j} + a_{2j}C_{2j} + \\cdots + a_{nj}C_{nj} \\] 1.4.2 Geometric interpretation of determinant Given matrix \\(A_{n \\times n}\\) \\[ \\begin{bmatrix} a_1^{T} \\\\ a_2^{T} \\\\ \\vdots \\\\ a_n^{T} \\end{bmatrix} \\] where \\(a_1, ..., a_n\\) are row vectors of A. Then \\(|\\det A|\\) is the volume of parallelotope constrained by \\(a_1, ..., a_n\\). When \\(A\\) is \\(2\\times2\\), \\(|\\det A|\\) is simply the area of the parallelogram defined by two side \\(a_1, a_2\\) 1.4.3 Properties of determinant A list of arithmetic properties of determinants, A is an \\(n\\times n\\) matrix: \\(\\det(A^T) = \\det(A)\\) \\(\\det(kA) = k^n \\det(A)\\) \\(\\det(AB) = \\det(A)\\det(B)\\) (although \\(AB \\not = BA\\) in general), it follows that \\(\\det(A^n) = \\det(A)^n\\) \\(\\det(A^{-1}) = 1 / \\det(A)\\), if \\(A\\) is invertible determinant is equal to the product of eigenvalues(counting multiplicity) \\(\\det(A) = \\prod_{i=1}^n{\\lambda_i}\\) If the \\(i\\)-th row (column) in A is a sum of the \\(i\\)-th row (column) of a matrix \\(B\\) and the \\(i\\)-th row (column) of a matrix \\(C\\) and all other rows in \\(B\\) and \\(C\\) are equal to the corresponding rows in \\(A\\) (that is \\(B\\) and \\(C\\) differ from \\(A\\) by one row only), then \\(\\det(A)=\\det(B)+\\det(C)\\). This can be proven by cofactor expansion across the \\(i\\)th row. The same applies to columns. Row operations on \\(A\\) has the following effect on \\(\\det A\\) if we multiply a single row in \\(A\\) by a scalar \\(k \\in \\mathbb{R}\\), then the determinant of the new matrix is \\(k\\det A\\) if we exchange two rows \\(a_i^T\\) and \\(a_j^T\\) of \\(A\\), determinant becomes \\(-\\det A\\) Add a multiple of one row to another row has no effect on determinant The first two effects can be easily understood in connection with geometric meaning of determinant. As for the third one, let us represent A with row vectors \\[ A = \\begin{vmatrix} a_1^T \\\\ \\vdots \\\\ a_i^T \\\\ \\vdots \\\\ a_j^T \\\\ \\vdots \\\\ a_n^T \\end{vmatrix} \\] Then \\(B\\), after performing replacing (add a multiple of the \\(j\\)th row to the \\(i\\)th row) on \\(A\\), becomes \\[ B = \\begin{vmatrix} a_1^T \\\\ \\vdots \\\\ a_i^T + ka_j^T \\\\ \\vdots \\\\ a_j^T \\\\ \\vdots \\\\ a_n^T \\end{vmatrix} \\] By property 6 \\(\\det(A) = \\det(B) + \\det(C)\\) when \\(B\\) and \\(C\\) only differs from \\(A\\) by the same row. So \\(\\det A\\) can be broke down into two parts \\[ |A| = \\begin{vmatrix} a_1^T \\\\ \\vdots \\\\ a_i^T \\\\ \\vdots \\\\ a_j^T \\\\ \\vdots \\\\ a_n^T \\end{vmatrix} + \\begin{vmatrix} a_1^T \\\\ \\vdots \\\\ ka_j^T \\\\ \\vdots \\\\ a_j^T \\\\ \\vdots \\\\ a_n^T \\end{vmatrix} \\] The second matrix on the right side has determinant \\(0\\), and \\(\\det A\\) stays the same after replacing. Note that all row operations don’t change whether or not a determinant is 0, only change it by a non-zero factor or change its sign. 1.4.4 Cramer’s rule Given an \\(n \\times n\\) matrix \\(A\\) and \\(\\boldsymbol{b}\\) in \\(\\mathbb{\\mathbb{R^n}}\\), denote \\(A_i(\\boldsymbol{b})\\) as the matrix derived by \\(A\\) by replacing column \\(i\\) by vector \\(\\boldsymbol{b}\\): \\[ A_i(\\boldsymbol{b}) = [\\boldsymbol{a}_1 \\cdots \\underbrace{\\boldsymbol{b}}_{\\text{column} \\,i} \\cdots \\boldsymbol{a}_n] \\] Theorem 1.4 (Cramer’s rule) Let \\(A\\) be an invertible \\(n \\times n\\) matrix. For any \\(\\boldsymbol{b}\\) in \\(\\mathbb{R^n}\\), the unique solution \\(\\boldsymbol{x}\\) of \\(A\\boldsymbol{x} = \\boldsymbol{b}\\) has entries given by: \\[ x_i = \\frac{\\det A_i(\\boldsymbol{b})}{\\det A} \\] 1.5 Trace The trace of square matrix \\(A\\) is the sum of its diagonal entries \\(\\sum_{i = 1}^{n}A_{ii}\\). The trace has the following properties: \\(\\text{tr}(A + B) = \\text{tr}A + \\text{tr}B\\) \\(\\text{tr}(kA) = k\\text{tr}A\\), \\(k\\) is a scalar \\(\\text{tr}(A^T) = \\text{tr}(A)\\) For \\(A\\), \\(B\\) such that \\(AB\\) is square, \\(\\text{tr}(AB) = \\text{tr}(BA)\\) Trace of product of multiple matrices is invariant to cyclic permutations, \\(\\text{tr}(ABC) = \\text{tr}(BCA) = \\text{tr}(CAB)\\). Note that the reordering cannot be done arbitrarily, fro example \\(\\text{tr}(ABC) \\not= \\text{tr}(ACG)\\) in general. Trace is equal to the sum of its eigenvalues (repeated accordin gto multiplicity) \\(\\text{tr}(A) = \\sum_{i = 1}^n{\\lambda_i}\\) \\(\\text{tr}(\\boldsymbol{a}\\boldsymbol{a}^T) = \\boldsymbol{a}^T\\boldsymbol{a}\\), a is a column vector 1.6 Inverse of a matrix Note that the inverse of a matrix is only defined for square matrces, so is determinants in Section 1.4. In practice \\(A^{-1}\\) is seldom computed, because computing both \\(A^{-1}\\) and \\(A^{-1}\\boldsymbol{b}\\) to solve linear equations takes about 3 times as many arithmetic operations as solving \\(A\\boldsymbol{x} = \\boldsymbol{b}\\) by row reduction. Assume that \\(A\\) and \\(B\\) are both non-singular Theorem 1.5 If A and B are both invertible matrces, we have \\[ (A^{-1})^{-1} = A \\] \\[ (AB)^{-1} = B^{-1}A^{-1} \\] \\[ (A^T)^{-1} = (A^{-1})^T \\] In 1.2, we know an algorithm of finding inverse matrices by row reducions on the augmented matrix \\([A \\;\\; I]\\). However, Cramer’s rule 1.4 leads to a general formula of calculating \\(A^{-1}\\), if it exists. The \\(j\\)th column of \\(A^{-1}\\) is a vector \\(\\boldsymbol{x}\\) that satisfies: \\[ A\\boldsymbol{x} = \\boldsymbol{e}_j \\] By Cramer’s rule \\[ \\{(i,j) \\text{ entry of } A^{-1}\\} = x_i = \\frac{\\det A_i{(\\boldsymbol{e}_j)}}{\\det A} \\] A cofactor expansion 1.3 down column \\(i\\) of \\(A_i{(\\boldsymbol{e}_j)}\\) shows that: \\[ \\det A_i{(\\boldsymbol{e}_j)} = (-1)^{i + j}\\det A_{ji} = C_{ji} \\] where \\(C_{ji}\\) is a cofactor of \\(A\\). Note that the (\\(i\\), \\(j\\))th entry of \\(A^{-1}\\) is the cofactor \\(C_{ji}\\) divided by \\(\\det A\\) (the subscript is reversed). Thus \\[\\begin{equation} \\tag{1.1} A^{-1} = \\frac{1}{\\det A} \\begin{bmatrix} C_{11} &amp; C_{21} &amp; \\cdots &amp; C_{n1} \\\\ C_{12} &amp; C_{22} &amp; \\cdots &amp; C_{n2} \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ C_{1n} &amp; C_{2n} &amp; \\cdots &amp; C_{nn} \\\\ \\end{bmatrix} \\end{equation}\\] The right side of Eq (1.1) is called the adjugate of \\(A\\), often denoted by \\(\\text{adj}\\, A\\). Theorem 1.6 (An Inverse Formula) Let \\(A\\) be an invertible \\(n \\times n\\) matrix. Then \\[ A^{-1} = \\frac{1}{\\det A}\\text{adj}\\, A \\] 1.7 Matrix multiplication as linear transformation The equation \\(A_{m \\times n} \\, x _{ n \\times 1} = b_{m \\times 1}\\) can arise in a way that is not directly connected with linear combination of column vectors. That is, we think of the matrix \\(A\\) as an force that “acts” on a vector \\(x\\) in \\(\\mathbb{R^n}\\) by multiplication to produce a new vector called \\(b\\) in \\(\\mathbb{\\mathbb{R^m}}\\). A transformation \\(T\\) from \\(\\mathbb{R^n}\\) to \\(\\mathbb{R^m}\\) is a rule that assigns each vector in \\(\\mathbb{R^n}\\) a vector \\(T(x)\\) in \\(\\mathbb{R^m}\\), which is called the image of (under the action of \\(T\\)). It can be show that such transformations induced by multiplying a matrix is a type of linear transformation, because it satisfies all required properties to be linear: \\[ \\begin{aligned} \\text{vector addition} \\quad A(\\boldsymbol{u} + \\boldsymbol{v}) &amp;= A\\boldsymbol{u} + A\\boldsymbol{v} \\\\ \\text{scalar multiplication} \\quad A(c\\boldsymbol{u}) &amp;= cA\\boldsymbol{u} \\end{aligned} \\] Theorem 1.7 (left multiplication as linear transformation) There is a one to one relationship between a linear transformation and a matrix. Let \\(T: \\mathbb{R^n} \\rightarrow \\mathbb{R^m}\\) be a linear transformation. Then there exists a unique matrix \\(A\\) such that: \\[ T(x) = Ax \\quad \\text{for all} \\; x \\; \\text{in} \\; \\mathbb{R^n} \\] In fact, \\(A\\) is a \\(m \\times n\\) matrix whose \\(j\\)th column is the vector \\(T(\\boldsymbol{e_j})\\), where \\(\\boldsymbol{e_j}\\) is the \\(j\\)th basis of \\(\\mathbb{R^n}\\) Proof: \\[ \\boldsymbol{x} = x_1\\boldsymbol{e_1} + \\dots + x_n{\\boldsymbol{e_n}} \\] And because \\(T(\\boldsymbol{x})\\) is a linear transformation: \\[ \\begin{split} T(\\boldsymbol{x}) &amp;= x_1T(\\boldsymbol{e_1}) + \\dots + x_nT(\\boldsymbol{e_n}) \\\\ &amp;= [T(\\boldsymbol{e_1}) \\, \\cdots \\, T(\\boldsymbol{e_n})]\\boldsymbol{x} \\\\ &amp;= (A\\boldsymbol{x})_{m \\times 1} \\end{split} \\] In other words, the transformation is specified once we know what all basis in \\(\\mathbb{R^n}\\) become in \\(\\mathbb{R^m}\\). The matrix \\(A\\) is called the standard matrix for the linear transformation \\(T\\). Definition 1.2 (A mapping is onto \\(\\mathbb{R^m}\\)) A mapping \\(T: \\mathbb{R^n} \\rightarrow \\mathbb{R^m}\\) is said to be onto if each \\(\\boldsymbol{b}\\) in \\(\\mathbb{R^m}\\) is the image of at least one \\(\\boldsymbol{x}\\) in Equivalently, \\(T\\) is onto \\(\\mathbb{R^m}\\) means that there exists at least one solution of \\(T(\\boldsymbol{x}) = \\boldsymbol{b}\\) Definition 1.3 (one-to-one mapping) A mapping T: \\(\\mathbb{R^n} \\rightarrow \\mathbb{R^m}\\) is said to be one-to-one if each \\(\\boldsymbol{b}\\) in \\(\\mathbb{R^m}\\) is the image of at most one \\(\\boldsymbol{x}\\) in \\(\\mathbb{R^n}\\) Equivalently, \\(T\\) is one-to-one if, for each \\(\\boldsymbol{b}\\) in \\(\\mathbb{R^m}\\), the equation \\(T(\\boldsymbol{x}) = \\boldsymbol{b}\\) has either a unique solution or none at all. 1.8 Statistics and proabability 1.8.1 Sample statistics "],
["vector-spaces.html", "Chapter 2 Vector spaces 2.1 Four subspaces", " Chapter 2 Vector spaces 2.1 Four subspaces "],
["orthogonality.html", "Chapter 3 Orthogonality 3.1 Metric spaces, normed spaces, inner product spaces 3.2 Orthogonal decomposition 3.3 Gram-Schmidt process 3.4 Orthonormal sets and orthogonal matrices 3.5 Lesat squares problems", " Chapter 3 Orthogonality 3.1 Metric spaces, normed spaces, inner product spaces Metrics generalize the notion of distance from Euclidean space. A metric space is a set together with a metric on the set (metric spaces don’t have to be vector spaces). The metric is a function that defines a concept of distance \\(\\in \\mathbb{R}\\) between any two members of the set. A metric must satisfies the following properties: \\(d(x, y) \\ge 0\\), with equality if and only if \\(x = y\\). Distances are non-negative, and the only point at zero distance from \\(x\\) is \\(x\\) itself \\(d(x, y) = d(y, x)\\). The distance is a symmetric function. \\(d(x, z) \\le d(x, y) + d(y, z)\\). Distance satisfies triangular inequality. Norms generalize the notion of length from Euclidean space. A norm on a real vector space \\(X\\) is a function: \\(||\\cdot||: V \\rightarrow \\mathbb{R}\\) that satisfies: \\(||x|| \\ge 0\\) for all \\(x \\in X\\), with equality if and only if \\(x = \\boldsymbol{0}\\) (nonnegative) \\(||\\lambda x|| = \\lambda ||x||\\), for all \\(x \\in X\\) and \\(\\lambda \\in \\mathbb{R}\\) (homogeneous) \\(||x + y|| \\le ||x|| + ||y||\\) (triangular inequality) A normed space is a metric space with the metric \\[ d(\\boldsymbol{x}, \\boldsymbol{y}) = ||\\boldsymbol{x} - \\boldsymbol{y}|| \\] So a normed space is a special case of metric spaces, a metric spcae may not necessarily has a norm associated with it. One can verify that \\(d(x, y) = ||x - y||\\) satisfies all properties of a metric. The most common function for norms on \\(\\mathbb{R}^n\\) are listed below, with \\(x = [x_1, x_2, ..., x_n]\\). \\[ \\begin{aligned} \\text{1-norm}: ||\\boldsymbol{x}||_1 &amp;= \\sum_{i=1}^{n}{|x_i|} \\\\ \\text{2-norm}: ||\\boldsymbol{x}||_2 &amp;= \\sqrt{\\sum_{i=1}^{n}{x_i}^2} \\\\ \\text{p-norm}: ||\\boldsymbol{x}||_p &amp;= (\\sum_{i=1}^{n}{|x_i|}^p)^{\\frac{1}{p}} \\quad (p \\ge 1) \\\\ \\text{maximum norm}: ||\\boldsymbol{x}||_{\\infty} &amp;= \\max\\{|x_1|, |x_2|, ..., |x_n|\\} \\end{aligned} \\] 1-norm is also called the Manhattan norm. 2-norm is the commonly used Euclidean distance, and the subscript \\(2\\) can be left out in most cases. p-norm is a generalization of 1-norm and 2-norm, requiring \\(p &gt; 1\\). When \\(p\\) turns infinity, \\(||x||_{\\infty}\\) is called the maximum norm. An inner product on a real vector space \\(X\\) is a function \\(\\langle \\cdot, \\cdot\\rangle: X \\times X \\rightarrow \\mathbb{R}\\) satisfying \\(\\langle x, x \\rangle \\ge 0\\), with equality if and only if \\(x = \\boldsymbol{0}\\) \\(\\langle x, y \\rangle = \\langle y, x \\rangle\\) \\(\\langle x + y, z\\rangle = \\langle x, z\\rangle + \\langle y, z\\rangle\\) and \\(\\langle \\lambda x, y\\rangle = \\lambda \\langle x, y \\rangle\\) A vector sapce equipped with such inner product is called a inner product space. Note that all inner product spaces are normed spaces, because a inner product induce a norm on a vector space: \\[ \\langle \\boldsymbol{x}, \\boldsymbol{\\boldsymbol{x}} \\rangle = ||\\boldsymbol{x}||^2 \\] The standard inner product defined on \\(\\mathbb{R}^{n}\\) is given by \\[ \\langle \\boldsymbol{x}, \\boldsymbol{y} \\rangle = \\sum_{i=1}^{n}{x_iy_i} = \\boldsymbol{x}^T\\boldsymbol{y} \\] The abstract spaces—metric spaces, normed spaces, and inner product spaces—are all examples of what are more generally called “topological spaces.” These spaces have been given in order of increasing structure. That is, every inner product space is a normed space, and in turn, every normed space is a metric space. 3.2 Orthogonal decomposition 3.2.1 Orthogonal complements if vector \\(\\boldsymbol{v}\\) is orthogonal to every vector in a subspace \\(W\\) of \\(\\mathbb{R^n}\\), then \\(\\boldsymbol{v}\\) is said to be orthogonal to \\(W\\). The subspace that contains the set of vectors that are orthogonal to \\(W\\) is called the orthogonal complement, denoted by \\(W^{\\perp}\\). \\[ W^{\\perp} = \\{\\boldsymbol{v} \\in W^{\\perp} | \\;\\boldsymbol{v} \\perp \\boldsymbol{x} \\; \\text{for all} \\; \\boldsymbol{x} \\in W\\} \\] This corresponds to discussions in Section 2.1, where \\[ (\\text{row} A)^{\\perp} = \\text{Nul} A \\\\ (\\text{col} A)^{\\perp} = \\text{Nul} A^T \\] Theorem 3.1 If \\(W\\) is a subspace of \\(\\mathbb{R}^n\\), \\(W^{\\perp}\\) is also a subspace of \\(\\mathbb{R}^n\\). It’s easy to verify that \\(W^{\\perp}\\) is closed under scalar multiplication, and under vector addition, and that any vector in \\(W\\) has \\(n\\) components. So that \\(W^{\\perp}\\) is a subspace of \\(\\mathbb{R}^n\\) 3.2.2 Orthogonal sets and orthogonal basis An orthogonal set is a set of vectors \\(\\{\\boldsymbol{u_1}, \\dots, \\boldsymbol{u_p}\\}\\) in \\(\\mathbb{R^n}\\), in which each pair of distinct vectors is orthogonal: \\(\\boldsymbol{u_i}^{T} \\boldsymbol{u_j} = 0 \\quad i\\not = j\\). Note that the set do not necessarily span the whole \\(\\mathbb{R^n}\\), but a subspace \\(W\\). Since vectors in orthogonal sets is mutually perpendicular, they must also be linearly independent and could form a basis for a subspace \\(W\\). In such case, they are called orthogonal basis. There is a particular advantage in using orthogonal basis rather than other basis, because we can find a easy representation of any vector in \\(W\\). Theorem 3.2 For each \\(\\boldsymbol{y}\\) in \\(W\\), there exists a linear combination \\[ y = c_1\\boldsymbol{u_1} + \\cdots + c_p\\boldsymbol{u_p} \\] and \\[ c_i = \\frac{\\boldsymbol{y} \\cdot \\boldsymbol{u_i}}{\\boldsymbol{u_i} \\cdot \\boldsymbol{u_i}} \\quad i = 1, \\cdots, p \\] where \\(\\{\\boldsymbol{u_1}, \\dots, \\boldsymbol{u_p}\\}\\) is an orthogonal basis. Proof \\[ \\begin{split} \\boldsymbol{u_1} \\cdot \\boldsymbol{y} &amp;= \\boldsymbol{u_1} \\cdot (c_1\\boldsymbol{u_1} + \\cdots + c_p\\boldsymbol{u_p}) \\\\ &amp;= c_1 \\boldsymbol{u_1} \\cdot \\boldsymbol{u_1} \\end{split} \\] So: \\[ c_1 = \\frac{\\boldsymbol{u_1} \\cdot \\boldsymbol{y}}{\\boldsymbol{u_1} \\cdot \\boldsymbol{u_1}} \\] Derivations for other \\(c_i\\) is similar. 3.2.3 Orthogonal decomposition Orthogonal decomposition split \\(\\boldsymbol{y}\\) in \\(\\mathbb{R^n}\\) into two vectors, one in \\(W\\) and one in its orthogonal compliment \\(W^{\\perp}\\). Theorem 1.5 Let \\(\\mathbb{R}^n\\) be a inner product space and \\(W\\) and subspace of \\(\\mathbb{R}^n\\). Then every \\(\\boldsymbol{v}\\) in \\(W\\) can be written uniquely in the form \\[ \\boldsymbol{v} = \\boldsymbol{v}_w + \\boldsymbol{v}_{\\perp} \\] where \\(\\boldsymbol{v}_w \\in W\\) and \\(\\boldsymbol{v}_{\\perp} \\in W^{\\perp}\\) PROOF Let \\(\\boldsymbol{u}_1, ..., \\boldsymbol{u}_m\\) be a orthonormal basis for \\(W\\), there exists linear combination according to Section 3.2.2 \\[ \\boldsymbol{v}_w = (\\boldsymbol{v} \\cdot \\boldsymbol{u}_1)\\boldsymbol{u}_1 + \\cdots + (\\boldsymbol{v} \\cdot \\boldsymbol{u}_m)\\boldsymbol{u}_m \\] and \\[ \\boldsymbol{v}_{\\perp} = \\boldsymbol{v} - \\boldsymbol{v}_w \\] It is clear that \\(\\boldsymbol{v}_W \\in W\\). And we can also show that \\(\\boldsymbol{v}_{\\perp}\\) is perpendicular to \\(W\\) \\[ \\begin{split} \\boldsymbol{v}_{\\perp} \\cdot \\boldsymbol{u}_i &amp;= [\\boldsymbol{v}- (\\boldsymbol{v} \\cdot \\boldsymbol{u}_1)\\boldsymbol{u}_1 - \\cdots - (\\boldsymbol{v} \\cdot \\boldsymbol{u}_m)\\boldsymbol{u}_m] \\cdot \\boldsymbol{u}_i \\\\ &amp;= (\\boldsymbol{v} \\cdot \\boldsymbol{u}_1) - [(\\boldsymbol{v} \\cdot \\boldsymbol{u}_i)\\boldsymbol{u}_i \\cdot \\boldsymbol{u}_i] \\\\ &amp;= 0 \\end{split} \\] which implies \\(\\boldsymbol{v}_{\\perp} \\in W^{\\perp}\\). To prove that \\(\\boldsymbol{v}_w\\) and \\(\\boldsymbol{v}_{\\perp}\\) are unique (does not depend on the choice of basis), let \\(\\boldsymbol{u}_1&#39;, ..., \\boldsymbol{u}_m&#39;\\) be another orthonormal basis for \\(W\\), and define \\(\\boldsymbol{v}_w&#39;\\) and \\(\\boldsymbol{v}_{\\perp}&#39;\\) similarly we want to get \\(\\boldsymbol{v}_w&#39; = \\boldsymbol{v}_w\\) and \\(\\boldsymbol{v}_{\\perp}&#39; = \\boldsymbol{v}_{\\perp}\\). By definition \\[ \\boldsymbol{v}_w + \\boldsymbol{v}_{\\perp} = \\boldsymbol{v} = \\boldsymbol{v}_w&#39; + \\boldsymbol{v}_{\\perp}&#39; \\] so \\[ \\underbrace{\\boldsymbol{v}_w - \\boldsymbol{v}_w&#39;}_{\\in W} = \\underbrace{\\boldsymbol{v}_{\\perp}&#39; - \\boldsymbol{v}_{\\perp}}_{\\in W^{\\perp}} \\] From the orthogonality of these subspaces, we have \\[ 0 = (\\boldsymbol{v}_w - \\boldsymbol{v}_w&#39;) \\cdot (\\boldsymbol{v}_{\\perp}&#39; - \\boldsymbol{v}_{\\perp}) = (\\boldsymbol{v}_w - \\boldsymbol{v}_w&#39;) \\cdot (\\boldsymbol{v}_w - \\boldsymbol{v}_w&#39;) = ||\\boldsymbol{v}_w - \\boldsymbol{v}_w&#39;||^2 \\] Similarly we have \\(||\\boldsymbol{v}_{\\perp}&#39; - \\boldsymbol{v}_{\\perp}||^2 = 0\\). The existence and uniqueness of the decomposition above mean that \\[ \\mathbb{R}^n = W \\oplus W^{\\perp} \\] whenever \\(W\\) is a subspace. 3.2.4 Best approximation Theorem 3.3 (The Best Approximation) Given \\(\\boldsymbol{y}\\) be any vector in \\(\\mathbb{R^n}\\), with its subspace \\(W\\), let \\(\\hat{\\boldsymbol{y}}\\) be the orthogonal projection of \\(\\boldsymbol{y}\\) onto \\(W\\). Then \\(\\hat{\\boldsymbol{y}}\\) is the closest point in \\(W\\) to \\(\\boldsymbol{y}\\) in the sense that \\[ ||\\boldsymbol{y} - \\hat{\\boldsymbol{y}}|| \\le ||\\boldsymbol{y} - \\boldsymbol{v}|| \\] PROOF Take \\(\\boldsymbol{v}\\) distinct from \\(\\hat{\\boldsymbol{y}}\\) in \\(W\\), we know that \\(\\boldsymbol{y} - \\hat{\\boldsymbol{y}}\\) is perpendicular to \\(\\boldsymbol{v}\\). According to Pythoagorean theorem, we have Figure 3.1: figure from page p352, ch6 (Lay 2006) \\[ ||\\boldsymbol{y}- \\boldsymbol{v}||^2 = ||\\boldsymbol{\\hat{y}} - \\boldsymbol{v}||^2 + ||\\boldsymbol{y} -\\boldsymbol{\\hat{y}}||^2 \\] When \\(\\boldsymbol{v}\\) is distinct from \\(\\boldsymbol{\\hat{y}}\\), \\(||\\boldsymbol{\\hat{y}} - \\boldsymbol{v}||^2\\) is non-negative, so the error term of choosing \\(\\boldsymbol{v}\\) is always larger than that of the orthogonal projection \\(\\boldsymbol{\\hat{y}}\\). 3.3 Gram-Schmidt process Then Gram-Schmidt process is a simple algorithm that transforms a set of linearly independent vectors into orthogonal or orthonormal basis for a subspace. In its essence, it is a sequential projection of \\(\\boldsymbol{x}_{i}\\) onto the space spanned by the previously created orthogonal set \\(\\{\\boldsymbol{v}_{1}, ..., \\boldsymbol{v}_{i-1}\\}\\), and take the term in the orthogonal compliment to be \\(\\boldsymbol{v}_{i}\\). Theorem 3.4 (the Gram-Schmidt process) Given a basis \\(\\{\\boldsymbol{x}_1, ..., \\boldsymbol{x}_p\\}\\) for a nonzero subspace \\(W\\) of \\(\\mathbb{R}^n\\), define \\[ \\begin{aligned} \\boldsymbol{v}_1 &amp;= \\boldsymbol{x}_1 \\\\ \\boldsymbol{v}_2 &amp;= \\boldsymbol{x}_2 - \\frac{\\boldsymbol{x}_2 \\cdot \\boldsymbol{v}_1}{\\boldsymbol{v}_1 \\cdot \\boldsymbol{v}_1}\\boldsymbol{v}_1 \\\\ \\boldsymbol{v}_3 &amp;= \\boldsymbol{x}_3 - \\frac{\\boldsymbol{x}_3 \\cdot \\boldsymbol{v}_1}{\\boldsymbol{v}_1 \\cdot \\boldsymbol{v}_1}\\boldsymbol{v}_1 - \\frac{\\boldsymbol{x}_3 \\cdot \\boldsymbol{v}_2}{\\boldsymbol{v}_2 \\cdot \\boldsymbol{v}_2}\\boldsymbol{v}_2 \\\\ &amp; \\vdots \\\\ \\boldsymbol{v}_p &amp;= \\boldsymbol{x}_p - \\frac{\\boldsymbol{x}_p \\cdot \\boldsymbol{v}_1}{\\boldsymbol{v}_1 \\cdot \\boldsymbol{v}_1}\\boldsymbol{v}_1 - \\frac{\\boldsymbol{x}_p \\cdot \\boldsymbol{v}_2}{\\boldsymbol{v}_2 \\cdot \\boldsymbol{v}_2}\\boldsymbol{v}_2 - \\cdots - \\frac{\\boldsymbol{x}_p \\cdot \\boldsymbol{v}_{p-1}}{\\boldsymbol{v}_{p-1} \\cdot \\boldsymbol{v}_{p-1}}\\boldsymbol{v}_{p-1} \\end{aligned} \\] Then \\(\\{\\boldsymbol{v}_1, ..., \\boldsymbol{v}_p\\}\\) is an orthogonal basis for \\(W\\). In addition \\[ \\text{Span}\\{\\boldsymbol{v}_1, ..., \\boldsymbol{v}_p\\} = \\text{Span}\\{\\boldsymbol{x}_1, ..., \\boldsymbol{x}_p\\} \\] To make \\(\\{\\boldsymbol{v}_1, ..., \\boldsymbol{v}_p\\}\\) an orthonormal basis, there is simply one more step of normalization \\[ \\{\\boldsymbol{q}_i = \\frac{\\boldsymbol{v}_i}{||\\boldsymbol{v}_i||}, \\;i = 1, ... p\\} \\] 3.3.1 QR factorizaiton For \\(A \\in \\mathbb{R}^{m \\times n}\\) with linearly independent columns \\(\\boldsymbol{x}_1, ..., \\boldsymbol{x}_n\\), apply the Gram-Schmidt process to \\(\\boldsymbol{x}_1, ..., \\boldsymbol{x}_n\\) amounts to factorizing \\(A\\). Theorem 3.5 (QR factorization) if \\(A\\) is an \\(m \\times n\\) matrix with full column rank, then \\(A\\) can be factored as \\(A = QR\\), where \\(Q\\) is an \\(m \\times n\\) matrix whose columns form an orthonormal basis of \\(\\text{Col}\\;A\\) and \\(R\\) is an \\(n \\times n\\) upper triangular invertible matrix with positive entries on its diagonal. PROOF Because \\(A_{m \\times n}\\) is full column rank, we can transform its column vector \\(\\{\\boldsymbol{x}_{1}, ..., \\boldsymbol{x}_{n}\\}\\) into a new set of orthonormal basis \\(\\{\\boldsymbol{q}_{1}, ..., \\boldsymbol{q}_{n}\\}\\) with Gram-Schmidt process. Let \\[ Q = [\\boldsymbol{q}_{1} \\;\\; \\cdots \\;\\; \\boldsymbol{q}_{n}] \\] For \\(\\boldsymbol{x}_i, \\; i = {1, ..., n}\\) in Span\\(\\{\\boldsymbol{x}_1, ... \\boldsymbol{x}_k\\}\\), there exists a set of constant \\(r_{1k}, ..., r_{kk}\\) such that1 \\[ \\boldsymbol{x}_k = r_{1k}\\boldsymbol{q}_{1} + \\cdots + r_{kk}\\boldsymbol{q}_{k} + 0 \\cdot\\boldsymbol{q}_{k+1} + \\cdots + 0 \\cdot \\boldsymbol{q}_{n} \\] So \\[ A = [\\boldsymbol{x}_{1} \\;\\; \\boldsymbol{x}_{2} \\;\\; \\cdots \\;\\; \\boldsymbol{x}_{n}] = [\\boldsymbol{q}_{1} \\;\\; \\boldsymbol{q}_{2} \\;\\; \\cdots \\;\\; \\boldsymbol{q}_{n}] \\begin{bmatrix} r_{11} &amp; r_{12} &amp; \\cdots &amp; r_{1n} \\\\ 0 &amp; r_{22} &amp; \\cdots &amp; r_{2n} \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\ 0 &amp; 0 &amp; \\cdots &amp; r_{nn} \\end{bmatrix} = QR \\] We could assume that \\(r_{kk} \\ge 0\\). (if \\(r_{kk} &lt; 0\\), multiply both \\(r_{kk}\\) and \\(\\boldsymbol{u}_k\\) by \\(-1\\)) 3.4 Orthonormal sets and orthogonal matrices An orthogonal set whose components are all unit vectors is said to be orthonormal sets. 3.4.1 Orthogonal matrices An orthogonal matrix is a square matrix \\(Q\\) whose inverse is its transpose: \\[ \\tag{3.1} QQ^T = Q^TQ = I \\] Another way of defining it is that an orthogonal matrix has both orthonormal columns and orthonormal rows. Orthogonal matrices have a nice property that they preserve inner products: \\[ (Q\\boldsymbol{x})^T(Q\\boldsymbol{y}) = \\boldsymbol{x}^TQ^TQ\\boldsymbol{y} = \\boldsymbol{x}^TI\\boldsymbol{y} = \\boldsymbol{x}^T\\boldsymbol{y} \\] A direct result is that \\(Q\\) preserves L2 norms \\[ ||Q\\boldsymbol{x}||_2 = \\sqrt{(Q\\boldsymbol{x})^T(Q\\boldsymbol{x})} = \\sqrt{\\boldsymbol{x}^T\\boldsymbol{x}} = ||\\boldsymbol{x}||_2 \\] Therefore multiplication by an orthogonal matrix can be considered as a transformation that preserves length, but may rotate or reflect the vector about the origin. Note that \\(Q\\) may not necessarily be a square matrix to satisfy \\(Q^TQ = I\\). For exmaple \\(Q \\in \\mathbb{R}^{m \\times n}, n &lt; m\\), but its columns and rows can still be orthonormal, then \\(QQ^T = I\\). But in most cases the term orthogonal implies a square matrix \\(Q\\). 3.5 Lesat squares problems Definition 3.1 the normal equation \\[ A^TA\\boldsymbol{x} = A^T\\boldsymbol{b} \\] \\[ \\begin{aligned} A^T(\\boldsymbol{b} - A\\hat{\\boldsymbol{x}}) &amp;= \\boldsymbol{0} \\end{aligned} \\\\ A^T\\boldsymbol{b} - A^TA\\hat{\\boldsymbol{x}} = 0 \\\\ \\hat{\\boldsymbol{x}} = (A^TA)^{-1}A^T\\boldsymbol{b} \\] because Span\\(\\{\\boldsymbol{x}_1, ... \\boldsymbol{x}_k\\}\\) is the same as Span\\(\\{\\boldsymbol{u}_1, ... \\boldsymbol{u}_k\\}\\)↩︎ "],
["eigenthings-and-quadratic-forms.html", "Chapter 4 Eigenthings and quadratic forms 4.1 Eigenvectors and eigenvalues 4.2 Diagnolization and similar matrices 4.3 Symmetric matrices 4.4 Quadratic forms 4.5 Rayleigh quotients 4.6 SVD", " Chapter 4 Eigenthings and quadratic forms 4.1 Eigenvectors and eigenvalues Definition 4.1 (Eigenvectors and eigenvalues) An eigenvector of an \\(n \\times n\\) matrix \\(A\\) is a nonzero vector \\(\\boldsymbol{x}\\) such that \\(A\\boldsymbol{x} = \\lambda\\boldsymbol{x}\\). \\(\\lambda\\) is the eigenvalue of \\(A\\) if there is a nontrivial solution \\(\\boldsymbol{x}\\) of \\(A\\boldsymbol{x} = \\lambda \\boldsymbol{x}\\); such an \\(\\boldsymbol{x}\\) is called an eigenvector corresponding to \\(\\lambda\\) To find eigenvalues and corresponding eigenvectors of \\(A\\), we look at the equation \\[ (A - \\lambda I)\\boldsymbol{x}= 0 \\] Since eigenvector \\(\\boldsymbol{x}\\) must be nonzero, \\((A - \\lambda I)\\) is a singular matrix \\[\\begin{equation} \\tag{4.1} \\det (A - \\lambda I) = 0 \\end{equation}\\] Eq (4.1) is called the characteristic equation of matrix \\(A\\). This is a scalar equation containing information about eigenvalues and eigenvectors of a square matrix \\(A\\). Theorem 1.1 Eigenvalues of a trangular matrix are its diagonal entries. PROOF Consider the \\(3 \\times 3\\) case. If \\(A\\) is upper triangular, then \\(A - \\lambda I\\) has the form \\[ \\begin{bmatrix} a_{11} - \\lambda &amp; a_{12} &amp; a_{13} \\\\ 0 &amp; a_{22} - \\lambda &amp; a_{23} \\\\ 0 &amp; 0 &amp; a_{33} - \\lambda \\end{bmatrix} \\] So the roots of characteristic are \\(a_{11}, a_{22}, a_{33}\\) respectively. There are some useful results about how eigenvalues change after various manipulations. For any \\(k, b \\in \\mathbb{R}\\), \\(\\boldsymbol{x}\\) is an eigenvector of \\(kA + bI\\) with eigenvalue \\(k\\lambda + b\\) PROOF \\[ (kA + bI)\\boldsymbol{x} = kA\\boldsymbol{x} + bI\\boldsymbol{x} = k \\lambda\\boldsymbol{x} + b\\boldsymbol{x} = (k\\lambda + b)\\boldsymbol{x} \\] 2, If \\(A\\) is invertible, then \\(\\boldsymbol{x}\\) is an eigenvector of \\(A^{-1}\\) with eigenvalue \\(1/\\lambda\\) PROOF \\[ \\boldsymbol{x} = A^{-1}A\\boldsymbol{x} = A^{-1}\\lambda \\boldsymbol{x} = \\lambda A^{-1}\\boldsymbol{x} \\] 3. \\(A^{k}\\boldsymbol{x} = \\lambda^{k}\\boldsymbol{x}\\) The next theorem is important in terms of diagonalization and spectral decomposition Theorem 4.1 For distinct eigenvalues \\(\\lambda_1, \\cdots, \\lambda_r\\) of an \\(n \\times n\\) matrix A, their corresponding eigenvectors \\(\\boldsymbol{v_1}, ..., \\boldsymbol{v_r}\\) are linearly independent. PROOF Suppose for r distinct eigenvalue \\(\\lambda_1, \\cdots, \\lambda_r\\), the set \\(\\{\\boldsymbol{v_1}, ..., \\boldsymbol{v_r}\\}\\) is not linearly independent, and \\(p\\) is the least index such that \\(\\boldsymbol{v}_{p+1}\\) is a linear combination of the preceding vectors. Then there exists scalars \\(c_1, \\cdots, c_p\\) such that \\[ c_1\\boldsymbol{v}_1 + \\cdots + c_p\\boldsymbol{v}_p = \\boldsymbol{v}_{p+1} \\tag{1} \\] Left multiply by \\(A\\), and note we have \\(A\\boldsymbol{v}_i = \\lambda_i\\boldsymbol{v}_i\\) for \\(i = 1, ..., n\\) \\[ c_1\\lambda_1\\boldsymbol{v}_1 + \\cdots + c_p\\lambda_p\\boldsymbol{v}_p = \\lambda_{p+1}\\boldsymbol{v}_{p+1} \\tag{2} \\] Multiplying both sides of (2) by \\(\\lambda_{p+1}\\) and subtracting (2) from the result \\[ c_1(\\lambda_1 - \\lambda_{p+1})\\boldsymbol{v}_1 +\\cdots + c_p(\\lambda_p - \\lambda_{p+1})\\boldsymbol{v}_p = 0 \\tag{3} \\] Since \\(\\boldsymbol{v}_1, ..., \\boldsymbol{v}_p\\) are linearly independent, weights in (3) must be all zero. Since \\(\\lambda_1, \\cdots, \\lambda_p\\) are distinct, hence \\(c_i = 0, \\, i = 1, ..., p\\). But then (5) says that eigenvector \\(\\boldsymbol{v}_{p+1}\\) is zero vector, which contradicts definition 4.1 4.2 Diagnolization and similar matrices Definition 4.2 (Diagonalization thoerem) An \\(n \\ times n\\) matrix \\(A\\) is diagnolizable if and only if A has \\(n\\) independent linearly independent eigenvectors. In such case, in \\(A = P \\Lambda P^{-1}\\), the diagonal entries of \\(D\\) are eigenvalues that correpond, respectively, to the eigenvectors of in \\(P\\) In other words, \\(A\\) is diagnolizable if and only if there are enough eigenvectors in form a basis of \\(R^n\\), called an eigenvector basis of \\(R^n\\) Proof \\[ \\begin{split} AP &amp;= A[\\boldsymbol{v}_1 \\cdots \\boldsymbol{v}_n] \\\\ &amp;= [A\\boldsymbol{v}_1 \\cdots A\\boldsymbol{v}_n] \\\\ &amp;= [\\lambda_1\\boldsymbol{v}_1 \\cdots \\lambda_n\\boldsymbol{v}_n] \\end{split} \\] while on the other side of the equation: \\[ \\begin{aligned} DP &amp;= [\\boldsymbol{v}_1 \\cdots \\boldsymbol{v}_n] \\begin{bmatrix} \\lambda_1 &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; \\lambda_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\lambda_n \\end{bmatrix} \\\\ &amp;= [\\lambda_1\\boldsymbol{v}_1 \\cdots \\lambda_n\\boldsymbol{v}_n] \\end{aligned} \\] So that \\[ \\begin{aligned} AP &amp;= PD \\\\ A &amp;= P \\Lambda P^{-1} \\end{aligned} \\] Because \\(P\\) contains \\(n\\) independent columns so it’s invertible. According to theorem 4.1, an \\(n \\times n\\) matrix with \\(n\\) distinct eigenvalues is diagonalizable. This is a sufficient condition. For matrices whose eigenvalues are not distinct, there is still a change that it is diagonalizable. For any matrix \\(A_{n\\times n}\\), as long as the sum of the dimensions of the eigenspaces equals \\(n\\) then \\(P\\) is invertible. This could happen in the following two scenarios The characteristic polynomial factors completely into linear factors. This is the case when \\(A\\) has n distinct eigenvalues. The dimension of the eigenspace for each \\(\\lambda_k\\) equals the multiplicity of \\(\\lambda_k\\). Thus \\(A\\) with repeated eigenvalues can still be diagonalizable. 4.2.1 Similarity If \\(A\\) and \\(B\\) are both \\(n \\times n\\) matrices, then \\(A\\) is similar to \\(N\\) if there is an invertible matrix \\(P\\) such that \\(P^{-1}AP = B\\), or equivalently if we write \\(Q\\) for \\(P^{-1}\\), \\(Q^{-1}BQ = A\\). Changing \\(A\\) into \\(P^{-1}AP\\) is called a similarity transformation. Theorem 3.1 If \\(A\\) and \\(B\\) are similar, they have the same eigenvalues. PROOF If \\(B = P^{-1}AP\\), then \\[ B - \\lambda I = P^{-1}AP - \\lambda P^{-1}P = P^{-1}(AP - \\lambda P) = P^{-1}(A - \\lambda I) P \\] so that \\[ \\det (B - \\lambda I ) = \\det(P) \\cdot \\det(A - \\lambda I ) \\cdot \\det(P^{-1}) \\] since \\(\\det(P) \\cdot \\det(P^{-1}) = \\det (I) = 1\\), we have \\[ \\det (B - \\lambda I) = \\det(A - \\lambda I) \\] As a result of their identical characteristic polynomial, \\(B\\) and \\(A\\) have the same eigenvalues. We can also show that eigenvector of \\(B\\) is \\(P\\boldsymbol{v}\\): \\[ \\begin{aligned} A\\boldsymbol{v} &amp;= \\lambda\\boldsymbol{v} \\\\ (P^{-1}BP)\\boldsymbol{v} &amp;= \\lambda\\boldsymbol{v} \\\\ P(P^{-1}BP)\\boldsymbol{v} &amp;= \\lambda P\\boldsymbol{v} \\\\ B(P\\boldsymbol{v}) = \\lambda P \\boldsymbol{v} \\end{aligned} \\] The similarity theorem leads to a interesting proposition. Proposition 4.1 For \\(A, B \\in \\mathbb{R}^{n \\times n}\\), \\(AB\\) and \\(BA\\) are similar matrices and therefore share the same set of eigenvalues. To prove this, we need to show that there exists a invertible matrix \\(A\\) such that \\(P^{-1}(AB)P = BA\\). Take \\(P = A\\) and the equation holds. It is easy to show that similarity is transitive: if \\(A\\) is similar to \\(B\\), \\(B\\) is similar to \\(C\\), then \\(A\\) is similar to \\(C\\). So similarity means a family of matrices with the same set of eigenvalues, the most special and simplest of which is the diagonal matrix (if this is an diagonalizable family). Some computer algorithms calculate eigenvalues of \\(A\\) in this manner: with a sequential choices of \\(P\\), the off-diagonal elements of \\(A\\) become smaller and smaller until \\(A\\) becomes a triangular matrix or diagonal matrix, whose eigenvalues are simply diagonal entries and is the same as \\(A\\). It is obvious that a diagonalizable matrix \\(A\\) is similar to diagonal matrix \\(D\\), whose diagonal entries are \\(A\\)’s eigenvalues \\(\\lambda_i\\), and \\(P = [\\boldsymbol{v}_1 \\;\\; \\cdots \\;\\; \\boldsymbol{v}_n]^{-1}\\) where \\(\\boldsymbol{v}_i, \\;i = 1,..., n\\) are eigenvectors corresponding to \\(\\lambda_i\\). But square matrix \\(A\\) can still be similar to matrices other than \\(D\\) with other choices of \\(P\\), and non-diagonal matrices can also have similar matrices of their own. In fact, every square matrix is similar to a matrix in Jordan matrix 4.2.2. Similarity is only a sufficient condition for identical eigenvalues. The matrices \\[ \\begin{bmatrix} 2 &amp; 1 \\\\ 0 &amp; 2 \\end{bmatrix} \\;\\text{and}\\; \\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 2 \\end{bmatrix} \\] are not similar even though they have the same eigenvalues. 4.2.2 Jordan matrix For non-diagonalizable matrices \\(A_{n \\times n}\\), the goal is to with similar transformation \\(P^{-1}AP\\) construct a matrix that is as nearest to a diagonal matrix as possible. Definition 4.3 The \\(n \\times n\\) matrix \\(J_{\\lambda, n}\\) with \\(\\lambda\\)s on the diagonal, \\(1\\)s on the superdiagonal and \\(0\\)s elsewhere is called a Jordan matrix. A Jordan matrix in Jordan normal form is a block matrix that has Jordan blocks down its block diagonal and is zero elsewhere An example of Jordan matrix, the appearance of \\(\\lambda_i\\) on the diagonal is equal to its multiplicity as \\(A\\)’s eigenvalue. \\[ \\begin{bmatrix} \\lambda_1 &amp; 1 &amp; \\\\ &amp; \\lambda_1 &amp; 1 &amp; \\\\ &amp; &amp; \\lambda_1 &amp; \\\\ &amp; &amp; &amp; \\lambda_2 &amp; 1 \\\\ &amp; &amp; &amp; &amp; \\lambda_2 \\\\ &amp; &amp; &amp; &amp; &amp; \\lambda_3 &amp; 1 \\\\ &amp; &amp; &amp; &amp; &amp; &amp; \\ddots \\\\ &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\lambda_n &amp; 1 \\\\ &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\lambda_n \\end{bmatrix} \\] An illustration from wikipedia, the circled area is the Jordan blcok. Though the purpose of this section was not the computation details of Jordan matrices, it helps to give a concrete example. Consider \\(A\\) \\[ A = \\begin{bmatrix} 5 &amp; 4 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; -1 &amp; -1 \\\\ -1 &amp; -1 &amp; 3 &amp; 0 \\\\ 1 &amp; 1 &amp; -1 &amp; 2 \\end{bmatrix} \\] Including multiplicity, the eigenvalues of \\(A\\) are \\(\\lambda = 1, 2, 4, 4\\). And for \\(\\lambda = 4\\), the eigenspace is 1 dimensional instead of 2, meaning \\(A\\) is not diagonalizable. Nonetheless, \\(A\\) is similar to the following Jordan matrix \\[ J = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 2 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 4 &amp; 1\\\\ 0 &amp; 0 &amp; 0 &amp; 4 \\end{bmatrix} \\] To obtain \\(P\\), recall that \\(P^{-1}AP = J\\). Let \\(P\\) have column vectors \\(p_i, \\; i = 1,...,4\\), then: \\[ A[\\boldsymbol{p}_1 \\; \\; \\boldsymbol{p}_2 \\;\\; \\boldsymbol{p}_3 \\;\\; \\boldsymbol{p}_4] = [\\boldsymbol{p}_1 \\; \\; \\boldsymbol{p}_2 \\;\\; \\boldsymbol{p}_3 \\;\\; \\boldsymbol{p}_4] \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 2 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 4 &amp; 1\\\\ 0 &amp; 0 &amp; 0 &amp; 4 \\end{bmatrix} = [\\boldsymbol{p}_1 \\;\\; 2\\boldsymbol{p}_2 \\;\\; 4\\boldsymbol{p}_3 \\;\\; \\boldsymbol{p}_3 + 4\\boldsymbol{p}_4] \\] We see that \\[ \\begin{aligned} (A - 1I)\\boldsymbol{p}_1 &amp;= \\boldsymbol{0} \\\\ (A - 2I)\\boldsymbol{p}_2 &amp;= \\boldsymbol{0} \\\\ (A - 4I)\\boldsymbol{p}_3 &amp;= \\boldsymbol{0} \\\\ (A - 1I)\\boldsymbol{p}_4 &amp;= \\boldsymbol{p}_3 \\end{aligned} \\] The solutions \\(\\boldsymbol{p}_i\\) are called generalized eigenvectors of \\(A\\). 4.3 Symmetric matrices A square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) is symmetric if \\(A = A^{T}\\), and anti-symmetric if \\(A = - A^{T}\\). It can be shown that for any \\(A \\in \\mathbb{R}^{n \\times n}\\), \\(A + A^T\\) is symmetric and \\(A - A^T\\) anti-symmetric. So any square matrix \\(A\\) can be wrote as a sum of a symmetric matrix and an anti-symmetric matrix \\[ A = \\frac{1}{2}(A + A^T) + \\frac{1}{2}(A - A^T) \\] It is common to denote the set of all symmetric matrices of size \\(n\\) as \\(\\mathbb{S}^n\\), and \\(A \\in \\mathbb{S}^n\\) means \\(A\\) is a symmetric \\(n \\times n\\) matrix. Symmetric matrices have some nice properties about diagonalization. Theorem 1.7 If \\(A\\) is symmetric, eigenvectors from distinct eigenvalues are orthogonal. PROOF Let \\(\\boldsymbol{v}_1\\) and \\(\\boldsymbol{v}_2\\) be eigenvectors that correspond to distinct eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\). Compute \\[ \\begin{split} \\lambda_1\\boldsymbol{v}_1 \\cdot \\boldsymbol{v}_2 &amp;= (\\lambda_1\\boldsymbol{v}_1)^T\\boldsymbol{v}_2 \\\\ &amp;= (\\boldsymbol{v}_1^TA^T)\\boldsymbol{v}_2 \\\\ &amp;= \\boldsymbol{v}_1^T(A\\boldsymbol{v}_2) \\\\ &amp;= \\boldsymbol{v}_1^T(\\lambda_2\\boldsymbol{v}_2) \\\\ &amp;= \\lambda_2\\boldsymbol{v}_1 \\cdot \\boldsymbol{v}_2 \\end{split} \\] because \\(\\lambda_1 \\not = \\lambda_2\\), \\(\\boldsymbol{v}_1 \\cdot \\boldsymbol{v}_2 = 0\\). For symmetric matrices \\(A \\in \\mathbb{R}^{n \\times n}\\) without \\(n\\) distinct eigenvalues, it turns out that the dimension of the eigenspace for each \\(\\lambda_k\\) always equals the multiplicity of \\(\\lambda_k\\). For this reason, if \\(A\\) is a symmetric matrix we can always construct a orthonormal set \\(\\{\\boldsymbol{q}_1 \\;\\; \\cdots \\;\\; \\boldsymbol{q}_n\\}\\) from \\(\\{\\boldsymbol{v}_1 \\;\\; \\cdots \\;\\; \\boldsymbol{v}_n\\}\\) such that \\[ Q^{T} = \\begin{bmatrix} \\boldsymbol{q}_1^T \\\\ \\vdots \\\\ \\boldsymbol{q}_n^T \\end{bmatrix} = Q^{-1} \\] Recall that matrix \\(A\\) with \\(n\\) linearly independent eigenvectors is diagonalizable and can be written as \\[ A = P \\Lambda P^{-1} \\] where \\(P = [\\boldsymbol{v}_1 \\;\\; \\cdots \\;\\; \\boldsymbol{v}_n]\\) and \\(\\Lambda\\) is a diagonal matrix with eigenvalues on its diagonal entries. With symmetric matrices, \\(\\{\\boldsymbol{v}_1, \\cdots, \\boldsymbol{v}_n\\}\\) must be linearly independent and can be transformed into a orthonormal basis \\(\\{\\boldsymbol{q}_1, \\cdots, \\boldsymbol{q}_n\\}\\). With orthogonal matrix \\(Q =[\\boldsymbol{q}_1 \\;\\; \\cdots \\;\\; \\boldsymbol{q}_n]\\), we have \\[\\begin{equation} \\tag{4.2} A = Q \\Lambda Q^{T} \\end{equation}\\] Such matrix \\(A\\) is said to be orthogonally diagonalizable. We have seen that for symmetric matrix \\(A\\), Eq (4.2) always holds. We can also also verify that if \\(A\\) is orthogonally diagonalizable then it is a symmetric matrix \\[ A^T = (Q \\Lambda Q^{T})^T = (Q^T)^T\\Lambda^TQ^T= Q \\Lambda Q^{T} = A \\] Theorem 4.2 An \\(n \\times n\\) matrix \\(A\\) is orthogonally diagonalizable if an only if \\(A\\) is a symmetric matrix. 4.3.1 Spectral decomposition For orthogonally diagonalizable matrix \\(A\\), we have \\[ A = Q \\Lambda Q^{T} = [\\boldsymbol{q}_1 \\;\\; \\cdots \\;\\; \\boldsymbol{q}_n] \\begin{bmatrix} \\lambda_1 &amp; &amp; \\\\ &amp; \\ddots \\\\ &amp; &amp; \\lambda_n \\end{bmatrix} \\begin{bmatrix} \\boldsymbol{q}_1^T \\\\ \\vdots \\\\ \\boldsymbol{q}_n \\end{bmatrix} \\] It follows that \\[\\begin{equation} \\tag{4.3} A = \\lambda_1\\boldsymbol{q}_1\\boldsymbol{q}_1^T + \\cdots + \\lambda_1\\boldsymbol{q}_n\\boldsymbol{q}_n^T \\end{equation}\\] Eq (4.3) is called the spectral decomposition, breaking \\(A\\) into pieces of rank 1 matrix. It got this name because he set of eigenvalues of a matrix \\(A\\) is sometimes called its spectrum. 4.4 Quadratic forms Definition 1.3 (Quadratic form) A quadratic form on \\(\\mathbb{R}^n\\) is a function \\(Q\\) defined on \\(\\mathbb{R}^n\\) whose value at a vector \\(\\boldsymbol{x}\\) in \\(\\mathbb{R}^n\\) can be computed by an expression of the form \\(Q(\\boldsymbol{x}) = \\boldsymbol{x}^TA\\boldsymbol{x}\\), where \\(A \\in \\mathbb{R}^{n \\times n}\\) is a symmetric matrix. \\(A\\) is called the matrix of the quadraticc form. There exists a one-to-one mapping between symmetric matrix \\(A\\) and the quadratic form. Consider the \\(3 \\times 3\\) case: \\[ \\boldsymbol{x} = \\begin{bmatrix} x_1 \\\\ x_3 \\\\ x_3 \\\\ \\end{bmatrix} , \\;\\; A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{bmatrix} \\\\ \\] \\[ \\begin{split} \\boldsymbol{x}^TA\\boldsymbol{x} &amp;= [x_1 \\;\\; x_2 \\;\\; x_3] \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_3 \\\\ x_3 \\\\ \\end{bmatrix} \\\\ &amp;= a_{11}x_1^2 + a_{22}x_2^2 + a_{33}x_3^2 + \\\\ &amp; \\quad(a_{12} + a_{21})x_1x_2 + (a_{13} + a_{31})x_1x_3 + (a_{23} + a_{32})x_2x_3 \\end{split} \\tag{1} \\] Since \\(A\\) is symmetric, we have \\(a_{ij} = a_{ji}\\), thus \\[ \\boldsymbol{x}^TA\\boldsymbol{x} = a_{11}x_1^2 + a_{22}x_2^2 + a_{33}x_3^2 + 2a_{12}x_1x_2 + 2a_{13}x_1x_3 + 2a_{23}x_2x_3 \\tag{2} \\] This verifies that \\(\\boldsymbol{x}^TA\\boldsymbol{x}\\) when \\(A \\in \\mathbb{R}^{n \\times n}\\) is symmetric does result in a quadratic function of \\(n\\) variables. Conversely, any quadratic function of \\(n\\) variables, like shown in \\((2)\\), can be expressed in terms of \\(\\boldsymbol{x}^TA\\boldsymbol{x}\\) with unique choice of symmetric matrix \\(A \\in \\mathbb{R}^{n \\times n}\\). 4.4.1 Change of variabele If \\(\\boldsymbol{x}\\) is a variable vector in \\(\\mathbb{R}^n\\), then a change of variable is an equation of the form \\[ \\begin{aligned} \\boldsymbol{x} &amp;= P\\boldsymbol{y} \\\\ \\text{or equivalently} \\quad \\boldsymbol{y} &amp;= P^{-1}\\boldsymbol{x} \\end{aligned} \\] where \\(P\\) is any invertible matrix \\(\\in \\mathbb{R}^{n \\times n}\\) Theorem 4.3 (The Principal Axes Theorem) Let \\(A\\) be an \\(n \\times n\\) symmetric matrix. Then there exists an orthogonal change of variable, \\(\\boldsymbol{x} = Q\\boldsymbol{y}\\), this transform the quadratic form \\(\\boldsymbol{x}^TA\\boldsymbol{x}\\) into a quadratic form \\(\\boldsymbol{y}^T\\Lambda\\boldsymbol{y}\\) with no cross-product term. \\(Q\\) is constructed with \\(A\\)’s orthonormal eigenvectors \\(\\boldsymbol{q}_1, ..., \\boldsymbol{q}_n\\). According to theorem (4.2): \\[ \\boldsymbol{x}^TA\\boldsymbol{x} = (Q\\boldsymbol{y})^TA(Q\\boldsymbol{y}) = \\boldsymbol{y}^TQ^{T}AQ\\boldsymbol{y} = \\boldsymbol{y}^T \\Lambda \\boldsymbol{y} \\] The principal axes theorem 4.3 shows that if \\(A\\) is diagonalizable, quadratic form \\(\\boldsymbol{x}^TA\\boldsymbol{x}\\) can be reexpressed into the form \\(\\lambda_1y_1^2 + \\lambda_2y_2^2 + \\cdots + \\lambda_ny_n^2\\) with change of variables \\(\\boldsymbol{x} = Q\\boldsymbol{y}\\). 4.4.2 Classification of quadratic forms A symmetric matrix \\(A \\in \\mathbb{S}^n\\) is positive definite (PD) if for all non-zero vectors \\(\\boldsymbol{x} \\in \\mathbb{R}^n,\\; \\boldsymbol{x}^TA\\boldsymbol{x} &gt; 0\\). We can denote positive definite matrix \\(A\\) as \\(A \\succ 0\\) (or \\(A &gt; 0\\)). The set of all positive definite matrices is denoted as \\(\\mathbb{S}_{++}^n\\) A symmetric matrix \\(A \\in \\mathbb{S}^n\\) is positive semidefinite (PSD) if for all non-zero vectors \\(\\boldsymbol{x} \\in \\mathbb{R}^n,\\; \\boldsymbol{x}^TA\\boldsymbol{x} \\ge 0\\). We can denote positive definite matrix \\(A\\) as \\(A \\succeq 0\\) (or \\(A \\ge 0\\)). The set of all positive semidefinite matrices is denoted as \\(\\mathbb{S}_{+}^n\\) A symmetric matrix \\(A \\in \\mathbb{S}^n\\) is negative definite (ND), denoted by \\(A \\prec 0\\) (or \\(A &lt; 0\\)), if for all non-zero vectors \\(\\boldsymbol{x} \\in \\mathbb{R}^n,\\; \\boldsymbol{x}^TA\\boldsymbol{x} &lt; 0\\). Similarly, a symmetric matrix \\(A \\in \\mathbb{S}^n\\) is negative semidefinite (NSD), denoted by \\(A \\preceq 0\\) (or \\(A \\le 0\\)), if for all non-zero vectors \\(\\boldsymbol{x} \\in \\mathbb{R}^n,\\; \\boldsymbol{x}^TA\\boldsymbol{x} \\le 0\\). Finally, a symmetric matrix \\(A \\in \\mathbb{S}^n\\) is indefinite, if it is neither positive semidefinite or negative semidefinite. In other words, if there exists \\(\\boldsymbol{x}, \\boldsymbol{x}&#39;, \\in \\mathbb{R}^{n}\\) such taht \\(\\boldsymbol{x}^TA\\boldsymbol{x} &gt; 0\\) and \\(\\boldsymbol{x&#39;}^TA\\boldsymbol{x}&#39; &gt; 0\\) Note that when talking about \\(A\\) being PD, PSD, ND, NSD or indefinite, \\(A\\) is always assumed to be symmetric. Also, if \\(A\\) is positive definite, then \\(−A\\) is negative definite and viceversa. Likewise, if \\(A\\) is positive semidefinite then \\(−A\\) is negative semidefinite and vice versa. If \\(A\\) is indefinite, then so is \\(−A\\). From theorem 4.3, we know that the sign of eigenvalues are closely related to classifications of symmetric matrices here. Take positive definite matrices for example, the following statements of \\(A\\) are equivalent: For any \\(\\boldsymbol{x} \\in \\mathbb{R}^n, \\; \\boldsymbol{x}^TA\\boldsymbol{x} &gt; 0\\) Let \\(\\lambda_i, \\; i = 1, ..., n\\) be \\(A\\)’s eigenvalues, \\(\\lambda_i &gt; 0\\) All leading determinants of \\(A &gt; 0\\) All pivots are \\(&gt; 0\\) Classification of \\(A \\in \\mathbb{S}^{n}\\) by its eigenvalue can be applied in general. Theorem 4.4 (Quadratic forms and eigenvalues) Let \\[A \\in \\mathbb{S}^{n}\\]. Then the quadratic form \\(\\boldsymbol{x}^TA\\boldsymbol{x}\\) and \\(A\\) is: positive definite if and only if the eigenvalues of \\(A\\) are all positive negative definite if and only if the eigenvalues of \\(A\\) are all negative indefinite if and only if \\(A\\) has both positive and negative eigenvalues Proposition 4.2 Given any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(A^TA\\) is a positive semidefinite matrix PROOF By definition, \\(A^TA\\) is a positive semidefinite matrix if for any \\(\\boldsymbol{x} \\in \\mathbb{R}^n\\), the quadratic form \\(\\boldsymbol{x}^T(A^TA)\\boldsymbol{x} \\ge 0\\). \\[ \\begin{split} \\boldsymbol{x}^T(A^TA)\\boldsymbol{x} &amp;= (\\boldsymbol{x}^TA^T)(A\\boldsymbol{x}) \\\\ &amp;= (A\\boldsymbol{x})^T(A\\boldsymbol{x}) \\\\ &amp;= ||A\\boldsymbol{x}||^2 \\end{split} \\] It turns out that the result is the square of the 2-norm of \\(A\\boldsymbol{x}\\) 4.5 Rayleigh quotients Let \\(A \\in \\mathbb{S}^n\\) and \\(\\boldsymbol{x} \\in \\mathbb{R}^n\\), Rayleigh quotient is defined as \\[ R_{A}(\\boldsymbol{x}) = \\frac{\\boldsymbol{x}^TA\\boldsymbol{x}}{\\boldsymbol{x}^T\\boldsymbol{x}} \\] The Rayleigh quotient has some nice properties: scale invariance: for any vector \\(\\boldsymbol{x} \\not= 0\\) and any scalar \\(\\alpha \\not= 0\\), \\(R_{A}(\\boldsymbol{x}) = R_{A}(\\alpha\\boldsymbol{x})\\) If \\(\\boldsymbol{x}\\) is a eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\), then \\(R_{A}(\\boldsymbol{x}) = \\lambda\\) The Rayleigh quotient is bounded by the largest and smallest eigenvalue of \\(A\\), i.e. \\[ \\lambda_{\\text{min}}(A) \\le R_{A}(\\boldsymbol{x}) \\le \\lambda_{\\text{max}}(A) \\] PROOF Since the Rayleigh quotient does not depend on the 2-norm of vector \\(\\boldsymbol{x}\\), we may assume a unit vector \\(\\boldsymbol{x}^T\\boldsymbol{x} = 1\\), and Rayleigh quotient simplifies to the quadratic form \\(\\boldsymbol{x}^TA\\boldsymbol{x}\\). Next, orthogonally diagonalize \\(A\\) as \\(Q \\Lambda Q\\), we know that when \\(\\boldsymbol{x} = Q \\boldsymbol{y}\\): \\[ \\boldsymbol{x}^TA\\boldsymbol{x} = \\boldsymbol{y}^T \\Lambda \\boldsymbol{y} \\tag{1} \\] Also \\[ 1= \\boldsymbol{x}^T\\boldsymbol{x} = (Q\\boldsymbol{y})^T Q\\boldsymbol{y} = \\boldsymbol{y}^TQ^TQ\\boldsymbol{y} = \\boldsymbol{y}^T\\boldsymbol{y} \\tag{2} \\] Expand \\(\\boldsymbol{y}^T \\Lambda \\boldsymbol{y}\\) in (1) we get \\[ \\boldsymbol{x}^TA\\boldsymbol{x} = \\lambda_1y_1^2 + \\lambda_2y_2^2 + \\cdots + \\lambda_ny_n^2 \\tag{3} \\] where \\(\\{\\lambda_1, ..., \\lambda_n\\}\\) are diagonal entries of \\(\\Lambda\\) and eigenvalues of \\(A\\). Let us suppose that the set \\(\\{\\lambda_1, \\lambda_2, \\cdots, \\lambda_n\\}\\) has already been ordered descendingly, so that \\(\\lambda_1 &gt; \\lambda_2 &gt; \\cdots &gt; \\lambda_n\\). We can obtain the inequality from (3) and the order of eigenvalues: \\[ \\begin{split} \\boldsymbol{x}^TA\\boldsymbol{x} &amp;= \\lambda_1y_1^2 + \\lambda_2y_2^2 + \\cdots + \\lambda_ny_n^2 \\\\ &amp;\\le \\lambda_1y_1^2 + \\underbrace{\\lambda_1y_2^2 + \\cdots + \\lambda_1y_n^2}_{\\lambda_1 \\text{ is the greatest eigenvalue}} \\\\ &amp;\\le \\lambda_1(\\boldsymbol{y}^T\\boldsymbol{y}) \\\\ &amp;= \\lambda_1 \\end{split} \\] The equation reach equality if and only if \\([y_1, y_2, \\cdots, y_n] = [1, 0, \\cdots, 0]\\). Since \\(\\boldsymbol{x} = Q\\boldsymbol{y}\\), we have \\[ \\boldsymbol{x} = \\begin{bmatrix} \\boldsymbol{q}_1 &amp; \\boldsymbol{q}_2 &amp; \\cdots &amp; \\boldsymbol{q}_n \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} = \\boldsymbol{q}_1 \\] Similarly, the minimum of the Rayleigh quotient will be \\(\\lambda_n\\), with \\(\\boldsymbol{x} = \\boldsymbol{q}_n\\). From the optimization perspective, the bound of Rayleigh quotient amounts to a constrained optimization problem \\[ \\begin{aligned} \\text{objective function} &amp;: \\boldsymbol{x}^TA\\boldsymbol{x}\\\\ \\text{subject to}&amp;: \\boldsymbol{x}^T\\boldsymbol{x} = 1 \\end{aligned} \\] The maximum and minimum of the objective function are \\(\\lambda_1\\) and \\(\\lambda_n\\), with \\(\\boldsymbol{x}\\) being \\(\\boldsymbol{q}_1\\) and \\(\\boldsymbol{q}_n\\) respectively. If we add more constraints, for example, that \\(\\boldsymbol{x}\\) should be orthogonal to \\(\\boldsymbol{q}_1\\), then \\(\\boldsymbol{x}^TA\\boldsymbol{x}\\) has maximum \\(\\lambda_2\\) attained at \\(\\boldsymbol{x} = \\lambda_2\\) Theorem 4.5 Let \\(A \\in \\mathbb{S}^n\\) with orthogonal diagonalization \\(A = Q\\Lambda Q^T\\), where the entries on the diagonal of \\(\\Lambda\\) are arranged so that \\(\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_n\\). Then for \\(k = 2, ...\\), the maximum of value of \\(\\boldsymbol{x}^T A \\boldsymbol{x}\\) subject to constraints \\[ \\boldsymbol{x}^T\\boldsymbol{x} = 1, \\;\\; \\boldsymbol{x}^T\\boldsymbol{q}_1 = 0, \\;\\; \\dots \\;\\;, \\boldsymbol{x}^T\\boldsymbol{q}_{k-1} = 0 \\] is the eigenvalue \\(\\lambda_k\\), and this maximum is attained at \\(\\boldsymbol{x} = \\boldsymbol{q}_k\\) PROOF From \\(\\boldsymbol{x} = P\\boldsymbol{y}\\) we know that \\[ \\boldsymbol{x} = y_1\\boldsymbol{q}_1 + \\cdots + + y_{k-1}\\boldsymbol{q}_{k-1} + y_k\\boldsymbol{q}_k + \\cdots + y_{n}\\boldsymbol{q}_n \\] Left multiply by \\(\\boldsymbol{q}_1^T\\) \\[ \\begin{aligned} \\boldsymbol{q}_1^T\\boldsymbol{x} &amp;= y_1\\boldsymbol{q}_1^T\\boldsymbol{q}_1 + \\cdots + + y_{k-1}\\boldsymbol{q}_1^T\\boldsymbol{q}_{k-1} + y_k\\boldsymbol{q}_1^T\\boldsymbol{q}_k + \\cdots + y_{n}\\boldsymbol{q}_1^T\\boldsymbol{q}_n \\\\ &amp;= y_1\\boldsymbol{q}_1^T\\boldsymbol{q}_1 \\\\ &amp;= y_1 \\end{aligned} \\] Since \\(\\boldsymbol{q}_1^T\\boldsymbol{x} = \\boldsymbol{x}^T\\boldsymbol{q}_1 = 0\\), we have \\(y_1 = 0\\). Similarly, \\(y_2 = \\cdots = y_{k-1} = 0\\), and \\(\\boldsymbol{y}\\) becomes \\([0 \\;\\; \\cdots \\;\\; 0 \\;\\; y_{k} \\;\\; \\cdots \\;\\; y_n]\\). And the inequality now becomes: \\[ \\begin{split} \\boldsymbol{x}^TA\\boldsymbol{x} &amp;= \\lambda_1y_1^2 + \\lambda_2y_2^2 + \\cdots + \\lambda_ny_n^2 \\\\ &amp;= \\lambda_ky_k^2 + \\cdots + \\lambda_ny_n^ 2 \\\\ &amp;\\le \\lambda_ky_k^2 + \\cdots + \\lambda_ky_n^2 \\\\ &amp;\\le \\lambda_k(\\boldsymbol{y}^T\\boldsymbol{y}) \\\\ &amp;= \\lambda_k \\end{split} \\] It’s easy to see that \\(\\boldsymbol{x}^TA\\boldsymbol{x}\\) gets its maximum \\(\\lambda_k\\) when \\(y_k = 0\\) and other weights being zero. So the solution \\(\\boldsymbol{x}\\) can be solved as \\[ \\begin{split} \\boldsymbol{x} &amp;= \\begin{bmatrix} \\boldsymbol{q}_1 &amp; \\cdots &amp; \\boldsymbol{q}_k &amp; \\boldsymbol{q}_{k+1} &amp; \\cdots &amp;\\boldsymbol{q}_n \\end{bmatrix} \\begin{bmatrix} 0 \\\\ \\vdots \\\\ \\underbrace{1}_{k\\text{th weight}} \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\\\ &amp;= \\boldsymbol{q}_k \\end{split} \\] 4.6 SVD 4.6.1 Singular values of m x n matrix Singular value decomposition illustrates a way of decomposing any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) into the form \\(U \\Sigma V^T\\), where \\(U = [\\boldsymbol{u}_1 \\;\\; \\cdots \\;\\; \\boldsymbol{u}_n]\\) and \\(V = [\\boldsymbol{v}_1 \\;\\; \\cdots \\;\\; \\boldsymbol{v}_n]\\) are both orthogonal matrices, and \\(\\Sigma\\) a diagonal matrix with entries being the square root of the eigenvalues of \\(A^TA\\) (perhaps plus some zeros). Before proceeding to the theorem, let’s explore the motivating idea behind SVD. For (square) diagonalizable matrix \\(A \\in \\mathbb{S}^{n}\\), the absolute value of the eigenvalues measure the amounts that \\(A\\) stretches or shrinks eigenvectors, consider the ratio between the length of \\(\\boldsymbol{x}\\) before and after left multiplied by \\(A\\) \\[ \\frac{||A\\boldsymbol{x}||}{||\\boldsymbol{x}||} = \\frac{||\\lambda\\boldsymbol{x}||}{||\\boldsymbol{x}||} = \\frac{\\lambda||\\boldsymbol{x}||}{||\\boldsymbol{x}||} = \\lambda \\] If \\(\\lambda_1\\) is the greatest eigenvalue, then the corresponding eigenvector \\(\\boldsymbol{v}_1\\) identifies the direction in which \\(A\\)’s stretching effect is greatest. So, the question is, can we identify such similar direction for rectangular matrices \\(A \\in \\mathbb{R}^{m \\times n}\\), even though they does not have eigenvalues and eigenvectors? The answer is yes. Note that maximize \\(\\frac{||A\\boldsymbol{x}||}{||\\boldsymbol{x}||}\\) is equivalent to maximize \\(\\frac{||A\\boldsymbol{x}||^2}{||\\boldsymbol{x}||^2}\\) $$ \\[\\begin{split} \\frac{||A\\boldsymbol{x}||^2}{||\\boldsymbol{x}||^2} &amp;= \\frac{(A\\boldsymbol{x})^T(A\\boldsymbol{x})}{\\boldsymbol{x}^T\\boldsymbol{x}} \\\\ &amp;= \\frac{\\boldsymbol{x}^T(A^TA)\\boldsymbol{x}}{\\boldsymbol{x}^T\\boldsymbol{x}} \\end{split}\\] $$ Since \\(A^TA\\) is symmetric, this is the form of a Rayleigh quotients 4.5! We know that the largest possible value is of this quotient \\(\\lambda_1\\), the greatest eigenvalue of \\(A^TA\\), with \\(\\boldsymbol{x} = \\boldsymbol{v}_1\\), among the orthonormal set \\(\\{\\boldsymbol{v}_1, \\cdots, \\boldsymbol{v}_n\\}\\). Note that here \\(V = [\\boldsymbol{v}_1 \\;\\; \\cdots \\;\\; \\boldsymbol{v}_n]\\) is already a orthogonal matrix, previously denoted by \\(Q\\). To sum up, the greatest possible stretching ratio of \\(A \\in \\mathbb{R}^{m \\times n}\\) on a vector \\(\\boldsymbol{x} \\in \\mathbb{R}^n\\) is \\(\\sqrt{\\lambda_1}\\). Generally, let \\(\\{\\boldsymbol{v}_1, \\cdots, \\boldsymbol{v}_n\\}\\) be a orthonormal basis for \\(\\mathbb{R}^n\\) consisting of eigenvectors of \\(A^TA\\), and \\(\\lambda_1, ..., \\lambda_n\\) be the eigenvalues of \\(A^TA\\), for \\(i = 1, \\cdots, n\\) \\[ ||A\\boldsymbol{v}_i|| ^ 2 = \\boldsymbol{v}_i^T(A^TA)\\boldsymbol{v}_i = \\lambda_i\\boldsymbol{v}_i^T\\boldsymbol{v}_i = \\lambda_i \\] Definition 4.4 (Singular values) The singular values of \\(A\\) are the square roots of the eigenvalues of \\(A^TA\\), denoted by \\(\\sigma_1, ..., \\sigma_n\\). That is, \\(\\sigma_i = \\sqrt{\\lambda_i}\\), and they are often arranged in descending order so that \\(\\lambda_1 \\ge \\cdots \\ge \\lambda_n\\). Geometrically, singular values of \\(A\\) are the lengths of the vectors \\(A\\boldsymbol{v}_1, ..., A\\boldsymbol{v}_n\\), where \\(\\{\\boldsymbol{v}_1, ..., \\boldsymbol{v}_n\\}\\) is the orthonormal basis of \\(A^TA\\)’s eigenspace. Theorem 4.6 Proceeding from previous definitons of singular values, and suppose \\(A\\) has at least one nonzero singular values. Then \\(\\{A\\boldsymbol{v}_1, ..., A\\boldsymbol{v}_r\\}\\) is an orthogonal basis for \\(\\text{Col}\\; A\\), and \\(\\text{rank} \\;A = r\\) PROOF First, let’s examine that \\(\\{A\\boldsymbol{v}_1, ..., A\\boldsymbol{v}_r\\}\\) is a orthogonal basis: any pair of two distinct vectors \\(A\\boldsymbol{v}_i, A\\boldsymbol{v}_j, \\; i,j = 1, ..., r\\) are orthogonal to each other $$ \\begin{split} (A_i)^T(A_j) &amp;= _iTATA_j \\ &amp;= _i^T(_j_j) \\ &amp;= 0 \\end{split} $$ Next, we will show that any vector in \\(\\text{Col}\\; A\\) is a linear a combination of \\(\\{A\\boldsymbol{v}_1, ..., A\\boldsymbol{v}_r\\}\\). Note that \\(\\{\\boldsymbol{v}_1, ..., \\boldsymbol{v}_r\\}\\) is a orthonormal basis of \\(A^TA\\)’s eigenspace \\(\\mathbb{R}\\). So for any vector \\(\\boldsymbol{y} = A\\boldsymbol{x}\\) in \\(\\text{Col}\\; A\\) , we can write \\(\\boldsymbol{x} = c_1\\boldsymbol{v}_1 + \\cdots + c_n\\boldsymbol{v}_n\\), thus \\[ \\begin{split} \\boldsymbol{y} &amp;= A\\boldsymbol{x} = A(c_1\\boldsymbol{v}_1 + \\cdots + c_n\\boldsymbol{v}_n) \\\\ &amp;= c_1 A \\boldsymbol{v}_1 + \\cdots + c_r A \\boldsymbol{v}_r + c_{r+1} A \\boldsymbol{v}_{r+1} + \\cdots + c_n A \\boldsymbol{v}_n \\end{split} \\tag{1} \\] Since \\(\\lambda_{r+1} = \\cdots = \\lambda_{n} = 0\\), \\(A\\boldsymbol{v}_{r+1}, ..., A\\boldsymbol{v}_{n}\\) have length \\(0\\): they are zero vectors. And (1) is reduced to \\[ \\boldsymbol{y} = c_1 A \\boldsymbol{v}_1 + \\cdots + c_r A\\boldsymbol{v}_r \\] Thus \\(\\boldsymbol{y} \\in \\text{Col}\\; A\\) is in Span\\(\\{A\\boldsymbol{v}_1, ..., A\\boldsymbol{v}_r\\}\\), and \\(\\{A\\boldsymbol{v}_1, ..., A\\boldsymbol{v}_r\\}\\) is an orthogonal basis for \\(\\text{Col} \\;A\\). This also shows that the column rank of \\(A\\) is equal to its number of nonzero singular values. 4.6.2 The singular value decomposition Let’s begin SVD by the \\(m \\times n\\) diagonal matrix \\(\\Sigma\\) of the form \\[ \\Sigma = \\begin{bmatrix} \\sigma_1 \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\sigma_r \\\\ &amp; &amp; &amp; 0 \\\\ &amp; &amp; &amp; &amp; \\ddots \\\\ &amp; &amp; &amp; &amp; &amp; 0 \\\\ \\end{bmatrix} \\tag{1} \\] There are \\(r\\) nonzero entries on the diagonal, being \\(A\\)’s nonzero singular values, and the left positions are filled by \\(0\\) to form a \\(m \\times n\\) matrix. If \\(r\\) equals \\(m\\) or \\(n\\) or both, some or all of the zero blocks do not appear. Theorem 4.7 (The Singular Value Decomposition) Let \\(A \\in \\mathbb{R}^{m \\times n}\\) with rank \\(r\\). There exists an diagonal matrix \\(\\mathbb{\\Sigma} \\in \\mathbb{R}^{m \\times n}\\) as in (1) for which the first \\(r\\ \\times r\\) block is a diagonal matrix with the first \\(r\\) singular values on its diagonal, and there exist \\(U \\in \\mathbb{R}^{m \\times m}\\) and \\(V \\in \\mathbb{R}^{n \\times n}\\) such that \\[ A = U \\Sigma V^T \\] \\[ \\begin{split} U\\Sigma &amp;= [\\boldsymbol{u}_1 \\;\\; \\cdots \\;\\; \\boldsymbol{u}_n] \\begin{bmatrix} \\sigma_1 \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\sigma_r \\\\ &amp; &amp; &amp; 0 \\\\ &amp; &amp; &amp; &amp; \\ddots \\\\ &amp; &amp; &amp; &amp; &amp; 0 \\\\ \\end{bmatrix} \\\\ &amp;= [\\sigma_1\\boldsymbol{u}_1 \\;\\; \\cdots \\;\\; \\sigma_n\\boldsymbol{u}_n] \\\\ &amp; = [A\\boldsymbol{v}_1 \\;\\; \\cdots \\;\\; A\\boldsymbol{v}_n] \\\\ &amp;= AV \\end{split} \\] "],
["matrix-calculus.html", "Chapter 5 Matrix calculus", " Chapter 5 Matrix calculus \\[ \\frac{\\partial{f(x)}}{\\partial{x}} \\] "],
["taylor-series-and-expansion.html", "Chapter 6 Taylor series and expansion", " Chapter 6 Taylor series and expansion "],
["probability-basics.html", "Chapter 7 Probability basics", " Chapter 7 Probability basics "],
["moments.html", "Chapter 8 Moments", " Chapter 8 Moments "],
["linear-models.html", "Chapter 9 Linear models 9.1 Least square estimation 9.2 Maximum likelihood estimation", " Chapter 9 Linear models 9.1 Least square estimation From theorem 3.3 we know that \\[ \\boldsymbol{\\beta} = (X^TX)^{-1}X^T\\boldsymbol{y} \\] thus: \\[ \\hat{\\boldsymbol{y}} = X\\boldsymbol{\\beta} = X (X^TX)^{-1}X^{T}\\boldsymbol{y} \\] When columns of the design matrix \\(A\\) are orthogonal, the orthogonal projection of \\(\\boldsymbol{y}\\) onto Col\\(X\\) isi given by \\[ \\hat{\\boldsymbol{y}} = \\frac{\\boldsymbol{x}_0^T\\boldsymbol{y}}{\\boldsymbol{x}_0^T\\boldsymbol{x}_0}\\boldsymbol{x}_0 + \\frac{\\boldsymbol{x}_1^T\\boldsymbol{y}}{\\boldsymbol{x}_1^T\\boldsymbol{x}_1}\\boldsymbol{x}_1 \\cdots + \\frac{\\boldsymbol{x}_p^T\\boldsymbol{y}}{\\boldsymbol{x}_p^T\\boldsymbol{x}_p}\\boldsymbol{x}_p \\] 9.2 Maximum likelihood estimation "],
["principal-component-analysis.html", "Chapter 10 Principal component analysis", " Chapter 10 Principal component analysis "],
["references.html", "References", " References Aggarwal, Charu C. 2020. Linear Algebra and Optimization for Machine Learning - A Textbook. Springer. https://doi.org/10.1007/978-3-030-40344-7. DasGupta, Anirban. 2011. Probability for Statistics and Machine Learning: Fundamentals and Advanced Topics. 1st ed. Springer Publishing Company, Incorporated. Deisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020. Lay, David. 2006. Linear Algebra and Its Applications. Vols. 3:CD-ROM. Pearson, Addison Wesley. Strang, Gilbert. 2006. Linear Algebra and Its Applications. Belmont, CA: Thomson, Brooks/Cole. http://www.amazon.com/Linear-Algebra-Its-Applications-Edition/dp/0030105676. Xie, Yihui. 2020. Bookdown: Authoring Books and Technical Documents with R Markdown. https://CRAN.R-project.org/package=bookdown. "]
]
