[
["index.html", "Mathematical Notes for Machine Learning Preface", " Mathematical Notes for Machine Learning Qiushi Yan 2020-08-12 Preface This document aims to collect notes on various mathematical topics relevant to machine learning problems, including linear algebra, calculus, probability theory, and possibly optimization (statistics should be in another stand-alone document). The document is generated by the bookdown package (Xie 2020). References (books, online courses, videos, papers) include: general Mathematics for Machine Learning by Garret Thomas Mathematics for Machine Learning (Deisenroth, Faisal, and Ong 2020) Linear Algebra Linear Algebra and its Applications (Lay 2006) Linear Algebra and Optimization for Machine Learning (Aggarwal 2020) Matrix Algebra: Theory, Computations, and Applications in Statistics (Gentle 2017) Linear Algebra and its Applications (Strang 2006) Linear Algebra Review and Reference on Stanford’s cs229 website Gilbert Strang. 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning. Spring 2018. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA. Probability Thoery Probability for Statistics and Machine Learning: Fundamentals and Advanced Topics (DasGupta 2011) Introduction to Probability (K. Blitzstein and Hwang 2019) Introduction to Probability (Bertsekas and Tsitsiklis 2008) John Tsitsiklis, and Patrick Jaillet. RES.6-012 Introduction to Probability. Spring 2018. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA. "],
["matrix-basics.html", "Chapter 1 Matrix basics 1.1 Matrix multiplication 1.2 Elemetary matrix and row operations 1.3 LU factorization 1.4 Determinants 1.5 Trace 1.6 Matrix inversion 1.7 Matrix multiplication as linear transformation 1.8 Statistics and proabability", " Chapter 1 Matrix basics 1.1 Matrix multiplication A common way of looking at matrix-vector multiplication \\(A\\bar{x}\\) is to think of as a linear combination of column vectors in \\(A\\): \\[ \\begin{aligned} A\\bar{x} &amp;= [\\bar{a}_1 \\;\\; \\cdots \\;\\; \\bar{a}_n] \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\\\ &amp;= x_1\\bar{a}_1 + \\cdots + x_n\\bar{a}_n \\end{aligned} \\] Likewise, for any \\(\\bar{x}^{T} = [x_1, \\cdots, x_n]^T\\) and matrix \\(A_{m \\times n}\\), \\(x^{T}A\\) can be thought of as a linear combination of rows in \\(A\\) to produce a new row vector: \\[ [x_1 \\;\\; \\cdots \\;\\; x_n] \\begin{bmatrix} \\bar{a}_1^T \\\\ \\vdots \\\\ \\bar{a}_n^T \\end{bmatrix} = x_1\\bar{a}_1^T + \\dots + x_n\\bar{a}_n^T \\] For matrix-matrix multiplication \\(AB\\), besides the dot product definition we can see it as column row expansion. Theorem 1.1 (Column-row expansion of \\(AB\\)) if \\(A\\) is \\(m \\times n\\) and \\(B\\) is \\(n \\times p\\), then \\[ \\begin{aligned} AB &amp;= [\\text{col}_1(A) \\cdots \\text{col}_n(A)] \\begin{bmatrix} \\text{row}_1(B) \\\\ \\vdots \\\\ \\text{row}_n(B) \\end{bmatrix} \\\\ &amp;= \\text{col}_1(A)\\text{row}_1(B) + \\cdots + \\text{col}_n(A)\\text{row}_n(B) \\end{aligned} \\] Note that each \\(\\text{col}_1(A)\\text{row}_1(B)\\) is a rank 1 \\(m \\times p\\) matrix. 1.2 Elemetary matrix and row operations An elementary matrix is one that is obtained by performing a single elementary row operation on an identity matrix \\(I\\). Each elementary matrix \\(E\\) is invertible. The inverse of \\(E\\) is the elementary matrix of the same type that transforms \\(E\\) back into \\(I\\). Left multiplication by a elementary matrix has a nice illustration. There are 3 primary types of elementary matrices (example for \\(3 \\times 3\\)): \\[ E_1 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ -4 &amp; 0 &amp; 1 \\end{bmatrix} ,\\; E_2 = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} ,\\; E_3 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 5 \\end{bmatrix} \\] \\(E_1, E_2, E_3\\) represents 3 types of elementary row operations applicable to a \\(3 \\times 3\\) matrix, Row addition, a scalar multiple of the \\(i\\)th row is added to the \\(j\\)th row Row interchange, the \\(i\\)th row and the \\(j\\)th row of the matrix are interchanged Row scaling, the \\(i\\)th row is multiplied by a scalar. \\[ \\begin{aligned} E_1A &amp;= \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ - 4a_{11} + a_{31} &amp; -4a_{12} + a_{32} &amp; -4a_{13} + a_{33} \\end{bmatrix} \\\\ \\\\ E_2A &amp;= \\begin{bmatrix} a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{bmatrix} \\\\ \\\\ E_3A &amp;= \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ 5a_{31} &amp; 5a_{32} &amp; 5a_{33} \\\\ \\end{bmatrix} \\end{aligned} \\] Thus, any row operation on \\(A\\) is equivalent to left multiply a corresponding elementary matrix \\(E\\). Since row operation are invertible, elementary matrices are invertible. This gives a general way of finding the inverse matrix of \\(A\\). Theorem 1.2 (an algorithm for finding inverse matrices) Row reduce the augmented matrix \\([A \\;\\; I]\\), when \\(A\\) becomes \\(I\\), \\(I\\) becomes \\(A^{-1}\\). Otherwise \\(A^{-1}\\) is not invertible. 1.3 LU factorization https://fml-fam.github.io/blog/2020/07/03/matrix-factorizations-for-data-analysis/ A factorization of matrix \\(A\\) is an equation that expresses \\(A\\) as a product of two or more matrices. Definition 1.1 (LU factorization) Suppose \\(A\\) can be reduced to an echelon form \\(U\\) using only row addition and rowo scaling, there exist a set of unit lower trangular matrices \\(E_1, \\dots, E_p\\) such that \\[ E_p \\cdots E_1A = U \\] Then \\[ A = (E_p \\cdots E_1)^{-1}U = LU \\] where \\(u\\) is a the upper triangular row echelon form (or upper trapezoidal), and \\(L\\) an lower triangular matrix \\[ L = (E_p \\cdots E_1)^{-1} \\] LU decomposition express a matrix (don’t have to be square or invertible) as the product of a square lower triangular matrix \\(L\\) and a rectangular upper triangular matrix \\(U\\). From the definition, we know that row operations on \\(A\\) must only be confined to row addition and row scaling, but not row interchange. Otherwise \\((E_p \\cdots E_1)^{-1}\\) cannot be lower triangular. The most common needs for row exchanges in row reduction is when \\(a_{11}\\) is 0. For example, the following matrix does not have a Lu factorization because it first requires exchanging two rows to produce row echelon form1 \\[ \\begin{bmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{bmatrix} \\] To give a former definition of LU factorization, one can show that if all principal submatrices are non-singular2, then the factorization exists. Note that this condition is not necessary, the following matrix has a zero in (2, 2) position violating the rule, but it can still be expressed in terms of LU \\[ \\begin{bmatrix} 2 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 0 &amp; 0 \\\\ 1 &amp; -\\frac{1}{2} &amp; 0 \\\\ 1 &amp; \\frac{1}{2} &amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; \\frac{1}{2} &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] If an LU factorization exists, we shall notice that the LU form is “asymmetric” on the diagonal, in the sense that L has all \\(1\\)s and \\(U\\) has the pivots. We can factor out another diagonal matrix \\(D\\) to also have all \\(1\\)s on \\(U\\)’s diagonal \\[ U = \\begin{bmatrix} d_{1} \\\\ &amp; d_{2} \\\\ &amp; &amp; \\ddots \\\\ &amp; &amp; &amp; d_n \\end{bmatrix} \\begin{bmatrix} 1 &amp; u_{12} / d_1 &amp; \\cdots &amp; u_{1n}/d_1 \\\\ &amp; 1 &amp; \\cdots &amp; u_{2n}/d_2 \\\\ &amp; &amp; \\ddots &amp; \\vdots \\\\ &amp; &amp; &amp; 1 \\end{bmatrix} \\] Then the \\(LU\\) factorization becomes \\(A = LDU\\), where \\(L\\) and \\(U\\) have ones on its diagonal and \\(D\\) is the diagonal matrix of pivots. In cases where row exchanges are needed to produce the echelon form, we can express the elimination process in terms of a permutation matrix \\(P\\) and the other set of row addition operations, defined by \\(L_1, ..., L_{k}\\) \\[ PL_{k}\\cdots L_1A = U \\] Note that permutation matrix \\(P\\) satisfies \\(PP^T = I\\). Multiplying both sides with \\(P^T\\) and the inverses of \\(L_1, ..., L_{k}\\), we obtain \\[ A = \\underbrace{ L_1^{-1} \\cdots L_k^{-1}}_{L}P^TU \\] One can try to polish this form by obtain a decomposition in which the permutation matrix occurs before the lower triangular matrix \\(L\\) (\\(L\\) and \\(P\\) are not the same after reordering) \\[ A = P^TLU \\] It’s also common to write this decomposition as \\(PA = LU\\). 1.4 Determinants 1.4.1 Cofactor expansion The \\((i, j)\\text{-cofactor}\\) of \\(A\\) is a number \\(C_{ij}\\) in \\(\\mathbb{R}\\) given by \\[ C_{ij} = (-1)^{i + j} \\det A_{ij} \\] where \\(A_{ij}\\) denotes the submatrix formed by deleting the \\(i\\)h row and \\(j\\)th column of \\(A\\). Theorem 1.3 (cofactor expansion) The determinant of an \\(n \\times n\\) matrix is given by a cofactor expasion across any row or column. For example, the expansion across the \\(i\\)th row is: \\[ \\det A = a_{i1}C_{i1} + a_{i2}C_{i2} + \\cdots + a_{in}C_{in} \\] and cross the \\(j\\)th column is \\[ \\det A = a_{1j}C_{1j} + a_{2j}C_{2j} + \\cdots + a_{nj}C_{nj} \\] 1.4.2 Geometric interpretation of determinant Given matrix \\(A_{n \\times n}\\) \\[ \\begin{bmatrix} a_1^{T} \\\\ a_2^{T} \\\\ \\vdots \\\\ a_n^{T} \\end{bmatrix} \\] where \\(a_1, ..., a_n\\) are row vectors of A. Then \\(|\\det A|\\) is the volume of parallelotope constrained by \\(a_1, ..., a_n\\). When \\(A\\) is \\(2\\times2\\), \\(|\\det A|\\) is simply the area of the parallelogram defined by two side \\(a_1, a_2\\) 1.4.3 Properties of determinant A list of arithmetic properties of determinants, A is an \\(n\\times n\\) matrix: \\(\\det(A^T) = \\det(A)\\) \\(\\det(kA) = k^n \\det(A)\\) \\(\\det(AB) = \\det(A)\\det(B)\\) (although \\(AB \\not = BA\\) in general), it follows that \\(\\det(A^n) = \\det(A)^n\\) \\(\\det(A^{-1}) = 1 / \\det(A)\\), if \\(A\\) is invertible determinant is equal to the product of eigenvalues(counting multiplicity) \\(\\det(A) = \\prod_{i=1}^n{\\lambda_i}\\) If the \\(i\\)-th row (column) in A is a sum of the \\(i\\)-th row (column) of a matrix \\(B\\) and the \\(i\\)-th row (column) of a matrix \\(C\\) and all other rows in \\(B\\) and \\(C\\) are equal to the corresponding rows in \\(A\\) (that is \\(B\\) and \\(C\\) differ from \\(A\\) by one row only), then \\(\\det(A)=\\det(B)+\\det(C)\\). This can be proven by cofactor expansion across the \\(i\\)th row. The same applies to columns. Row operations on \\(A\\) has the following effect on \\(\\det A\\) if we multiply a single row in \\(A\\) by a scalar \\(k \\in \\mathbb{R}\\), then the determinant of the new matrix is \\(k\\det A\\) if we exchange two rows \\(a_i^T\\) and \\(a_j^T\\) of \\(A\\), determinant becomes \\(-\\det A\\) Add a multiple of one row to another row has no effect on determinant The first two effects can be easily understood in connection with geometric meaning of determinant. As for the third one, let us represent A with row vectors \\[ A = \\begin{vmatrix} a_1^T \\\\ \\vdots \\\\ a_i^T \\\\ \\vdots \\\\ a_j^T \\\\ \\vdots \\\\ a_n^T \\end{vmatrix} \\] Then \\(B\\), after performing replacing (add a multiple of the \\(j\\)th row to the \\(i\\)th row) on \\(A\\), becomes \\[ B = \\begin{vmatrix} a_1^T \\\\ \\vdots \\\\ a_i^T + ka_j^T \\\\ \\vdots \\\\ a_j^T \\\\ \\vdots \\\\ a_n^T \\end{vmatrix} \\] By property 6 \\(\\det(A) = \\det(B) + \\det(C)\\) when \\(B\\) and \\(C\\) only differs from \\(A\\) by the same row. So \\(\\det A\\) can be broke down into two parts \\[ |A| = \\begin{vmatrix} a_1^T \\\\ \\vdots \\\\ a_i^T \\\\ \\vdots \\\\ a_j^T \\\\ \\vdots \\\\ a_n^T \\end{vmatrix} + \\begin{vmatrix} a_1^T \\\\ \\vdots \\\\ ka_j^T \\\\ \\vdots \\\\ a_j^T \\\\ \\vdots \\\\ a_n^T \\end{vmatrix} \\] The second matrix on the right side has determinant \\(0\\), and \\(\\det A\\) stays the same after replacing. Note that all row operations don’t change whether or not a determinant is 0, only change it by a non-zero factor or change its sign. 1.4.4 Cramer’s rule Given an \\(n \\times n\\) matrix \\(A\\) and \\(\\bar{b}\\) in \\(\\mathbb{\\mathbb{R^n}}\\), denote \\(A_i(\\bar{b})\\) as the matrix derived by \\(A\\) by replacing column \\(i\\) by vector \\(\\bar{b}\\): \\[ A_i(\\bar{b}) = [\\bar{a}_1 \\cdots \\underbrace{\\bar{b}}_{\\text{column} \\,i} \\cdots \\bar{a}_n] \\] Theorem 1.4 (Cramer’s rule) Let \\(A\\) be an invertible \\(n \\times n\\) matrix. For any \\(\\bar{b}\\) in \\(\\mathbb{R^n}\\), the unique solution \\(\\bar{x}\\) of \\(A\\bar{x} = \\bar{b}\\) has entries given by: \\[ x_i = \\frac{\\det A_i(\\bar{b})}{\\det A} \\] 1.5 Trace The trace of square matrix \\(A\\) is the sum of its diagonal entries \\(\\sum_{i = 1}^{n}A_{ii}\\). The trace has the following properties: \\(\\text{tr}(A + B) = \\text{tr}A + \\text{tr}B\\) \\(\\text{tr}(kA) = k\\text{tr}A\\), \\(k\\) is a scalar \\(\\text{tr}(A^T) = \\text{tr}(A)\\) For \\(A\\), \\(B\\) such that \\(AB\\) is square, \\(\\text{tr}(AB) = \\text{tr}(BA)\\) Trace of product of multiple matrices is invariant to cyclic permutations, \\(\\text{tr}(ABC) = \\text{tr}(BCA) = \\text{tr}(CAB)\\). Note that the reordering cannot be done arbitrarily, fro example \\(\\text{tr}(ABC) \\not= \\text{tr}(ACG)\\) in general. Trace is equal to the sum of its eigenvalues (repeated according to multiplicity) \\(\\text{tr}(A) = \\sum_{i = 1}^n{\\lambda_i}\\) \\(\\text{tr}(\\bar{a}\\bar{a}^T) = \\bar{a}^T\\bar{a}\\) 1.6 Matrix inversion Note that the inverse of a matrix is only defined for square matrces, so is determinants in Section 1.4. In practice \\(A^{-1}\\) is seldom computed, because computing both \\(A^{-1}\\) and \\(A^{-1}\\bar{b}\\) to solve linear equations takes about 3 times as many arithmetic operations as solving \\(A\\bar{x} = \\bar{b}\\) by row reduction. Assume that \\(A\\) and \\(B\\) are both non-singular Theorem 1.5 If A and B are both invertible matrces, we have \\[ (A^{-1})^{-1} = A \\] \\[ (AB)^{-1} = B^{-1}A^{-1} \\] \\[ (A^T)^{-1} = (A^{-1})^T \\] In 1.2, we know an algorithm of finding inverse matrices by row reducions on the augmented matrix \\([A \\;\\; I]\\). However, Cramer’s rule 1.4 leads to a general formula of calculating \\(A^{-1}\\), if it exists. The \\(j\\)th column of \\(A^{-1}\\) is a vector \\(\\bar{x}\\) that satisfies: \\[ A\\bar{x} = \\bar{e}_j \\] By Cramer’s rule \\[ \\{(i,j) \\text{ entry of } A^{-1}\\} = x_i = \\frac{\\det A_i{(\\bar{e}_j)}}{\\det A} \\] A cofactor expansion 1.3 down column \\(i\\) of \\(A_i{(\\bar{e}_j)}\\) shows that: \\[ \\det A_i{(\\bar{e}_j)} = (-1)^{i + j}\\det A_{ji} = C_{ji} \\] where \\(C_{ji}\\) is a cofactor of \\(A\\). Note that the (\\(i\\), \\(j\\))th entry of \\(A^{-1}\\) is the cofactor \\(C_{ji}\\) divided by \\(\\det A\\) (the subscript is reversed). Thus \\[\\begin{equation} \\tag{1.1} A^{-1} = \\frac{1}{\\det A} \\begin{bmatrix} C_{11} &amp; C_{21} &amp; \\cdots &amp; C_{n1} \\\\ C_{12} &amp; C_{22} &amp; \\cdots &amp; C_{n2} \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ C_{1n} &amp; C_{2n} &amp; \\cdots &amp; C_{nn} \\\\ \\end{bmatrix} \\end{equation}\\] The right side of Eq (1.1) is called the adjugate of \\(A\\), often denoted by \\(\\text{adj}\\, A\\). Theorem 1.6 (An Inverse Formula) Let \\(A\\) be an invertible \\(n \\times n\\) matrix. Then \\[ A^{-1} = \\frac{1}{\\det A}\\text{adj}\\, A \\] 1.6.1 The matrix inversion lemma Lemma 1.1 (The Matrix Inversion Lemma) Let \\(A\\) be an invertible \\(n \\times n\\) square matrix, and \\(\\bar{v}\\) and \\(\\bar{u}\\) be d-dimensional vectors. Then, \\(A + \\bar{u}\\bar{v}^T\\) is invertible if and only if \\(\\bar{v}^TA\\bar{u} \\not = -1\\). In such a case, the inverse formula is given by \\[ (A + \\bar{u}\\bar{v}^T) = A^{-1} - \\frac{A^{-1}\\bar{u}\\bar{v}^TA^{-1}}{1 + \\bar{v}^TA^{-1}\\bar{u}} \\] Theorem 1.7 (Woodbury Identity) Let \\(A\\) be an invertible \\(n \\times n\\) square matrix, and \\(U, V\\) nonzero \\(n \\times k\\) matrices for some small values of \\(k\\). Then, \\(A + UV^T\\) is invertible if and only if the \\(k \\times k\\) amtrix \\((I +V^TA^{-1}U)\\) is invertible. In such a case, the inverse formula is given by \\[ (A + UV)^{-1} = A^{-1} - A^{-1}U(I + V^TA^{-1}U)^{-1}V^TA^{-1} \\] It’s easy to find that the Woodbury Identity is an extension of the matrix inversion lemma, where \\(\\bar{u} \\rightarrow U\\) and \\(\\bar{v} \\rightarrow V\\). Proposition 1.1 For square matrix \\(P\\) (not assuming invertible), if \\(I + P\\) is invertible, then \\((I + P)^{-1}\\) satisfies: \\[ (I + P)^{-1} = I - (I + P)^{-1}P \\] As a quick check, right and left multiply \\((I + P)\\) to see if we get an identity. \\[ \\begin{split} (I + P)^{-1}(I + P) &amp;= I \\\\ &amp;= \\Big(I - (I + P)^{-1}P \\Big )(I + P) \\\\ &amp;= I + P -(1 + P)^{-1}P(I + P) \\\\ &amp;= (I + P)( I - (I + P)^{-1}P) \\\\ &amp;= (I + P)(I + P)^{-1}\\\\ &amp;= I \\end{split} \\] and left multiply \\[ \\begin{split} (I + P)(I + P)^{-1} &amp;= I \\\\ &amp;= (I + P)(I - (I + P)^{-1}P) \\\\ &amp;= I + P - P\\\\ &amp;= I \\end{split} \\] Proposition 1.2 (Push Through Identity) Let \\(U\\) and \\(V\\) be \\(m \\times n\\) matrices, we have \\[ U^T(I + VU^T)^{-1} = (I + U^TV)^{-1}U^T \\] The above result shows the following for any \\(m \\times n\\) matrix \\(A\\) and any scalar \\(\\lambda &gt; 0\\) \\[ A^T(\\lambda I + AA^T)^{-1} = (\\lambda I + A^TA)^{-1}A^T \\] 1.7 Matrix multiplication as linear transformation Another way to look at \\(A_{m \\times n} \\, x _{ n \\times 1} = b_{m \\times 1}\\), besides linear combination of column vectors, is to think of the matrix \\(A\\) as an force that “acts” on a vector \\(x\\) in \\(\\mathbb{R^n}\\) by multiplication to produce a new vector called \\(b\\) in \\(\\mathbb{\\mathbb{R^m}}\\). A transformation \\(T\\) from \\(\\mathbb{R^n}\\) to \\(\\mathbb{R^m}\\) is a rule that assigns each vector {x} in \\(\\mathbb{R^n}\\) a vector \\(T(x)\\) in \\(\\mathbb{R^m}\\), which is called the image of {x} (under the action of \\(T\\)). It can be show that such transformations induced by multiplying a matrix is a type of linear transformation, because it satisfies all required properties to be linear: \\[ \\begin{aligned} \\text{vector addition} \\quad A(\\bar{u} + \\bar{v}) &amp;= A\\bar{u} + A\\bar{v} \\\\ \\text{scalar multiplication} \\quad A(c\\bar{u}) &amp;= cA\\bar{u} \\end{aligned} \\] Theorem 1.8 (left multiplication as linear transformation) There is a one to one relationship between a linear transformation and a matrix. Let \\(T: \\mathbb{R^n} \\rightarrow \\mathbb{R^m}\\) be a linear transformation. Then there exists a unique matrix \\(A\\) such that: \\[ T(x) = Ax \\quad \\text{for all} \\; x \\; \\text{in} \\; \\mathbb{R^n} \\] In fact, \\(A\\) is a \\(m \\times n\\) matrix whose \\(j\\)th column is the vector \\(T(\\bar{e_j})\\), where \\(\\bar{e_j}\\) is the \\(j\\)th basis of \\(\\mathbb{R^n}\\) Proof: \\[ \\bar{x} = x_1\\bar{e_1} + \\dots + x_n{\\bar{e_n}} \\] And because \\(T(\\bar{x})\\) is a linear transformation: \\[ \\begin{split} T(\\bar{x}) &amp;= x_1T(\\bar{e_1}) + \\dots + x_nT(\\bar{e_n}) \\\\ &amp;= [T(\\bar{e_1}) \\, \\cdots \\, T(\\bar{e_n})]\\bar{x} \\\\ &amp;= (A\\bar{x})_{m \\times 1} \\end{split} \\] In other words, the transformation is specified once we know what all basis in \\(\\mathbb{R^n}\\) become in \\(\\mathbb{R^m}\\). The matrix \\(A\\) is called the standard matrix for the linear transformation \\(T\\). Definition 1.2 (A mapping is onto \\(\\mathbb{R^m}\\)) A mapping \\(T: \\mathbb{R^n} \\rightarrow \\mathbb{R^m}\\) is said to be onto if each \\(\\bar{b}\\) in \\(\\mathbb{R^m}\\) is the image of at least one \\(\\bar{x}\\) in Equivalently, \\(T\\) is onto \\(\\mathbb{R^m}\\) means that there exists at least one solution of \\(T(\\bar{x}) = \\bar{b}\\) Definition 1.3 (one-to-one mapping) A mapping T: \\(\\mathbb{R^n} \\rightarrow \\mathbb{R^m}\\) is said to be one-to-one if each \\(\\bar{b}\\) in \\(\\mathbb{R^m}\\) is the image of at most one \\(\\bar{x}\\) in \\(\\mathbb{R^n}\\) Equivalently, \\(T\\) is one-to-one if, for each \\(\\bar{b}\\) in \\(\\mathbb{R^m}\\), the equation \\(T(\\bar{x}) = \\bar{b}\\) has either a unique solution or none at all. 1.7.1 Matrix multiplication as geometric operators \\[ \\begin{bmatrix} \\cos\\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{bmatrix} \\begin{bmatrix} \\rho \\cos\\alpha \\\\ \\rho \\sin\\alpha \\end{bmatrix} = \\begin{bmatrix} \\rho(\\cos\\theta\\cos\\alpha - \\sin\\theta\\sin\\alpha) \\\\ \\rho(\\sin\\theta \\cos\\alpha + \\cos \\theta \\sin\\alpha) \\end{bmatrix} = \\begin{bmatrix} \\rho \\cos(\\theta + \\alpha) \\\\ \\rho \\sin(\\theta + \\alpha) \\end{bmatrix} \\] 1.8 Statistics and proabability 1.8.1 Sample statistics In particular, a non-singular matrix with \\(a_{11} = 0\\) cannot have LU decomposition↩︎ square matrices whose diagonal entries are those of the original matrix↩︎ "],
["vector-spaces.html", "Chapter 2 Vector spaces 2.1 Vector space 2.2 Metric spaces, normed spaces, inner product spaces 2.3 Subspaces 2.4 Fundamental theorem of linear algebra 2.5 Rank", " Chapter 2 Vector spaces Vector spaces, metric spaces, normed spaces, and inner product spaces are places where computations in linear algebra happen. These spaces are defined more or less to generalize properties of Euclidean space. 2.1 Vector space A vector space \\(V\\) is a nonempty set, the elements of which are called vectors, also called linear spaces. A vector space comes with two operations predefined: vectors can be added together, and vectors can be multiplied by real numbers called scalars. For all vectors \\(\\bar{u}, \\bar{v}\\) and \\(\\bar{w}\\), and all scalars \\(c\\) and \\(d\\) in \\(V\\), the following axioms of vector space must hold: The sum of \\(\\bar{u}\\) and \\(\\bar{v}\\) is in \\(V\\) The scalar multiple of \\(\\bar{u}\\) by c, denoted by \\(c\\bar{u}\\), is in \\(V\\) There exists additive identity (denoted by \\(\\bar{0}\\)) such that \\(\\bar{x} + \\bar{0} = \\bar{x}\\). Similarly, multiplicative identity (written \\(\\bar{1}\\)) means \\(\\bar{1}\\bar{x} = \\bar{x}\\) There exists an additive inverse (written \\(-\\bar{x}\\)) such that \\(-\\bar{x} + \\bar{x} = \\bar{0}\\) Communitivity: \\(\\bar{x} + \\bar{y} = \\bar{y} + \\bar{x}\\) Associativity: \\((\\bar{x} + \\bar{y}) + \\bar{z} = \\bar{x} + (\\bar{y} + \\bar{z})\\), and \\(\\alpha(\\beta\\bar{x}) = (\\alpha\\beta)\\bar{x}\\) Distributivity: \\(\\alpha(\\bar{x} + \\bar{y}) = \\alpha\\bar{x} + \\alpha\\bar{y}\\) and \\((\\alpha + \\beta)\\bar{x} = \\alpha\\bar{x} + \\beta\\bar{x}\\) A set of vectors \\(\\bar{v}_1, ..., \\bar{v}_n \\in V\\) are set to be linearly independent if \\[ \\begin{align*} c_1\\bar{v}_1 + \\cdots + c_n\\bar{v}_n &amp;= 0 &amp;&amp; \\text{implies } c_1 = \\cdots = c_n \\end{align*} \\] The span of \\(\\bar{v}_1, ..., \\bar{v}_n\\) is the set of all vectors that can be expressed as a linear combination of them. 2.1.1 Euclidean space Euclidean space is the quintessential vector space, denoted by \\(\\mathbb{R}^n\\). The two must-have operations of vector spaces are valid in \\(\\mathbb{R}^n\\) \\[ \\bar{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}, \\;\\; \\bar{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, \\;\\; \\alpha \\in \\mathbb{R} \\\\ \\bar{x} + \\bar{y} = \\begin{bmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{bmatrix}, \\;\\; \\alpha\\bar{x} = \\begin{bmatrix} \\alpha x_1 \\\\ \\vdots \\\\ \\alpha x_n \\end{bmatrix} \\] Euclidean spaces have other structures defined in addition to the plainest vector space. We can calculate dot product, length, distance by \\[ \\begin{aligned} \\text{dot product between } \\bar{x} \\text{ and } \\bar{y}: \\bar{x} \\cdot\\bar{y} &amp;= \\sum_{i=1}^{n}{x_iy_i} \\\\ \\text{length of } \\bar{x}: \\|\\bar{x}\\| &amp;= \\sqrt{\\bar{x} \\cdot \\bar{x}} \\\\ \\text{distance between } \\bar{x} \\text{ and } \\bar{y}: \\text{dist}(\\bar{x}, \\bar{y}) &amp;= \\|\\bar{x} - \\bar{y}\\| \\end{aligned} \\] 2.2 Metric spaces, normed spaces, inner product spaces Metric spaces, normed spaces and inner product spaces capture important properties of Euclidean space in a more general way (distance, length, angle). Although metric spaces are not required to be vector spaces, it is always assumed in linear algebra that this is the case. For this reason, metric spaces are short for “metric linear space”. Normed spaces and inner product spaces are defined to be extensions of metric linear spaces, so that they must be vector spaces. Metrics generalize the notion of distance from Euclidean space. A metric space is a set together with a metric on the set (metric spaces don’t have to be vector spaces). The metric is a function that defines a concept of distance \\(\\in \\mathbb{R}\\) between any two members of the set. A metric must satisfies the following properties: \\(d(\\bar{x}, \\bar{y}) \\ge 0\\), with equality if and only if \\(\\bar{x} = \\bar{y}\\). Distances are non-negative, and the only point at zero distance from \\(x\\) is \\(x\\) itself \\(d(\\bar{x}, \\bar{y}) = d(\\bar{y}, \\bar{x})\\). The distance is a symmetric function. \\(d(\\bar{x}, \\bar{z}) \\le d(\\bar{x}, \\bar{y}) + d(\\bar{y}, \\bar{z})\\). Distance satisfies triangular inequality. Norms generalize the notion of length from Euclidean space. A norm on a real vector space \\(X\\) is a function: \\(\\|\\cdot\\|: V \\rightarrow \\mathbb{R}\\) that satisfies: \\(\\|\\bar{x}\\| \\ge 0\\) for all \\(\\bar{x} \\in X\\), with equality if and only if \\(\\bar{x} = \\bar{0}\\) (nonnegative) \\(\\|\\lambda \\bar{x}\\| = \\lambda \\|\\bar{x}\\|\\), for all \\(\\bar{x} \\in X\\) and \\(\\lambda \\in \\mathbb{R}\\) (homogeneous) \\(\\|\\bar{x} + \\bar{y}\\| \\le \\|\\bar{x}\\| + \\|\\bar{y}\\|\\) (triangular inequality) A normed space is a metric space with the metric \\[ d(\\bar{x}, \\bar{y}) = \\|\\bar{x} - \\bar{y}\\| \\] So a normed space is a special case of metric spaces, a metric spcae may not necessarily has a norm associated with it. One can verify that \\(d(\\bar{x}, \\bar{y}) = \\|\\bar{x} - \\bar{y}\\|\\) satisfies all properties of a metric. The most common function for norms on \\(\\mathbb{R}^n\\) are listed below, with \\(\\bar{x} = [x_1, x_2, ..., x_n]\\). \\[ \\begin{align*} \\text{1-norm}: \\|\\bar{x}\\|_1 &amp;= \\sum_{i=1}^{n}{|x_i|}\\\\ \\text{2-norm}: \\|\\bar{x}\\|_2 &amp;= \\sqrt{\\sum_{i=1}^{n}{x_i}^2} \\\\ \\text{p-norm}: \\|\\bar{x}\\|_p &amp;= (\\sum_{i=1}^{n}{|x_i|}^p)^{\\frac{1}{p}} \\quad (p \\ge 1) \\\\ \\text{maximum norm}: \\|\\bar{x}\\|_{\\infty} &amp;= \\max\\{|x_1|, |x_2|, ..., |x_n|\\} \\end{align*} \\] 1-norm is also called the Manhattan norm. 2-norm is the Euclidean norm, the subscript \\(2\\) can be left out in \\(\\mathbb{R}^n\\). p-norm is a generalization of 1-norm and 2-norm, requiring \\(p &gt; 1\\). When \\(p\\) turns infinity, \\(\\|\\bar{x}\\|_{\\infty}\\) is called the maximum norm. An inner product on a real vector space \\(X\\) is a function \\(\\langle \\cdot, \\cdot\\rangle: X \\times X \\rightarrow \\mathbb{R}\\) satisfying \\(\\langle \\bar{x}, \\bar{y} \\rangle \\ge 0\\), with equality if and only if \\(x = \\bar{0}\\) \\(\\langle \\bar{x}, \\bar{y} \\rangle = \\langle \\bar{y}, \\bar{x} \\rangle\\) \\(\\langle \\bar{x} + \\bar{y}, \\bar{z}\\rangle = \\langle \\bar{x}, \\bar{z}\\rangle + \\langle \\bar{y}, \\bar{z}\\rangle\\) and \\(\\langle \\lambda \\bar{x}, \\bar{y}\\rangle = \\lambda \\langle \\bar{x}, \\bar{y} \\rangle\\) A vector sapce equipped with such inner product is called a inner product space. Note that all inner product spaces are normed spaces, because a inner product induce a norm on a vector space: \\[ \\langle \\bar{x}, \\bar{x} \\rangle = \\|\\bar{x}\\|^2 \\] The standard inner product defined on \\(\\mathbb{R}^{n}\\) is the dot product, given by \\[ \\langle \\bar{x}, \\bar{y} \\rangle = \\sum_{i=1}^{n}{x_iy_i} = \\bar{x}^T\\bar{y} \\] The abstract spaces—metric spaces, normed spaces, and inner product spaces—are all examples of what are more generally called “topological spaces” (linear topological space if they are assumed to be vector spaces first). These spaces have been given in order of increasing structure. That is, every inner product space is a normed space, and in turn, every normed space is a metric space. 2.2.1 Restricted definition of inner products in \\(R^n\\) Sometimes it suffice only to generalize the dot product, with the definition of inner product, in Euclidean space \\(\\mathbb{R}^n\\) instead of other inner product spaces. For example, many engineering applications measure similarity between vectors using the dot product after stretching the two vectors in some directions, with linear transformation \\(A\\). Therefore, we can given a restricted definition of inner product that is meant to be used in \\(\\mathbb{R}^n\\). Definition 2.1 (Restricted definition of inner product) The generalized dot product \\(\\langle \\bar{x}, \\bar{y}\\rangle\\) in \\(\\mathbb{R}^n\\) between two vectors, is the dot product between \\(A\\bar{x}\\) and \\(A\\bar{y}\\), for some \\(n \\times n\\) non-singular matrix \\(A\\). The inner product \\(\\langle \\bar{x}, \\bar{y}\\rangle\\) can also be expressed using the Gram matrix \\(S = A^TA\\) \\[ \\langle \\bar{x}, \\bar{y}\\rangle = (A\\bar{x})^T(A\\bar{y}) = \\bar{x}A^TA\\bar{y} = \\bar{x}^TS\\bar{y} \\] It’s easy to see that when \\(S\\) is the identity matrix, the inner product is the dot product. This definition of inner product also induces consines and distances with respect to transformation \\(A\\) \\[ \\cos_A{(\\bar{x}, \\bar{y})} = \\frac{\\langle \\bar{x}, \\bar{y}\\rangle}{\\sqrt{\\langle \\bar{x}, \\bar{x}\\rangle}\\sqrt{\\langle \\bar{y}, \\bar{y}\\rangle}} = \\frac{\\bar{x}S\\bar{y}}{\\sqrt{\\bar{x}S\\bar{x}}\\sqrt{\\bar{y}S\\bar{y}}} = \\frac{(A\\bar{x})^T(A\\bar{y})}{\\|A\\bar{x}\\|_2\\|A\\bar{y}\\|_2} \\\\ \\text{dist}(\\bar{x}, \\bar{y}) = \\sqrt{\\langle \\bar{x} - \\bar{y}, \\bar{x} - \\bar{y} \\rangle} = \\sqrt{(\\bar{x} - \\bar{y})^TS(\\bar{x} - \\bar{y})} = \\|A\\bar{x} - A\\bar{y} \\|_2 \\] 2.3 Subspaces If \\(V\\) is a subspace, then \\(S \\subseteq A\\) if \\(\\bar{0} \\in S\\) \\(S\\) is closed under addition: if \\(\\bar{x}, \\bar{y} \\in S\\), then \\(\\bar{x} + \\bar{y} \\in S\\) \\(S\\) is closed under scalar multiplication if \\(\\bar{x} \\in S, \\alpha \\in \\mathbb{R}\\) then \\(\\alpha\\bar{x} \\in S\\) \\(V\\) is always a subspace of itself. If \\(U\\) and \\(W\\) are all subspaces of \\(V\\), then the sum of these two subspaces are defined as \\[ U + W = \\{\\bar{u} + \\bar{v} \\;| \\; \\bar{w} \\in U, \\bar{w} \\in W \\} \\] If \\(U\\) and \\(W\\) are perpendicular, in other words, \\(U \\cap W = \\bar{0}\\). Then \\(U + W\\) are said to be a direct sum and written \\(U \\oplus W\\). Dimensions of sums of subspaces has the following property \\[ \\text{dim}(U + W) = \\text{dim}(U) + \\text{dim}(W) - \\text{dim}(U \\cap W) \\] It follows that if \\(U\\) is perpendicular to \\(W\\), \\(W\\), \\[ \\text{dim}(U \\oplus W) = \\text{dim}(V) = \\text{dim}(U) + \\text{dim}(W) \\] 2.4 Fundamental theorem of linear algebra The columnspace (also called range) of matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) is the span of the columns of \\(A\\) \\[ \\mathcal{R}(A) = \\{\\bar{v} \\in \\mathbb{R}^m\\;|\\; \\bar{v} = A\\bar{x}, \\, \\bar{x} \\in \\mathbb{R}^n\\} \\] Similarly, the rowspace of \\(A\\) is the span of its rows \\(\\mathcal{R}(A^T)\\). The nullspace (also called kernel) of \\(A\\) is the set of solutions to \\(A\\bar{v} = \\bar{0}\\) \\[ \\mathcal{N}(A) = \\{\\bar{x} \\in \\mathbb{R}^n\\;|\\;A\\bar{x} = \\bar{0}\\} \\] And the left null space is all \\(\\bar{x}\\) that satisfies \\(A^T\\bar{x} = \\bar{0}\\). The word “left” in this context stems from the fact that \\(A^T\\bar{x}= \\bar{0}\\) is equivalent to \\(\\bar{x}^TA=\\bar{0}\\) where y “acts” on A from the left. The relationship between these four subspaces present the fundamental theorem of linear algebra Theorem 2.1 (The fundamental theorem of Linear Algebra) \\(\\mathcal{R}(A) = \\mathcal{N}(A^T)^{\\perp}\\), and \\(\\dim(\\mathcal{R}(A)) + \\dim(\\mathcal{N}(A^T)) = m\\) \\(\\mathcal{R}(A^T) = \\mathcal{N}(A)^{\\perp}\\), and \\(\\dim(\\mathcal{R}(A^T)) + \\dim(\\mathcal{N}(A)) = n\\) If the rank (defined next section) of \\(A \\in \\mathbb{R}^{m \\times n}\\) is \\(r\\) \\(m - r\\) is the dimension of the left null space of \\(A\\) \\(n - r\\) is the dimension of the null space of \\(A\\) 2.5 Rank The rank of a vector space is its dimension. Definition 1.1 The rank of a matrix is equal to the rank of its column space, which is the same as the rank of its column space. I often think of rank of \\(A\\) as the total volume of information that the matrix can offer. From the definition of matrix rank, we know that \\(A\\)’s row rank (the dimension of \\(\\mathcal{R}(A^T)\\))equals its column rank \\(\\mathcal{R}(A)\\). A way to verify this are presented below. All matrices can be reduced into a (possibly rectangular) diagonal matrix with elementary row and column operations. First we can row reduce the matrix into row echelon form, then use column operations to convert positions above the pivot to zero. Thus, any \\(A \\in \\mathbb{R}^{m \\times n}\\) can be expressed in the following form \\[ RAC = \\Lambda \\tag{1} \\] where \\(R\\) is the product of the elementary matrices that perform row opertations, and \\(C\\) for column operations. Since \\(C\\) is invertible, we can write \\[ RA = \\Lambda C^{-1} \\tag{2} \\] From (1) we know that row rank of \\(A\\) is identical to that the number of non-zero entries in \\(\\Lambda\\), on the ground that row operations on \\(A\\) does not change its row rank, and \\(C^{-1}\\) only scale diagonal entries of \\(\\Lambda\\) to some multiple. Similarly, \\(AC = R^{-1}\\Lambda\\) shows that column rank of \\(A\\) is the same as the number of non-zero diagonal entries in \\(\\Lambda\\). Therefore, row rank \\(=\\) column rank. 2.5.1 Effect of operations on matrix rank Let \\(A, B \\in \\mathbb{R}^{m \\times n}\\) have ranks \\(a\\) and \\(b\\) Corollary 2.1 \\(|a - b| \\le r(A + B) \\le a + b\\) \\(r(AB) &lt; \\min(a, b)\\) PROOF For (1), rows / columns of \\(A + B\\) can be expressed as linear combinations of rows / columns of \\(A\\) and \\(B\\). For (2), Each column of \\(AB\\) is a linear combination of columns of A, and each row is a linear combination of rows of \\(B\\). Therefore, \\(r(AB)\\) can not exceed either rank of \\(A\\) or \\(B\\). There is also a corollary on the lower bound of \\(r(AB)\\), which is \\(a + b - n\\). Note that \\(n\\) is the shared dimension. I have not found a concise proof about this, but this property leads to a interesting result: when one of \\(A\\) and \\(B\\) are square and full rank, \\(\\min(r(AB)) = \\max(r(AB))\\) Corollary 2.2 Multiplying \\(A\\) with a square matrix \\(B\\) of full rank does not change the rank of \\(A\\). If \\(A\\) and \\(B\\) are both singular, then \\(AB\\) is non-singular if and only if \\(A\\) and \\(B\\) are both non-singular PROOF Suppose \\(B\\) is \\(n \\times n\\), the minimum rank of \\(AB\\) is \\(a + n - n = a\\), and the maximum rank is \\(\\min(a, n) = a\\). Thus, multiplying by a full rank matrix preserves rank: \\(r(AB) = r(A)\\). (4) follows naturally after (3). 2.5.2 Gram matrix Proposition 2.1 (Gram matrix) The matrix \\(A^TA\\) is said to be the Gram matrix of column space of \\(A_{m \\times n}\\). The columns of \\(A\\) are linearly independent if and only if \\(A^TA\\) is invertible. PROOF When \\(A^TA\\) is invertible, it has rank \\(n\\). Therefore, each of the factors of \\(A^TA\\) has at least rank \\(n\\), and this means columns of \\(A\\) are linearly independent (since \\(r(A) \\le \\min(m, n)\\)). Similarly, \\(AA^T\\) are called the left Gram matrix of rowspace of \\(A\\). And \\(AA^T\\) is invertible if and only rows of \\(A\\) are linearly independent. Proposition 2.2 For any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(A\\), \\(A^TA\\) and \\(AA^T\\) always have the same rank. \\[ r(A) = r(A^TA) = r(AA^T) \\] PROOF For \\(r(A) = r(A^TA)\\), suppose \\(r(A) = r\\), then \\(\\dim(\\mathcal{N}(A)) = n - \\dim(\\mathcal{R}(A^T)) = n - r\\). Note that for any \\(\\bar{x}\\) that satisfies \\(A\\bar{x} = 0\\), we have \\(A^TA\\bar{x} = 0\\). It follows that \\(A\\) and \\(A^TA\\) have the same null space, \\(\\mathcal{N}(A) = \\mathcal{N}(A^TA)\\). Since \\(A^TA \\in \\mathbb{R}^{n \\times n}\\), we have \\(r(A^TA) = \\dim(\\mathcal{R}((A^TA)^T)) = n - \\dim(\\mathcal{N}(A^TA)) = r\\). For \\(r(A) = R(AA^T)\\), note that \\(r(A) = r(A^T)\\), and that \\(\\mathcal{N}(A^T) = \\mathcal{N}(AA^T)\\), then the conclusion presents itself. From the SVD perspective(Corollary 4.1 and Section 5.2), one can show that \\(A^TA\\) and \\(AA^T\\) have the same set of nonzero eigenvalues, and \\(r(A)\\) is the same as the number of nonzero eigenvalues of \\(A^A\\) or \\(AA^T\\), so that \\(r(A) = r(A^TA) = r(AA^T)\\). "],
["orthogonality.html", "Chapter 3 Orthogonality 3.1 Orthogonal decomposition 3.2 Idempotent and Projection Matrices 3.3 Gram-Schmidt process 3.4 Orthonormal sets and orthogonal matrices 3.5 Lesat squares problems", " Chapter 3 Orthogonality 3.1 Orthogonal decomposition 3.1.1 Orthogonal complements if vector \\(\\bar{v}\\) is orthogonal to every vector in a subspace \\(W\\) of \\(\\mathbb{R^n}\\), then \\(\\bar{v}\\) is said to be orthogonal to \\(W\\). The subspace that contains the set of vectors that are orthogonal to \\(W\\) is called the orthogonal complement, denoted by \\(W^{\\perp}\\). \\[ W^{\\perp} = \\{\\bar{v} \\in W^{\\perp} | \\;\\bar{v} \\perp \\bar{x} \\; \\text{for all} \\; \\bar{x} \\in W\\} \\] This corresponds to discussions in Section 2.4, where \\[ \\mathcal{R}(A^T) = \\mathcal{N}(A) \\\\ \\mathcal{R}(A) = \\mathcal{N}{(A^T)} \\] Theorem 1.1 If \\(W\\) is a subspace of \\(\\mathbb{R}^n\\), \\(W^{\\perp}\\) is also a subspace of \\(\\mathbb{R}^n\\). It’s easy to verify that \\(W^{\\perp}\\) is closed under scalar multiplication, and under vector addition, and that any vector in \\(W\\) has \\(n\\) components. So that \\(W^{\\perp}\\) is a subspace of \\(\\mathbb{R}^n\\) 3.1.2 Orthogonal sets and orthogonal basis An orthogonal set is a set of vectors \\(\\{\\bar{u_1}, \\dots, \\bar{u_p}\\}\\) in \\(\\mathbb{R^n}\\), in which each pair of distinct vectors is orthogonal: \\(\\bar{u_i}^{T} \\bar{u_j} = 0 \\quad i\\not = j\\). Note that the set do not necessarily span the whole \\(\\mathbb{R^n}\\), but a subspace \\(W\\). Since vectors in orthogonal sets is mutually perpendicular, they must also be linearly independent and could form a basis for a subspace \\(W\\). In such case, they are called orthogonal basis. There is a particular advantage in using orthogonal basis rather than other basis, because we can find a easy representation of any vector in \\(W\\). Theorem 3.1 For each \\(\\bar{y}\\) in \\(W\\), there exists a linear combination \\[ y = c_1\\bar{u_1} + \\cdots + c_p\\bar{u_p} \\] and \\[ c_i = \\frac{\\bar{y} \\cdot \\bar{u_i}}{\\bar{u_i} \\cdot \\bar{u_i}} \\quad i = 1, \\cdots, p \\] where \\(\\{\\bar{u_1}, \\dots, \\bar{u_p}\\}\\) is an orthogonal basis. Proof \\[ \\begin{split} \\bar{u_1} \\cdot \\bar{y} &amp;= \\bar{u_1} \\cdot (c_1\\bar{u_1} + \\cdots + c_p\\bar{u_p}) \\\\ &amp;= c_1 \\bar{u_1} \\cdot \\bar{u_1} \\end{split} \\] So: \\[ c_1 = \\frac{\\bar{u_1} \\cdot \\bar{y}}{\\bar{u_1} \\cdot \\bar{u_1}} \\] Derivations for other \\(c_i\\) is similar. 3.1.3 Orthogonal decomposition Orthogonal decomposition split \\(\\bar{y}\\) in \\(\\mathbb{R^n}\\) into two vectors, one in \\(W\\) and one in its orthogonal compliment \\(W^{\\perp}\\). Theorem 3.2 Let \\(\\mathbb{R}^n\\) be a inner product space and \\(W\\) and subspace of \\(\\mathbb{R}^n\\). Then every \\(\\bar{v}\\) in \\(W\\) can be written uniquely in the form \\[ \\bar{v} = \\bar{v}_w + \\bar{v}_{\\perp} \\] where \\(\\bar{v}_w \\in W\\) and \\(\\bar{v}_{\\perp} \\in W^{\\perp}\\) PROOF Let \\(\\bar{u}_1, ..., \\bar{u}_m\\) be a orthonormal basis for \\(W\\), there exists linear combination according to Section 3.1.2 \\[ \\bar{v}_w = (\\bar{v} \\cdot \\bar{u}_1)\\bar{u}_1 + \\cdots + (\\bar{v} \\cdot \\bar{u}_m)\\bar{u}_m \\] and \\[ \\bar{v}_{\\perp} = \\bar{v} - \\bar{v}_w \\] It is clear that \\(\\bar{v}_W \\in W\\). And we can also show that \\(\\bar{v}_{\\perp}\\) is perpendicular to \\(W\\) \\[ \\begin{split} \\bar{v}_{\\perp} \\cdot \\bar{u}_i &amp;= [\\bar{v}- (\\bar{v} \\cdot \\bar{u}_1)\\bar{u}_1 - \\cdots - (\\bar{v} \\cdot \\bar{u}_m)\\bar{u}_m] \\cdot \\bar{u}_i \\\\ &amp;= (\\bar{v} \\cdot \\bar{u}_1) - [(\\bar{v} \\cdot \\bar{u}_i)\\bar{u}_i \\cdot \\bar{u}_i] \\\\ &amp;= 0 \\end{split} \\] which implies \\(\\bar{v}_{\\perp} \\in W^{\\perp}\\). To prove that \\(\\bar{v}_w\\) and \\(\\bar{v}_{\\perp}\\) are unique (does not depend on the choice of basis), let \\(\\bar{u}_1&#39;, ..., \\bar{u}_m&#39;\\) be another orthonormal basis for \\(W\\), and define \\(\\bar{v}_w&#39;\\) and \\(\\bar{v}_{\\perp}&#39;\\) similarly we want to get \\(\\bar{v}_w&#39; = \\bar{v}_w\\) and \\(\\bar{v}_{\\perp}&#39; = \\bar{v}_{\\perp}\\). By definition \\[ \\bar{v}_w + \\bar{v}_{\\perp} = \\bar{v} = \\bar{v}_w&#39; + \\bar{v}_{\\perp}&#39; \\] so \\[ \\underbrace{\\bar{v}_w - \\bar{v}_w&#39;}_{\\in W} = \\underbrace{\\bar{v}_{\\perp}&#39; - \\bar{v}_{\\perp}}_{\\in W^{\\perp}} \\] From the orthogonality of these subspaces, we have \\[ 0 = (\\bar{v}_w - \\bar{v}_w&#39;) \\cdot (\\bar{v}_{\\perp}&#39; - \\bar{v}_{\\perp}) = (\\bar{v}_w - \\bar{v}_w&#39;) \\cdot (\\bar{v}_w - \\bar{v}_w&#39;) = \\|\\bar{v}_w - \\bar{v}_w&#39;\\|^2 \\] Similarly we have \\(\\|\\bar{v}_{\\perp}&#39; - \\bar{v}_{\\perp}\\|^2 = 0\\). The existence and uniqueness of the decomposition above mean that \\[ \\mathbb{R}^n = W \\oplus W^{\\perp} \\] whenever \\(W\\) is a subspace. 3.1.4 Best approximation Theorem 3.3 (The Best Approximation) Given \\(\\bar{y}\\) be any vector in \\(\\mathbb{R^n}\\), with its subspace \\(W\\), let \\(\\hat{\\bar{y}}\\) be the orthogonal projection of \\(\\bar{y}\\) onto \\(W\\). Then \\(\\hat{\\bar{y}}\\) is the closest point in \\(W\\) to \\(\\bar{y}\\) in the sense that \\[ \\|\\bar{y} - \\hat{\\bar{y}}\\| \\le \\|\\bar{y} - \\bar{v}\\| \\] PROOF Take \\(\\bar{v}\\) distinct from \\(\\hat{\\bar{y}}\\) in \\(W\\), we know that \\(\\bar{y} - \\hat{\\bar{y}}\\) is perpendicular to \\(\\bar{v}\\). According to Pythoagorean theorem, we have Figure 3.1: figure from page p352, ch6 (Lay 2006) \\[ \\|\\bar{y}- \\bar{v}\\|^2 = \\|\\bar{\\hat{y}} - \\bar{v}\\|^2 + \\|\\bar{y} -\\bar{\\hat{y}}\\|^2 \\] When \\(\\bar{v}\\) is distinct from \\(\\bar{\\hat{y}}\\), \\(\\|\\bar{\\hat{y}} - \\bar{v}\\|^2\\) is non-negative, so the error term of choosing \\(\\bar{v}\\) is always larger than that of the orthogonal projection \\(\\bar{\\hat{y}}\\). 3.2 Idempotent and Projection Matrices \\[ \\begin{split} P_S\\bar{v} &amp;= (\\bar{v} \\cdot \\bar{u}_1)\\bar{u}_1 + \\cdots + (\\bar{v} \\cdot \\bar{u}_m)\\bar{u}_m \\\\ &amp;= \\bar{v}^T\\bar{u}_1\\bar{u}_1 + \\cdots + \\bar{v}^T\\bar{u}_m\\bar{u}_m\\\\ &amp;= (\\bar{u}_1\\bar{u}_1^T)\\bar{v} + \\cdots + (\\bar{u}_m\\bar{u}_m^T)\\bar{v} \\\\ &amp;= (\\sum_{i=1}^{M}{\\bar{u}_i\\bar{u}_i^T})\\bar{v}\\\\ &amp;= \\begin{bmatrix} \\bar{u}_1 &amp; \\cdots &amp; \\bar{u}_m \\end{bmatrix} \\begin{bmatrix} \\bar{u}_1^T \\\\ \\vdots \\\\ \\bar{u}_m^T \\end{bmatrix}\\bar{x} \\\\ &amp;= UU^T\\bar{x} \\end{split} \\] In practical problems, there are times when it is more convenient to use matrix at hand rather than producing an orthonormal basis. Another way to derive projection matrices with matrix calculus \\[ \\begin{split} \\|A\\bar{x} - \\bar{b}\\|_2^2 &amp;= (A\\bar{x} - \\bar{b})^T(A\\bar{x} - \\bar{b}) \\\\ &amp;= \\bar{x}^TA^TA\\bar{x} - 2\\bar{b}^TA\\bar{x} + \\bar{b}^T\\bar{b} \\end{split} \\] $$ \\begin{split} _{x}({x}TATA{x} - 2A{x}{b} + {b}^T{b}) &amp;=_x({x}TATA{x}) - _x{2{b}^TA{x}} + _x{{b}^T{b}} \\ &amp;= 2(A^TA){x} - 2A^T{b} \\end{split} $$ \\[ \\bar{x} = (A^TA)^{-1}A^T\\bar{b} \\] \\[ A\\bar{x} = A(A^TA)^{-1}A^T\\bar{b} = \\hat{\\bar{b}} \\] 3.3 Gram-Schmidt process Then Gram-Schmidt process is a simple algorithm that transforms a set of linearly independent vectors into orthogonal or orthonormal basis for a subspace. In its essence, it is a sequential projection of \\(\\bar{x}_{i}\\) onto the space spanned by the previously created orthogonal set \\(\\{\\bar{v}_{1}, ..., \\bar{v}_{i-1}\\}\\), and take the term in the orthogonal compliment to be \\(\\bar{v}_{i}\\). Theorem 3.4 (the Gram-Schmidt process) Given a basis \\(\\{\\bar{x}_1, ..., \\bar{x}_p\\}\\) for a nonzero subspace \\(W\\) of \\(\\mathbb{R}^n\\), define \\[ \\begin{aligned} \\bar{v}_1 &amp;= \\bar{x}_1 \\\\ \\bar{v}_2 &amp;= \\bar{x}_2 - \\frac{\\bar{x}_2 \\cdot \\bar{v}_1}{\\bar{v}_1 \\cdot \\bar{v}_1}\\bar{v}_1 \\\\ \\bar{v}_3 &amp;= \\bar{x}_3 - \\frac{\\bar{x}_3 \\cdot \\bar{v}_1}{\\bar{v}_1 \\cdot \\bar{v}_1}\\bar{v}_1 - \\frac{\\bar{x}_3 \\cdot \\bar{v}_2}{\\bar{v}_2 \\cdot \\bar{v}_2}\\bar{v}_2 \\\\ &amp; \\vdots \\\\ \\bar{v}_p &amp;= \\bar{x}_p - \\frac{\\bar{x}_p \\cdot \\bar{v}_1}{\\bar{v}_1 \\cdot \\bar{v}_1}\\bar{v}_1 - \\frac{\\bar{x}_p \\cdot \\bar{v}_2}{\\bar{v}_2 \\cdot \\bar{v}_2}\\bar{v}_2 - \\cdots - \\frac{\\bar{x}_p \\cdot \\bar{v}_{p-1}}{\\bar{v}_{p-1} \\cdot \\bar{v}_{p-1}}\\bar{v}_{p-1} \\end{aligned} \\] Then \\(\\{\\bar{v}_1, ..., \\bar{v}_p\\}\\) is an orthogonal basis for \\(W\\). In addition \\[ \\text{Span}\\{\\bar{v}_1, ..., \\bar{v}_p\\} = \\text{Span}\\{\\bar{x}_1, ..., \\bar{x}_p\\} \\] To make \\(\\{\\bar{v}_1, ..., \\bar{v}_p\\}\\) an orthonormal basis, there is simply one more step of normalization \\[ \\{\\bar{q}_i = \\frac{\\bar{v}_i}{\\|\\bar{v}_i\\|}, \\;i = 1, ... p\\} \\] 3.3.1 QR factorizaiton For \\(A \\in \\mathbb{R}^{m \\times n}\\) with linearly independent columns \\(\\bar{x}_1, ..., \\bar{x}_n\\), apply the Gram-Schmidt process to \\(\\bar{x}_1, ..., \\bar{x}_n\\) amounts to factorizing \\(A\\). Theorem 3.5 (QR factorization) if \\(A\\) is an \\(m \\times n\\) matrix with full column rank, then \\(A\\) can be factored as \\(A = QR\\), where \\(Q\\) is an \\(m \\times n\\) matrix whose columns form an orthonormal basis of \\(\\text{Col}\\;A\\) and \\(R\\) is an \\(n \\times n\\) upper triangular invertible matrix with positive entries on its diagonal. PROOF Because \\(A_{m \\times n}\\) is full column rank, we can transform its column vector \\(\\{\\bar{x}_{1}, ..., \\bar{x}_{n}\\}\\) into a new set of orthonormal basis \\(\\{\\bar{q}_{1}, ..., \\bar{q}_{n}\\}\\) with Gram-Schmidt process. Let \\[ Q = [\\bar{q}_{1} \\;\\; \\cdots \\;\\; \\bar{q}_{n}] \\] For \\(\\bar{x}_i, \\; i = {1, ..., n}\\) in Span\\(\\{\\bar{x}_1, ... \\bar{x}_k\\}\\), there exists a set of constant \\(r_{1k}, ..., r_{kk}\\) such that3 \\[ \\bar{x}_k = r_{1k}\\bar{q}_{1} + \\cdots + r_{kk}\\bar{q}_{k} + 0 \\cdot\\bar{q}_{k+1} + \\cdots + 0 \\cdot \\bar{q}_{n} \\] So \\[ A = [\\bar{x}_{1} \\;\\; \\bar{x}_{2} \\;\\; \\cdots \\;\\; \\bar{x}_{n}] = [\\bar{q}_{1} \\;\\; \\bar{q}_{2} \\;\\; \\cdots \\;\\; \\bar{q}_{n}] \\begin{bmatrix} r_{11} &amp; r_{12} &amp; \\cdots &amp; r_{1n} \\\\ 0 &amp; r_{22} &amp; \\cdots &amp; r_{2n} \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\ 0 &amp; 0 &amp; \\cdots &amp; r_{nn} \\end{bmatrix} = QR \\] We could assume that \\(r_{kk} \\ge 0\\). (if \\(r_{kk} &lt; 0\\), multiply both \\(r_{kk}\\) and \\(\\bar{u}_k\\) by \\(-1\\)) 3.4 Orthonormal sets and orthogonal matrices An orthogonal set whose components are all unit vectors is said to be orthonormal sets. 3.4.1 Orthogonal matrices An orthogonal matrix is a square matrix \\(Q\\) whose inverse is its transpose: \\[ \\tag{3.1} QQ^T = Q^TQ = I \\] Another way of defining it is that an orthogonal matrix has both orthonormal columns and orthonormal rows. Orthogonal matrices have a nice property that they preserve inner products: \\[ (Q\\bar{x})^T(Q\\bar{y}) = \\bar{x}^TQ^TQ\\bar{y} = \\bar{x}^TI\\bar{y} = \\bar{x}^T\\bar{y} \\] A direct result is that \\(Q\\) preserves L2 norms \\[ \\|Q\\bar{x}\\|_2 = \\sqrt{(Q\\bar{x})^T(Q\\bar{x})} = \\sqrt{\\bar{x}^T\\bar{x}} = \\|\\bar{x}\\|_2 \\] Therefore multiplication by an orthogonal matrix can be considered as a transformation that preserves length, but may rotate or reflect the vector about the origin. Note that \\(Q\\) may not necessarily be a square matrix to satisfy \\(Q^TQ = I\\). For exmaple \\(Q \\in \\mathbb{R}^{m \\times n}, n &lt; m\\), but its columns and rows can still be orthonormal, then \\(QQ^T = I\\). But in most cases the term orthogonal implies a square matrix \\(Q\\). 3.5 Lesat squares problems Definition 3.1 the normal equation \\[ A^TA\\bar{x} = A^T\\bar{b} \\] \\[ \\begin{aligned} A^T(\\bar{b} - A\\hat{\\bar{x}}) &amp;= \\bar{0} \\end{aligned} \\\\ A^T\\bar{b} - A^TA\\hat{\\bar{x}} = 0 \\\\ \\hat{\\bar{x}} = (A^TA)^{-1}A^T\\bar{b} \\] because Span\\(\\{\\bar{x}_1, ... \\bar{x}_k\\}\\) is the same as Span\\(\\{\\bar{u}_1, ... \\bar{u}_k\\}\\)↩︎ "],
["eigenthings-and-quadratic-forms.html", "Chapter 4 Eigenthings and quadratic forms 4.1 Eigenvectors and eigenvalues 4.2 Diagnolization and similar matrices 4.3 Symmetric matrices 4.4 Quadratic forms 4.5 Cholesky factorization 4.6 Rayleigh quotients", " Chapter 4 Eigenthings and quadratic forms 4.1 Eigenvectors and eigenvalues Definition 4.1 (Eigenvectors and eigenvalues) An eigenvector of an \\(n \\times n\\) matrix \\(A\\) is a nonzero vector \\(\\bar{x}\\) such that \\(A\\bar{x} = \\lambda\\bar{x}\\). \\(\\lambda\\) is the eigenvalue of \\(A\\) if there is a nontrivial solution \\(\\bar{x}\\) of \\(A\\bar{x} = \\lambda \\bar{x}\\); such an \\(\\bar{x}\\) is called an eigenvector corresponding to \\(\\lambda\\) To find eigenvalues and corresponding eigenvectors of \\(A\\), we look at the equation \\[ (A - \\lambda I)\\bar{x}= 0 \\] Since eigenvector \\(\\bar{x}\\) must be nonzero, \\((A - \\lambda I)\\) is a singular matrix \\[\\begin{equation} \\tag{4.1} \\det (A - \\lambda I) = 0 \\end{equation}\\] Eq (4.1) is called the characteristic equation of matrix \\(A\\). This is a scalar equation containing information about eigenvalues and eigenvectors of a square matrix \\(A\\). Theorem 1.1 Eigenvalues of a trangular matrix are its diagonal entries. PROOF Consider the \\(3 \\times 3\\) case. If \\(A\\) is upper triangular, then \\(A - \\lambda I\\) has the form \\[ \\begin{bmatrix} a_{11} - \\lambda &amp; a_{12} &amp; a_{13} \\\\ 0 &amp; a_{22} - \\lambda &amp; a_{23} \\\\ 0 &amp; 0 &amp; a_{33} - \\lambda \\end{bmatrix} \\] So the roots of characteristic are \\(a_{11}, a_{22}, a_{33}\\) respectively. There are some useful results about how eigenvalues change after various manipulations. For any \\(k, b \\in \\mathbb{R}\\), \\(\\bar{x}\\) is an eigenvector of \\(kA + bI\\) with eigenvalue \\(k\\lambda + b\\) If \\(A\\) is invertible, then \\(\\bar{x}\\) is an eigenvector of \\(A^{-1}\\) with eigenvalue \\(1/\\lambda\\) \\(A^{k}\\bar{x} = \\lambda^{k}\\bar{x}\\) PROOF For (1) \\[ (kA + bI)\\bar{x} = kA\\bar{x} + bI\\bar{x} = k \\lambda\\bar{x} + b\\bar{x} = (k\\lambda + b)\\bar{x} \\] For(2) \\[ \\bar{x} = A^{-1}A\\bar{x} = A^{-1}\\lambda \\bar{x} = \\lambda A^{-1}\\bar{x} \\] The next theorem is important in terms of diagonalization and spectral decomposition Theorem 4.1 For distinct eigenvalues \\(\\lambda_1, \\cdots, \\lambda_r\\) of an \\(n \\times n\\) matrix A, their corresponding eigenvectors \\(\\bar{v_1}, ..., \\bar{v_r}\\) are linearly independent. PROOF Suppose for r distinct eigenvalue \\(\\lambda_1, \\cdots, \\lambda_r\\), the set \\(\\{\\bar{v_1}, ..., \\bar{v_r}\\}\\) is not linearly independent, and \\(p\\) is the least index such that \\(\\bar{v}_{p+1}\\) is a linear combination of the preceding vectors. Then there exists scalars \\(c_1, \\cdots, c_p\\) such that \\[ c_1\\bar{v}_1 + \\cdots + c_p\\bar{v}_p = \\bar{v}_{p+1} \\tag{1} \\] Left multiply by \\(A\\), and note we have \\(A\\bar{v}_i = \\lambda_i\\bar{v}_i\\) for \\(i = 1, ..., n\\) \\[ c_1\\lambda_1\\bar{v}_1 + \\cdots + c_p\\lambda_p\\bar{v}_p = \\lambda_{p+1}\\bar{v}_{p+1} \\tag{2} \\] Multiplying both sides of (2) by \\(\\lambda_{p+1}\\) and subtracting (2) from the result \\[ c_1(\\lambda_1 - \\lambda_{p+1})\\bar{v}_1 +\\cdots + c_p(\\lambda_p - \\lambda_{p+1})\\bar{v}_p = 0 \\tag{3} \\] Since \\(\\bar{v}_1, ..., \\bar{v}_p\\) are linearly independent, weights in (3) must be all zero. Since \\(\\lambda_1, \\cdots, \\lambda_p\\) are distinct, hence \\(c_i = 0, \\, i = 1, ..., p\\). But then (5) says that eigenvector \\(\\bar{v}_{p+1}\\) is zero vector, which contradicts definition 4.1 Corollary 4.1 Let \\(A \\in \\mathbb{R}^{m \\times n}\\) \\(A^TA\\) and \\(AA^T\\) has the same set of nonzero eigenvalues. PROOF Let \\(\\lambda\\) be a nonzero eigenvalue of \\(A^TA\\) and \\(\\bar{x}\\) its eigenvector \\[ \\begin{split} (A^TA)\\bar{x} &amp;= \\lambda\\bar{x} \\\\ \\end{split} \\] Left multiply by \\(A\\) \\[ AA^T(A\\bar{x}) = \\lambda (A\\bar{x}) \\] We will have to verify that \\(A\\bar{x}\\) is no zero vector before concluding \\(\\lambda\\) is also an eigenvector of \\(AA^T\\). Suppose \\(A\\bar{x} = 0\\), then \\(A^TA\\bar{x} =\\lambda\\bar{x} = 0\\). Since \\(\\bar{x}\\) is a eigenvector which is nonzero, \\(\\lambda = 0\\), which contradicts our former statement. Thus, any nonzero eigenvalue of \\(A^TA\\) is also an eigenvalue of \\(AA^T\\). \\(A^TA\\) and \\(AA^T\\) are known as Gram matrix and left Gram matrix in corollary 2.1 4.1.1 Additional properties of eigenvalues and eigenvectors Let \\(A \\in \\mathbb{R}^{n \\times n}\\) with eigenvalues \\(\\lambda_1, ..., \\lambda_n\\). Here are some additional properties of this matrix and its eigenvlaues: The trace of \\(A\\) is the sum of all eigenvalues \\[ \\text{tr}(A) = \\sum_{i=1}^{n}{\\lambda_i} \\] The determinant of \\(A\\) is the product of all its eigenvalues. \\[ \\det(A) = \\prod_{i=1}^{n}{\\lambda_i} \\] The eigenvalues of \\(k\\)th power of \\(A\\), i.e. \\(A^k\\), is \\(\\lambda_1^k, ..., \\lambda_n^k\\) If \\(A\\) is invertible, then eigenvalues of \\(A^{-1}\\) are \\(\\frac{1}{\\lambda_1}, ..., \\frac{1}{\\lambda_n}\\) For a polynomial function \\(P\\) the eigenvalues of \\(P(A)\\) are \\(P(\\lambda_1), ..., P(\\lambda_n)\\) 4.1.2 Left eigenvectors and right eigenvectors \\[ \\bar{x}A = \\lambda\\bar{x} \\] 4.2 Diagnolization and similar matrices Definition 4.2 (Diagonalization thoerem) An \\(n \\times n\\) matrix \\(A\\) is diagnolizable if and only if A has \\(n\\) independent linearly independent eigenvectors. In such case, in \\(A = P \\Lambda P^{-1}\\), the diagonal entries of \\(D\\) are eigenvalues that correpond, respectively, to the eigenvectors of in \\(P\\) In other words, \\(A\\) is diagnolizable if and only if there are enough eigenvectors in form a basis of \\(R^n\\), called an eigenvector basis of \\(R^n\\) Proof \\[ \\begin{split} AP &amp;= A[\\bar{v}_1 \\cdots \\bar{v}_n] \\\\ &amp;= [A\\bar{v}_1 \\cdots A\\bar{v}_n] \\\\ &amp;= [\\lambda_1\\bar{v}_1 \\cdots \\lambda_n\\bar{v}_n] \\end{split} \\] while on the other side of the equation: \\[ \\begin{aligned} DP &amp;= [\\bar{v}_1 \\cdots \\bar{v}_n] \\begin{bmatrix} \\lambda_1 &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; \\lambda_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\lambda_n \\end{bmatrix} \\\\ &amp;= [\\lambda_1\\bar{v}_1 \\cdots \\lambda_n\\bar{v}_n] \\end{aligned} \\] So that \\[ \\begin{aligned} AP &amp;= PD \\\\ A &amp;= P \\Lambda P^{-1} \\end{aligned} \\] Because \\(P\\) contains \\(n\\) independent columns so it’s invertible. According to theorem 4.1, an \\(n \\times n\\) matrix with \\(n\\) distinct eigenvalues is diagonalizable. This is a sufficient condition. For matrices whose eigenvalues are not distinct, there is still a change that it is diagonalizable. For any matrix \\(A_{n\\times n}\\), as long as the sum of the dimensions of the eigenspaces equals \\(n\\) then \\(P\\) is invertible. This could happen in the following two scenarios The characteristic polynomial factors completely into linear factors. This is the case when \\(A\\) has n distinct eigenvalues. The dimension of the eigenspace for each \\(\\lambda_k\\) equals the multiplicity of \\(\\lambda_k\\). Thus \\(A\\) with repeated eigenvalues can still be diagonalizable. Repeated eigenvalues create the possibility that a diagonalization might not exist. Particularly, if less than \\(r_i\\) eigenvectors exist for an eigenvalue with multiplicity \\(r_i\\), a diagonalization does not exist. Such a matrix is said to be deflective. 4.2.1 Similarity If \\(A\\) and \\(B\\) are both \\(n \\times n\\) matrices, then \\(A\\) is similar to \\(B\\) if there is an invertible matrix \\(P\\) such that \\(P^{-1}AP = B\\), or equivalently if we write \\(Q\\) for \\(P^{-1}\\), \\(Q^{-1}BQ = A\\). Changing \\(A\\) into \\(P^{-1}AP\\) is called a similarity transformation. Theorem 3.1 If \\(A\\) and \\(B\\) are similar, they have the same eigenvalues. PROOF If \\(B = P^{-1}AP\\), then \\[ B - \\lambda I = P^{-1}AP - \\lambda P^{-1}P = P^{-1}(AP - \\lambda P) = P^{-1}(A - \\lambda I) P \\] so that \\[ \\det (B - \\lambda I ) = \\det(P) \\cdot \\det(A - \\lambda I ) \\cdot \\det(P^{-1}) \\] since \\(\\det(P) \\cdot \\det(P^{-1}) = \\det (I) = 1\\), we have \\[ \\det (B - \\lambda I) = \\det(A - \\lambda I) \\] As a result of their identical characteristic polynomial, \\(B\\) and \\(A\\) have the same eigenvalues. We can also show that eigenvector of \\(B\\) is \\(P\\bar{v}\\): \\[ \\begin{aligned} A\\bar{v} &amp;= \\lambda\\bar{v} \\\\ (P^{-1}BP)\\bar{v} &amp;= \\lambda\\bar{v} \\\\ P(P^{-1}BP)\\bar{v} &amp;= \\lambda P\\bar{v} \\\\ B(P\\bar{v}) = \\lambda P \\bar{v} \\end{aligned} \\] The similarity theorem leads to a interesting result. Corollary 2.1 For \\(A, B \\in \\mathbb{R}^{n \\times n}\\), \\(AB\\) and \\(BA\\) are similar matrices and therefore share the same set of eigenvalues. To prove this, we need to show that there exists a invertible matrix \\(A\\) such that \\(P^{-1}(AB)P = BA\\). Take \\(P = A\\) and the equation holds. It is easy to show that similarity is transitive: if \\(A\\) is similar to \\(B\\), \\(B\\) is similar to \\(C\\), then \\(A\\) is similar to \\(C\\). So similarity means a family of matrices with the same set of eigenvalues, the most special and simplest of which is the diagonal matrix (if this is an diagonalizable family). Some computer algorithms calculate eigenvalues of \\(A\\) in this manner: with a sequential choices of \\(P\\), the off-diagonal elements of \\(A\\) become smaller and smaller until \\(A\\) becomes a triangular matrix or diagonal matrix, whose eigenvalues are simply diagonal entries and is the same as \\(A\\). It is obvious that a diagonalizable matrix \\(A\\) is similar to diagonal matrix \\(D\\), whose diagonal entries are \\(A\\)’s eigenvalues \\(\\lambda_i\\), and \\(P = [\\bar{v}_1 \\;\\; \\cdots \\;\\; \\bar{v}_n]^{-1}\\) where \\(\\bar{v}_i, \\;i = 1,..., n\\) are eigenvectors corresponding to \\(\\lambda_i\\). But square matrix \\(A\\) can still be similar to matrices other than \\(D\\) with other choices of \\(P\\), and non-diagonal matrices can also have similar matrices of their own. In fact, every square matrix is similar to a matrix in Jordan matrix 4.2.2. Similarity is only a sufficient condition for identical eigenvalues. The matrices \\[ \\begin{bmatrix} 2 &amp; 1 \\\\ 0 &amp; 2 \\end{bmatrix} \\;\\text{and}\\; \\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 2 \\end{bmatrix} \\] are not similar even though they have the same eigenvalues. 4.2.2 Jordan matrix For non-diagonalizable square matrix \\(A_{n \\times n}\\), the goal is to with similar transformation \\(P^{-1}AP\\) construct a matrix that is as nearest to a diagonal matrix as possible. Definition 4.3 The \\(n \\times n\\) matrix \\(J_{\\lambda, n}\\) with \\(\\lambda\\)s on the diagonal, \\(1\\)s on the superdiagonal and \\(0\\)s elsewhere is called a Jordan matrix. A Jordan matrix in Jordan normal form is a block matrix that has Jordan blocks down its block diagonal and is zero elsewhere An example of Jordan matrix, the appearance of \\(\\lambda_i\\) on the diagonal is equal to its multiplicity as \\(A\\)’s eigenvalue. \\[ \\begin{bmatrix} \\lambda_1 &amp; 1 &amp; \\\\ &amp; \\lambda_1 &amp; 1 &amp; \\\\ &amp; &amp; \\lambda_1 &amp; \\\\ &amp; &amp; &amp; \\lambda_2 &amp; 1 \\\\ &amp; &amp; &amp; &amp; \\lambda_2 \\\\ &amp; &amp; &amp; &amp; &amp; \\lambda_3 &amp; 1 \\\\ &amp; &amp; &amp; &amp; &amp; &amp; \\ddots \\\\ &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\lambda_n &amp; 1 \\\\ &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\lambda_n \\end{bmatrix} \\] An illustration from wikipedia, the circled area is the Jordan blcok. Though the purpose of this section was not the computation details of Jordan matrices, it helps to give a concrete example. Consider \\(A\\) \\[ A = \\begin{bmatrix} 5 &amp; 4 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; -1 &amp; -1 \\\\ -1 &amp; -1 &amp; 3 &amp; 0 \\\\ 1 &amp; 1 &amp; -1 &amp; 2 \\end{bmatrix} \\] Including multiplicity, the eigenvalues of \\(A\\) are \\(\\lambda = 1, 2, 4, 4\\). And for \\(\\lambda = 4\\), the eigenspace is 1 dimensional instead of 2, meaning \\(A\\) is not diagonalizable. Nonetheless, \\(A\\) is similar to the following Jordan matrix \\[ J = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 2 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 4 &amp; 1\\\\ 0 &amp; 0 &amp; 0 &amp; 4 \\end{bmatrix} \\] To obtain \\(P\\), recall that \\(P^{-1}AP = J\\). Let \\(P\\) have column vectors \\(p_i, \\; i = 1,...,4\\), then: \\[ A[\\bar{p}_1 \\; \\; \\bar{p}_2 \\;\\; \\bar{p}_3 \\;\\; \\bar{p}_4] = [\\bar{p}_1 \\; \\; \\bar{p}_2 \\;\\; \\bar{p}_3 \\;\\; \\bar{p}_4] \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 2 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 4 &amp; 1\\\\ 0 &amp; 0 &amp; 0 &amp; 4 \\end{bmatrix} = [\\bar{p}_1 \\;\\; 2\\bar{p}_2 \\;\\; 4\\bar{p}_3 \\;\\; \\bar{p}_3 + 4\\bar{p}_4] \\] We see that \\[ \\begin{aligned} (A - 1I)\\bar{p}_1 &amp;= \\bar{0} \\\\ (A - 2I)\\bar{p}_2 &amp;= \\bar{0} \\\\ (A - 4I)\\bar{p}_3 &amp;= \\bar{0} \\\\ (A - 1I)\\bar{p}_4 &amp;= \\bar{p}_3 \\end{aligned} \\] The solutions \\(\\bar{p}_i\\) are called generalized eigenvectors of \\(A\\). 4.2.3 Simultaneous Diagonalization A diagonlizable matrix family that share the same eigenvectors is called simultaneously diagonalizable. This notion is complimentary to a family of similar matrices that are diagonalizable, share eigenvalues but not eigenvectors. Definition 4.4 (Simultaneously diagonalizable) Two diagonalizable matrices \\(A\\) and \\(B\\) are said to be simultaneously diagonalizable if a \\(n \\ times n\\) matrix \\(P\\) exists, such that \\(P^{-1}AP\\) and \\(P^{-1}BP\\) are diagonal matrices. In other words \\[ A = P\\Lambda_1P^{-1} \\\\ B = P\\Lambda_2P^{-1} \\] The geometric interpretation of simultaneously diagonalizable matrices is that they perform scaling in the same set of directions. Theorem 4.2 If \\(A\\) and \\(B\\) are diagonalizable matrices, they are simultaneously diagonalizable if and only if they commute, such that \\(AB\\) = \\(BA\\). I don’t know how to prove this yet. But this theorem is useful in identifying diagonalizable matrices with the same eigenvectors. 4.2.4 Cayley-Hamilton theorem For any square matrix \\(A_{n \\times n}\\), the characteristic polynomial of \\(\\lambda\\) is defined as \\[ \\det(A - \\lambda I) \\] We can obtain a polynomial of matrix \\(A\\) by substituting \\(A\\) for \\(\\lambda\\), and \\(kI\\) for constant terms. For example, the matrix form of the polynomial \\(3\\lambda^2 + 2\\lambda + 2\\) is \\(3A^2 + 2A + 2I\\). Theorem 4.3 (Cayley-Hamilton Theorem) Let \\(f(\\lambda)\\) be the polynomial function of the characteristic polynomial \\(\\det(A - \\lambda I)\\), where \\(A\\) is a square matrix. Then \\(f(A)\\) evaluates to a zero matrix. PROOF Though the Caley Hamilton theorem 4.3 applies to any square matrix \\(A\\). Our proof only address the case for diagonalizable matrices. When \\(A\\) is digonalizable, the polynomial of \\(A\\) takes the form \\[ f(A) = Pf(\\Lambda)P^{-1} \\] Since \\(f(\\lambda) = \\det(A - \\lambda I)\\), and the diagonal entries of \\(\\Lambda\\) are the eigenvalues of \\(A\\). Evaluate \\(f(\\lambda)\\) at each diagonal entry of \\(\\Lambda\\) will be zero. Thus \\(f(A)\\) is a zero matrix. A direct result derived from the Cayley Hamilton theorem is that for every intertible matrix \\(A\\), its inverse \\(A^{-1}\\) can be represented as a polynomial of \\(A\\) with degree \\(d - 1\\). Proposition 4.1 (Polynomial representation of matrix inverse) The inverse of an invertible square matrix \\(A\\) is a polynomial of \\(A\\) with degree at most \\(d -1\\). Since the constant term in the characteristic polynomial is the product of eigenvalues, which is nonzero for nonsingular matrices, we can write the Cayley-Hamilton matrix polynomial \\(f(A)\\) in the form \\(f(A) = A \\cdot g(A) + kI\\). \\(A \\cdot g(A)\\) is obtained by factoring out \\(A\\) from the d-degree matrix polynomial, leaving \\(g(A)\\) with degree of \\(d - 1\\). Since \\(f(A)\\) evaluates to zero, we have \\[ A \\underbrace{\\Big( - g(A) / k\\Big)}_{A^{-1}} = I \\] Therefore, \\(A^{-1}\\) is shown to be a polynomial of \\(A\\). 4.3 Symmetric matrices A square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) is symmetric if \\(A = A^{T}\\), and anti-symmetric if \\(A = - A^{T}\\). It can be shown that for any \\(A \\in \\mathbb{R}^{n \\times n}\\), \\(A + A^T\\) is symmetric and \\(A - A^T\\) anti-symmetric. So any square matrix \\(A\\) can be wrote as a sum of a symmetric matrix and an anti-symmetric matrix \\[ A = \\frac{1}{2}(A + A^T) + \\frac{1}{2}(A - A^T) \\] It is common to denote the set of all symmetric matrices of size \\(n\\) as \\(\\mathbb{S}^n\\), and \\(A \\in \\mathbb{S}^n\\) means \\(A\\) is a symmetric \\(n \\times n\\) matrix. Symmetric matrices have some nice properties about diagonalization. Theorem 4.4 If \\(A\\) is symmetric, eigenvectors from distinct eigenvalues are orthogonal. PROOF Let \\(\\bar{v}_1\\) and \\(\\bar{v}_2\\) be eigenvectors that correspond to distinct eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\). Compute \\[ \\begin{split} \\lambda_1\\bar{v}_1 \\cdot \\bar{v}_2 &amp;= (\\lambda_1\\bar{v}_1)^T\\bar{v}_2 \\\\ &amp;= (\\bar{v}_1^TA^T)\\bar{v}_2 \\\\ &amp;= \\bar{v}_1^T(A\\bar{v}_2) \\\\ &amp;= \\bar{v}_1^T(\\lambda_2\\bar{v}_2) \\\\ &amp;= \\lambda_2\\bar{v}_1 \\cdot \\bar{v}_2 \\end{split} \\] because \\(\\lambda_1 \\not = \\lambda_2\\), \\(\\bar{v}_1 \\cdot \\bar{v}_2 = 0\\). For symmetric matrices \\(A \\in \\mathbb{R}^{n \\times n}\\) without \\(n\\) distinct eigenvalues, it turns out that the dimension of the eigenspace for each \\(\\lambda_k\\) always equals the multiplicity of \\(\\lambda_k\\). For this reason, if \\(A\\) is a symmetric matrix we can always construct a orthonormal set \\(\\{\\bar{q}_1 \\;\\; \\cdots \\;\\; \\bar{q}_n\\}\\) from \\(\\{\\bar{v}_1 \\;\\; \\cdots \\;\\; \\bar{v}_n\\}\\) such that \\[ Q^{T} = \\begin{bmatrix} \\bar{q}_1^T \\\\ \\vdots \\\\ \\bar{q}_n^T \\end{bmatrix} = Q^{-1} \\] Recall that matrix \\(A\\) with \\(n\\) linearly independent eigenvectors is diagonalizable and can be written as \\[ A = P \\Lambda P^{-1} \\] where \\(P = [\\bar{v}_1 \\;\\; \\cdots \\;\\; \\bar{v}_n]\\) and \\(\\Lambda\\) is a diagonal matrix with eigenvalues on its diagonal entries. With symmetric matrices, \\(\\{\\bar{v}_1, \\cdots, \\bar{v}_n\\}\\) must be linearly independent and can be transformed into a orthonormal basis \\(\\{\\bar{q}_1, \\cdots, \\bar{q}_n\\}\\). With orthogonal matrix \\(Q =[\\bar{q}_1 \\;\\; \\cdots \\;\\; \\bar{q}_n]\\), we have \\[\\begin{equation} \\tag{4.2} A = Q \\Lambda Q^{T} \\end{equation}\\] Such matrix \\(A\\) is said to be orthogonally diagonalizable. We have seen that for symmetric matrix \\(A\\), Eq (4.2) always holds. We can also also verify that if \\(A\\) is orthogonally diagonalizable then it is a symmetric matrix \\[ A^T = (Q \\Lambda Q^{T})^T = (Q^T)^T\\Lambda^TQ^T= Q \\Lambda Q^{T} = A \\] Theorem 4.5 An \\(n \\times n\\) matrix \\(A\\) is orthogonally diagonalizable if an only if \\(A\\) is a symmetric matrix. 4.3.1 Spectral decomposition For orthogonally diagonalizable matrix \\(A\\), we have \\[ A = Q \\Lambda Q^{T} = [\\bar{q}_1 \\;\\; \\cdots \\;\\; \\bar{q}_n] \\begin{bmatrix} \\lambda_1 &amp; &amp; \\\\ &amp; \\ddots \\\\ &amp; &amp; \\lambda_n \\end{bmatrix} \\begin{bmatrix} \\bar{q}_1^T \\\\ \\vdots \\\\ \\bar{q}_n \\end{bmatrix} \\] It follows that \\[\\begin{equation} \\tag{4.3} A = \\lambda_1\\bar{q}_1\\bar{q}_1^T + \\cdots + \\lambda_1\\bar{q}_n\\bar{q}_n^T \\end{equation}\\] Eq (4.3) is called the spectral decomposition, breaking \\(A\\) into pieces of rank 1 matrix. It got this name because he set of eigenvalues of a matrix \\(A\\) is sometimes called its spectrum. 4.3.2 A-Rrthogonality Definition 4.5 (A-Orthogonality) Column vector \\(\\bar{v}_i\\) and \\(\\bar{v}_j\\) are said to be A-orthogonal if \\(\\bar{v}_i^TA\\bar{v}_j\\) for some \\(n \\times n\\) invertible matrix \\(A\\). Similarly, a set of column vectors \\(\\bar{v}_1, ..., \\bar{v}_n\\) is A-orthogonal, if and only if \\(\\bar{v}_i^TA\\bar{v}_i = 0\\) for each pair of vectors. 4.4 Quadratic forms Definition 4.6 (Quadratic form) A quadratic form on \\(\\mathbb{R}^n\\) is a function \\(Q\\) defined on \\(\\mathbb{R}^n\\) whose value at a vector \\(\\bar{x}\\) in \\(\\mathbb{R}^n\\) can be computed by an expression of the form \\(Q(\\bar{x}) = \\bar{x}^TA\\bar{x}\\), where \\(A \\in \\mathbb{R}^{n \\times n}\\) is a symmetric matrix. \\(A\\) is called the matrix of the quadraticc form. There exists a one-to-one mapping between symmetric matrix \\(A\\) and the quadratic form. Consider the \\(3 \\times 3\\) case: \\[ \\bar{x} = \\begin{bmatrix} x_1 \\\\ x_3 \\\\ x_3 \\\\ \\end{bmatrix} , \\;\\; A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{bmatrix} \\\\ \\] \\[ \\begin{split} \\bar{x}^TA\\bar{x} &amp;= [x_1 \\;\\; x_2 \\;\\; x_3] \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_3 \\\\ x_3 \\\\ \\end{bmatrix} \\\\ &amp;= a_{11}x_1^2 + a_{22}x_2^2 + a_{33}x_3^2 + \\\\ &amp; \\quad(a_{12} + a_{21})x_1x_2 + (a_{13} + a_{31})x_1x_3 + (a_{23} + a_{32})x_2x_3 \\end{split} \\tag{1} \\] Since \\(A\\) is symmetric, we have \\(a_{ij} = a_{ji}\\), thus \\[ \\bar{x}^TA\\bar{x} = a_{11}x_1^2 + a_{22}x_2^2 + a_{33}x_3^2 + 2a_{12}x_1x_2 + 2a_{13}x_1x_3 + 2a_{23}x_2x_3 \\tag{2} \\] This verifies that \\(\\bar{x}^TA\\bar{x}\\) when \\(A \\in \\mathbb{R}^{n \\times n}\\) is symmetric does result in a quadratic function of \\(n\\) variables. Conversely, any quadratic function of \\(n\\) variables, like shown in \\((2)\\), can be expressed in terms of \\(\\bar{x}^TA\\bar{x}\\) with unique choice of symmetric matrix \\(A \\in \\mathbb{R}^{n \\times n}\\). 4.4.1 Change of variabele If \\(\\bar{x}\\) is a variable vector in \\(\\mathbb{R}^n\\), then a change of variable is an equation of the form \\[ \\begin{aligned} \\bar{x} &amp;= P\\bar{y} \\\\ \\text{or equivalently} \\quad \\bar{y} &amp;= P^{-1}\\bar{x} \\end{aligned} \\] where \\(P\\) is any invertible matrix \\(\\in \\mathbb{R}^{n \\times n}\\) Theorem 4.6 (The Principal Axes Theorem) Let \\(A\\) be an \\(n \\times n\\) symmetric matrix. Then there exists an orthogonal change of variable, \\(\\bar{x} = Q\\bar{y}\\), this transform the quadratic form \\(\\bar{x}^TA\\bar{x}\\) into a quadratic form \\(\\bar{y}^T\\Lambda\\bar{y}\\) with no cross-product term. \\(Q\\) is constructed with \\(A\\)’s orthonormal eigenvectors \\(\\bar{q}_1, ..., \\bar{q}_n\\). According to theorem (4.2): \\[ \\bar{x}^TA\\bar{x} = (Q\\bar{y})^TA(Q\\bar{y}) = \\bar{y}^TQ^{T}AQ\\bar{y} = \\bar{y}^T \\Lambda \\bar{y} \\] The principal axes theorem 4.6 shows that if \\(A\\) is diagonalizable, quadratic form \\(\\bar{x}^TA\\bar{x}\\) can be reexpressed into the form \\(\\lambda_1y_1^2 + \\lambda_2y_2^2 + \\cdots + \\lambda_ny_n^2\\) with change of variables \\(\\bar{x} = Q\\bar{y}\\). 4.4.2 Classification of quadratic forms A symmetric matrix \\(A \\in \\mathbb{S}^n\\) is positive definite (PD) if for all non-zero vectors \\(\\bar{x} \\in \\mathbb{R}^n,\\; \\bar{x}^TA\\bar{x} &gt; 0\\). We can denote positive definite matrix \\(A\\) as \\(A \\succ 0\\) (or \\(A &gt; 0\\)). The set of all positive definite matrices is denoted as \\(\\mathbb{S}_{++}^n\\) A symmetric matrix \\(A \\in \\mathbb{S}^n\\) is positive semidefinite (PSD) if for all non-zero vectors \\(\\bar{x} \\in \\mathbb{R}^n,\\; \\bar{x}^TA\\bar{x} \\ge 0\\). We can denote positive definite matrix \\(A\\) as \\(A \\succeq 0\\) (or \\(A \\ge 0\\)). The set of all positive semidefinite matrices is denoted as \\(\\mathbb{S}_{+}^n\\) A symmetric matrix \\(A \\in \\mathbb{S}^n\\) is negative definite (ND), denoted by \\(A \\prec 0\\) (or \\(A &lt; 0\\)), if for all non-zero vectors \\(\\bar{x} \\in \\mathbb{R}^n,\\; \\bar{x}^TA\\bar{x} &lt; 0\\). Similarly, a symmetric matrix \\(A \\in \\mathbb{S}^n\\) is negative semidefinite (NSD), denoted by \\(A \\preceq 0\\) (or \\(A \\le 0\\)), if for all non-zero vectors \\(\\bar{x} \\in \\mathbb{R}^n,\\; \\bar{x}^TA\\bar{x} \\le 0\\). Finally, a symmetric matrix \\(A \\in \\mathbb{S}^n\\) is indefinite, if it is neither positive semidefinite or negative semidefinite. In other words, if there exists \\(\\bar{x}, \\bar{x}&#39;, \\in \\mathbb{R}^{n}\\) such taht \\(\\bar{x}^TA\\bar{x} &gt; 0\\) and \\(\\bar{x&#39;}^TA\\bar{x}&#39; &gt; 0\\) Note that when talking about \\(A\\) being PD, PSD, ND, NSD or indefinite, \\(A\\) is always assumed to be symmetric. Also, if \\(A\\) is positive definite, then \\(−A\\) is negative definite and viceversa. Likewise, if \\(A\\) is positive semidefinite then \\(−A\\) is negative semidefinite and vice versa. If \\(A\\) is indefinite, then so is \\(−A\\). From theorem 4.6, we know that the sign of eigenvalues are closely related to classifications of symmetric matrices here. Take positive definite matrices for example, the following statements of \\(A\\) are equivalent: For any \\(\\bar{x} \\in \\mathbb{R}^n, \\; \\bar{x}^TA\\bar{x} &gt; 0\\) Let \\(\\lambda_i, \\; i = 1, ..., n\\) be \\(A\\)’s eigenvalues, \\(\\lambda_i &gt; 0\\) All leading determinants of \\(A &gt; 0\\) All pivots are \\(&gt; 0\\) Classification of \\(A \\in \\mathbb{S}^{n}\\) by its eigenvalue can be applied in general. Theorem 4.7 (Quadratic forms and eigenvalues) Let \\(A \\in \\mathbb{S}^{n}\\). Then the quadratic form \\(\\bar{x}^TA\\bar{x}\\) and \\(A\\) is: positive definite if and only if the eigenvalues of \\(A\\) are all positive negative definite if and only if the eigenvalues of \\(A\\) are all negative indefinite if and only if \\(A\\) has both positive and negative eigenvalues Corollary 4.2 Given positive definite matrices \\(A, B \\in \\mathbb{S}^n\\) and \\(\\alpha \\in \\mathbb{R}\\), the following results remain to be positive definite. Scalar multiplication of PD matrices \\(\\alpha A\\) are PD matrices The sum of PD matrices \\(A +B\\) are PD matrices If a PD matrix is invertible, its inverse \\(A^{-1}\\) is also PD. Similar matrix of a PD matrix is PD. Corollary 4.3 Given any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(A^TA\\) and \\(AA^T\\) are always positive semidefinite matrices PROOF By definition, \\(A^TA\\) is a positive semidefinite matrix if for any \\(\\bar{x} \\in \\mathbb{R}^n\\), the quadratic form \\(\\bar{x}^T(A^TA)\\bar{x} \\ge 0\\). \\[ \\begin{split} \\bar{x}^T(A^TA)\\bar{x} &amp;= (\\bar{x}^TA^T)(A\\bar{x}) \\\\ &amp;= (A\\bar{x})^T(A\\bar{x}) \\\\ &amp;= \\|A\\bar{x}\\|^2 \\end{split} \\] It turns out that the result is the square of the 2-norm of \\(A\\bar{x}\\) (nonnegative). This also tells \\(A^TA\\) is positive definite when \\(\\bar{x} \\not\\subseteq \\mathcal{N}(A)\\) Similarly, the quadratic form for \\(AA^T\\) can be refactored in to the 2-norm of \\(A^T\\bar{x}\\). Corollary 4.4 \\(A^TA +\\lambda I\\) and \\(AA^T + \\lambda I\\) are always positive definite and invertible for \\(\\lambda &gt; 0\\) PROOF From the previous corollary 4.3 we know that \\(A^TA\\) and \\(AA^T\\) are positive semidefinite, and that they have the same nonzero eigenvalues from corollary 4.1. According to Section 4.1.1, eigenvalues for \\(P(A)\\) are \\(P(\\lambda)\\) for polynomial function \\(P\\). Therefore, \\(A^TA +\\lambda I\\) and \\(AA^T + \\lambda I\\) share a positive set of \\(n\\) eigenvalues \\(\\lambda_1 + \\lambda, ..., \\lambda_r + \\lambda, \\lambda, ..., \\lambda\\), so they are PD and invertible. 4.5 Cholesky factorization Lemma 4.1 A symmetric matrix \\(A \\in \\mathbb{S}^n\\) is positive semidefinite if and only if it can be expressed in the gram matrix form \\(B^TB\\) of some matrix \\(B\\). The previous corollary 4.3 shows that if \\(A = B^TB\\) then it is positive definite. Conversely, if \\(A\\) is PSD (or PD), we have \\(A = Q \\Lambda Q^T\\) where \\(\\Lambda\\)’s diagonal entries are all nonnegative. Then we can set \\(\\Lambda ^{1/2} = \\Sigma\\) and \\(B = (Q\\Sigma)^T\\). Then \\(A = Q\\Sigma^2Q^T = (Q\\Sigma)(Q\\Sigma)^T = B^TB\\). Note that we could also have stated this lemma using \\(BB^T\\) instead of \\(B^TB\\), and the proof is similar. This lemma is inspiring in that for every PD matrix \\(A \\in \\mathbb{R}^n\\), there exists factorization \\(A = BB^T\\). In fact, this factorization is not unique. We can use an orthogonal matrix \\(P\\) to create an additional orthogonal factorization \\(A = BB^T = B(PP^T)B^T = (BP)(BP)^T\\). Since the initial \\(B = P\\Lambda^{1/2}\\) is full rank, with an appropriate choice of \\(P\\), we can turn the \\(PB\\) into an lower triangular matrix \\(L\\) such that \\(A = LL^T\\). The uniqueness of this factorization with lower triangular matrix \\(L\\) can be proved with induction. The decomposition of PD matrices into the product of an lower triangular matrix and its transpose is called the Cholesky factorization, in this factorization \\[ A = LL^T \\] To make it more clear \\[ \\begin{bmatrix} a_{11} &amp; \\cdots &amp; a_{1n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; \\cdots &amp; a_{nn} \\end{bmatrix} = \\begin{bmatrix} l_{11} &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ l_{n1} &amp; \\cdots &amp; l_{nn} \\end{bmatrix} \\begin{bmatrix} l_{11} &amp; \\cdots &amp; l_{n1} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; l_{nn} \\end{bmatrix} \\] Since \\(L\\) is lower triangular, we can solve \\(L\\) from \\(A = LL^T\\) with a system of equations that can be easily solved using back-substitution. For example, \\(l_{11} = \\sqrt{a_{11}}\\), and \\(a_{i1} / l_{11}\\). The Cholesky factorization is a special case of LU decomposition 4.6 Rayleigh quotients Let \\(A \\in \\mathbb{S}^n\\) and \\(\\bar{x} \\in \\mathbb{R}^n\\), Rayleigh quotient is defined as \\[ R_{A}(\\bar{x}) = \\frac{\\bar{x}^TA\\bar{x}}{\\bar{x}^T\\bar{x}} \\] The Rayleigh quotient has some nice properties: scale invariance: for any vector \\(\\bar{x} \\not= 0\\) and any scalar \\(\\alpha \\not= 0\\), \\(R_{A}(\\bar{x}) = R_{A}(\\alpha\\bar{x})\\) If \\(\\bar{x}\\) is a eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\), then \\(R_{A}(\\bar{x}) = \\lambda\\) The Rayleigh quotient is bounded by the largest and smallest eigenvalue of \\(A\\), i.e. \\[ \\lambda_{\\text{min}}(A) \\le R_{A}(\\bar{x}) \\le \\lambda_{\\text{max}}(A) \\] PROOF Since the Rayleigh quotient does not depend on the 2-norm of vector \\(\\bar{x}\\), we may assume a unit vector \\(\\bar{x}^T\\bar{x} = 1\\), and Rayleigh quotient simplifies to the quadratic form \\(\\bar{x}^TA\\bar{x}\\). Next, orthogonally diagonalize \\(A\\) as \\(Q \\Lambda Q\\), we know that when \\(\\bar{x} = Q \\bar{y}\\): \\[ \\bar{x}^TA\\bar{x} = \\bar{y}^T \\Lambda \\bar{y} \\tag{1} \\] Also \\[ 1= \\bar{x}^T\\bar{x} = (Q\\bar{y})^T Q\\bar{y} = \\bar{y}^TQ^TQ\\bar{y} = \\bar{y}^T\\bar{y} \\tag{2} \\] Expand \\(\\bar{y}^T \\Lambda \\bar{y}\\) in (1) we get \\[ \\bar{x}^TA\\bar{x} = \\lambda_1y_1^2 + \\lambda_2y_2^2 + \\cdots + \\lambda_ny_n^2 \\tag{3} \\] where \\(\\{\\lambda_1, ..., \\lambda_n\\}\\) are diagonal entries of \\(\\Lambda\\) and eigenvalues of \\(A\\). Let us suppose that the set \\(\\{\\lambda_1, \\lambda_2, \\cdots, \\lambda_n\\}\\) has already been ordered descendingly, so that \\(\\lambda_1 &gt; \\lambda_2 &gt; \\cdots &gt; \\lambda_n\\). We can obtain the inequality from (3) and the order of eigenvalues: \\[ \\begin{split} \\bar{x}^TA\\bar{x} &amp;= \\lambda_1y_1^2 + \\lambda_2y_2^2 + \\cdots + \\lambda_ny_n^2 \\\\ &amp;\\le \\lambda_1y_1^2 + \\underbrace{\\lambda_1y_2^2 + \\cdots + \\lambda_1y_n^2}_{\\lambda_1 \\text{ is the greatest eigenvalue}} \\\\ &amp;\\le \\lambda_1(\\bar{y}^T\\bar{y}) \\\\ &amp;= \\lambda_1 \\end{split} \\] The equation reach equality if and only if \\([y_1, y_2, \\cdots, y_n] = [1, 0, \\cdots, 0]\\). Since \\(\\bar{x} = Q\\bar{y}\\), we have \\[ \\bar{x} = \\begin{bmatrix} \\bar{q}_1 &amp; \\bar{q}_2 &amp; \\cdots &amp; \\bar{q}_n \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} = \\bar{q}_1 \\] Similarly, the minimum of the Rayleigh quotient will be \\(\\lambda_n\\), with \\(\\bar{x} = \\bar{q}_n\\). From the optimization perspective, the bound of Rayleigh quotient amounts to a constrained optimization problem \\[ \\begin{aligned} \\text{objective function} &amp;: \\bar{x}^TA\\bar{x}\\\\ \\text{subject to}&amp;: \\bar{x}^T\\bar{x} = 1 \\end{aligned} \\] The maximum and minimum of the objective function are \\(\\lambda_1\\) and \\(\\lambda_n\\), with \\(\\bar{x}\\) being \\(\\bar{q}_1\\) and \\(\\bar{q}_n\\) respectively. If we add more constraints, for example, that \\(\\bar{x}\\) should be orthogonal to \\(\\bar{q}_1\\), then \\(\\bar{x}^TA\\bar{x}\\) has maximum \\(\\lambda_2\\) attained at \\(\\bar{x} = \\lambda_2\\) Theorem 4.8 Let \\(A \\in \\mathbb{S}^n\\) with orthogonal diagonalization \\(A = Q\\Lambda Q^T\\), where the entries on the diagonal of \\(\\Lambda\\) are arranged so that \\(\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_n\\). Then for \\(k = 2, ...\\), the maximum of value of \\(\\bar{x}^T A \\bar{x}\\) subject to constraints \\[ \\bar{x}^T\\bar{x} = 1, \\;\\; \\bar{x}^T\\bar{q}_1 = 0, \\;\\; \\dots \\;\\;, \\bar{x}^T\\bar{q}_{k-1} = 0 \\] is the eigenvalue \\(\\lambda_k\\), and this maximum is attained at \\(\\bar{x} = \\bar{q}_k\\) PROOF From \\(\\bar{x} = P\\bar{y}\\) we know that \\[ \\bar{x} = y_1\\bar{q}_1 + \\cdots + + y_{k-1}\\bar{q}_{k-1} + y_k\\bar{q}_k + \\cdots + y_{n}\\bar{q}_n \\] Left multiply by \\(\\bar{q}_1^T\\) \\[ \\begin{aligned} \\bar{q}_1^T\\bar{x} &amp;= y_1\\bar{q}_1^T\\bar{q}_1 + \\cdots + + y_{k-1}\\bar{q}_1^T\\bar{q}_{k-1} + y_k\\bar{q}_1^T\\bar{q}_k + \\cdots + y_{n}\\bar{q}_1^T\\bar{q}_n \\\\ &amp;= y_1\\bar{q}_1^T\\bar{q}_1 \\\\ &amp;= y_1 \\end{aligned} \\] Since \\(\\bar{q}_1^T\\bar{x} = \\bar{x}^T\\bar{q}_1 = 0\\), we have \\(y_1 = 0\\). Similarly, \\(y_2 = \\cdots = y_{k-1} = 0\\), and \\(\\bar{y}\\) becomes \\([0 \\;\\; \\cdots \\;\\; 0 \\;\\; y_{k} \\;\\; \\cdots \\;\\; y_n]\\). And the inequality now becomes: \\[ \\begin{split} \\bar{x}^TA\\bar{x} &amp;= \\lambda_1y_1^2 + \\lambda_2y_2^2 + \\cdots + \\lambda_ny_n^2 \\\\ &amp;= \\lambda_ky_k^2 + \\cdots + \\lambda_ny_n^ 2 \\\\ &amp;\\le \\lambda_ky_k^2 + \\cdots + \\lambda_ky_n^2 \\\\ &amp;\\le \\lambda_k(\\bar{y}^T\\bar{y}) \\\\ &amp;= \\lambda_k \\end{split} \\] It’s easy to see that \\(\\bar{x}^TA\\bar{x}\\) gets its maximum \\(\\lambda_k\\) when \\(y_k = 0\\) and other weights being zero. So the solution \\(\\bar{x}\\) can be solved as \\[ \\begin{split} \\bar{x} &amp;= \\begin{bmatrix} \\bar{q}_1 &amp; \\cdots &amp; \\bar{q}_k &amp; \\bar{q}_{k+1} &amp; \\cdots &amp;\\bar{q}_n \\end{bmatrix} \\begin{bmatrix} 0 \\\\ \\vdots \\\\ \\underbrace{1}_{k\\text{th weight}} \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\\\ &amp;= \\bar{q}_k \\end{split} \\] "],
["singular-value-decomposition.html", "Chapter 5 Singular value decomposition 5.1 Singular values of m x n matrix 5.2 The singular value decomposition 5.3 Matrix norms 5.4 Low rank approximation", " Chapter 5 Singular value decomposition 5.1 Singular values of m x n matrix The singular value decomposition illustrates a way of decomposing any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) into the form \\(U \\Sigma V^T\\), where \\(U = [\\bar{u}_1 \\;\\; \\cdots \\;\\; \\bar{u}_n]\\) and \\(V = [\\bar{v}_1 \\;\\; \\cdots \\;\\; \\bar{v}_n]\\) are both orthogonal matrices, and \\(\\Sigma\\) a diagonal matrix with entries being the square root of the eigenvalues of \\(A^TA\\) (perhaps plus some zeros). Before proceeding to the theorem, let’s explore the motivating idea behind SVD. For (square) diagonalizable matrix \\(A \\in \\mathbb{S}^{n}\\), the absolute value of the eigenvalues measure the amounts that \\(A\\) stretches or shrinks eigenvectors, consider the ratio between the length of \\(\\bar{x}\\) before and after left multiplied by \\(A\\) \\[ \\frac{\\|A\\bar{x}\\|}{\\|\\bar{x}\\|} = \\frac{\\|\\lambda\\bar{x}\\|}{\\|\\bar{x}\\|} = \\frac{\\lambda\\|\\bar{x}\\|}{\\|\\bar{x}\\|} = \\lambda \\] If \\(\\lambda_1\\) is the greatest eigenvalue, then the corresponding eigenvector \\(\\bar{v}_1\\) identifies the direction in which \\(A\\)’s stretching effect is greatest. So, the question is, can we identify a similar ratio and direction for rectangular matrices \\(A \\in \\mathbb{R}^{m \\times n}\\), even though they does not have eigenvalues and eigenvectors? The answer is yes. Note that maximize \\(\\frac{\\|A\\bar{x}\\|}{\\|\\bar{x}\\|}\\) (now \\(\\bar{x}\\) is any vector \\(\\in \\mathbb{R}^n\\)) is equivalent to maximize \\(\\frac{\\|A\\bar{x}\\|^2}{\\|\\bar{x}\\|^2}\\) \\[ \\begin{split} \\frac{\\|A\\bar{x}\\|^2}{\\|\\bar{x}\\|^2} &amp;= \\frac{(A\\bar{x})^T(A\\bar{x})}{\\bar{x}^T\\bar{x}} \\\\ &amp;= \\frac{\\bar{x}^T(A^TA)\\bar{x}}{\\bar{x}^T\\bar{x}} \\end{split} \\] Since \\(A^TA\\) is symmetric, this is the form of a Rayleigh quotients 4.6! We know that the largest possible value is of this quotient \\(\\lambda_1\\), the greatest eigenvalue of \\(A^TA\\), with \\(\\bar{x} = \\bar{v}_1\\), among the orthonormal set \\(\\{\\bar{v}_1, \\cdots, \\bar{v}_n\\}\\). Note that here \\(V = [\\bar{v}_1 \\;\\; \\cdots \\;\\; \\bar{v}_n]\\) is already a orthogonal matrix, previously denoted by \\(Q\\). To sum up, the greatest possible stretching ratio of \\(A \\in \\mathbb{R}^{m \\times n}\\) on a vector \\(\\bar{x} \\in \\mathbb{R}^n\\) is \\(\\sqrt{\\lambda_1}\\). Generally, let \\(\\{\\bar{v}_1, \\cdots, \\bar{v}_n\\}\\) be a orthonormal basis for \\(\\mathbb{R}^n\\) consisting of eigenvectors of \\(A^TA\\), and \\(\\lambda_1, ..., \\lambda_n\\) be the eigenvalues of \\(A^TA\\), for \\(i = 1, \\cdots, n\\) \\[ \\|A\\bar{v}_i\\| ^ 2 = \\bar{v}_i^T(A^TA)\\bar{v}_i = \\lambda_i\\bar{v}_i^T\\bar{v}_i = \\lambda_i \\] From corollary 4.3, we know that \\(A^TA\\) are positive semidefinite matrices. Thus, \\(\\lambda_i \\ge 0, \\, i = 1, ..., n\\), and we can find their square root \\(\\sigma_i = \\sqrt{\\lambda_i}\\). Definition 5.1 (Singular values) The singular values of \\(A\\) are the square roots of the eigenvalues of \\(A^TA\\), denoted by \\(\\sigma_1, ..., \\sigma_n\\). That is, \\(\\sigma_i = \\sqrt{\\lambda_i}\\), and they are often arranged in descending order so that \\(\\lambda_1 \\ge \\cdots \\ge \\lambda_n\\). Geometrically, singular values of \\(A\\) are the length of the vectors \\(A\\bar{v}_1, ..., A\\bar{v}_n\\), where \\(\\{\\bar{v}_1, ..., \\bar{v}_n\\}\\) is the orthonormal basis of \\(A^TA\\)’s eigenspace. Theorem 5.1 Proceeding from previous definitons of singular values, and suppose \\(A\\) has at least one nonzero singular values. Then \\(\\{A\\bar{v}_1, ..., A\\bar{v}_r\\}\\) is an orthogonal basis for \\(\\mathcal{R}(A)\\), and \\(\\text{rank} \\;A = r\\) PROOF First, let’s examine that \\(\\{A\\bar{v}_1, ..., A\\bar{v}_r\\}\\) is a orthogonal basis: any pair of two distinct vectors \\(A\\bar{v}_i, A\\bar{v}_j, \\; i,j = 1, ..., r\\) are orthogonal to each other \\[ \\begin{split} (A\\bar{v}_i)^T(A\\bar{v}_j) &amp;= \\bar{v}_i^TA^TA\\bar{v}_j \\\\ &amp;= \\bar{v}_i^T(\\lambda_j\\bar{v}_j) \\\\ &amp;= 0 \\end{split} \\] Next, we show that any vector in \\(\\mathcal{R}(A)\\) is a linear a combination of \\(\\{A\\bar{v}_1, ..., A\\bar{v}_r\\}\\). Note that \\(\\{\\bar{v}_1, ..., \\bar{v}_n\\}\\) is a orthonormal basis of \\(A^TA\\)’s eigenspace \\(\\mathbb{R}^n\\). Therefore, for any vector \\(\\bar{y} = A\\bar{x}\\) in \\(\\mathcal{R}(A)\\) , there exists \\(\\bar{x} = c_1\\bar{v}_1 + \\cdots + c_n\\bar{v}_n\\), thus \\[ \\begin{split} \\bar{y} &amp;= A\\bar{x} = A(c_1\\bar{v}_1 + \\cdots + c_n\\bar{v}_n) \\\\ &amp;= c_1 A \\bar{v}_1 + \\cdots + c_r A \\bar{v}_r + c_{r+1} A \\bar{v}_{r+1} + \\cdots + c_n A \\bar{v}_n \\end{split} \\tag{1} \\] Since \\(\\lambda_{r+1} = \\cdots = \\lambda_{n} = 0\\), \\(A\\bar{v}_{r+1}, ..., A\\bar{v}_{n}\\) have length \\(0\\): they are zero vectors. And (1) is reduced to \\[ \\bar{y} = c_1 A \\bar{v}_1 + \\cdots + c_r A\\bar{v}_r \\] Thus any \\(\\bar{y} \\in \\mathcal{R}(A)\\) is in Span\\(\\{A\\bar{v}_1, ..., A\\bar{v}_r\\}\\), and \\(\\{A\\bar{v}_1, ..., A\\bar{v}_r\\}\\) is an orthogonal basis for \\(\\mathcal{R}(A)\\). This also shows that the column rank of \\(A\\) is equal to its number of nonzero singular values. 5.2 The singular value decomposition Let’s begin SVD by the \\(m \\times n\\) diagonal matrix \\(\\Sigma\\) of the form \\[ \\Sigma = \\begin{bmatrix} \\sigma_1 \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\sigma_r \\\\ &amp; &amp; &amp; 0 \\\\ &amp; &amp; &amp; &amp; \\ddots \\\\ &amp; &amp; &amp; &amp; &amp; 0 \\\\ \\end{bmatrix} \\tag{1} \\] There are \\(r\\) nonzero entries on the diagonal, being \\(A\\)’s nonzero singular values, and the left positions are filled by \\(0\\) to form a \\(m \\times n\\) matrix. If \\(r\\) equals \\(m\\) or \\(n\\) or both, some or all of the zero blocks do not appear. Theorem 5.2 (The Singular Value Decomposition) Let \\(A \\in \\mathbb{R}^{m \\times n}\\) with rank \\(r\\). There exists an diagonal matrix \\(\\mathbb{\\Sigma} \\in \\mathbb{R}^{m \\times n}\\) as in (1) for which the first \\(r\\ \\times r\\) block is a diagonal matrix with the first \\(r\\) singular values of \\(A\\) on its diagonal, and there exist \\(U \\in \\mathbb{R}^{m \\times m}\\) and \\(V \\in \\mathbb{R}^{n \\times n}\\) such that \\[ A = U \\Sigma V^T \\] PROOF Since \\(A\\) has \\(r\\) nonzero singular values which measure the length of \\(A\\bar{v}_i, \\; i = 1, ...n\\), there exists orthogonal basis \\(\\{A\\bar{v}_1, ..., A\\bar{v}_r\\}\\) for \\(\\mathcal{R}(A)\\), we can further normalize the set to produce the orthonormal set \\(\\bar{u}_1, ..., \\bar{u}_r\\): \\[ \\bar{u}_i = \\frac{A\\bar{v}_i}{\\sigma_i}, \\;\\; i = 1, ..., r \\] Now we can extend \\(\\{\\bar{u}_1, ..., \\bar{u}_r\\}\\) to an orthonormal basis \\(\\{\\bar{u}_1, ..., \\bar{u}_m\\}\\) of \\(\\mathbb{R}^m\\), and let \\[ U = [\\bar{u}_1 \\;\\; \\cdots \\;\\; \\bar{u}_m], \\quad V = [\\bar{v}_1 \\;\\; \\cdots \\;\\; \\bar{v}_n] \\] and \\(\\Sigma\\) be as be as in (1) above. Write out \\[ \\begin{split} U\\Sigma &amp;= [\\bar{u}_1 \\;\\; \\cdots \\;\\; \\bar{u}_r \\;\\; \\cdots \\;\\; \\bar{u}_m]_{m \\times m} \\begin{bmatrix} \\sigma_1 \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\sigma_r \\\\ &amp; &amp; &amp; 0 \\\\ &amp; &amp; &amp; &amp; \\ddots \\\\ &amp; &amp; &amp; &amp; &amp; 0 \\\\ \\end{bmatrix}_{m\\times n} \\\\ &amp;= [\\sigma_1\\bar{u}_1 \\;\\; \\cdots \\;\\; \\sigma_r\\bar{u}_r \\;\\; \\bar{0} \\;\\; \\cdots \\;\\; \\bar{0}] \\\\ &amp; = [A\\bar{v}_1 \\;\\; \\cdots \\;\\; A\\bar{v}_r \\;\\; A\\bar{v}_{r+1} \\;\\; \\cdots \\;\\; A \\bar{v}_n] \\\\ &amp;= A_{m \\times n}V_{n \\times n} \\end{split} \\] And because \\(V\\) is orthogonal \\[ A = U \\Sigma V^{-1} = U \\Sigma V^{T} \\] \\(\\bar{u}_i\\) and \\(\\bar{v}_i\\) are called left eigenvector and right eigenvector of \\(A\\) respectively. It’s easy to verify that the spectral decomposition 4.3.1 is a special case of SVD when \\(A \\in \\mathbb{R}^{n}, \\;\\; m = n\\). In that case, \\(\\Sigma\\) is a square matrix and \\(U\\) is equal to \\(V\\). When \\(\\Sigma\\) contains rows or columns of zeros (i.e, \\(r &lt; \\min(m, n)\\)), we can write SVD in a more compact form. Divide \\(U, \\Sigma, V\\) into submatrices \\[ U = [U_r \\;\\; U_{m-r}], \\quad \\text{where } U_r = [\\bar{u}_1 \\;\\; \\cdots \\;\\; \\bar{u}_r] \\\\ V = [V_r \\;\\; V_{m-r}], \\quad \\text{where } V_r = [\\bar{v}_1 \\;\\; \\cdots \\;\\; \\bar{v}_r] \\\\ \\Sigma = \\begin{bmatrix} D &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} \\quad \\text{where } D = \\begin{bmatrix} \\lambda_1 \\\\ &amp; \\ddots \\\\ &amp; &amp; \\lambda_r \\end{bmatrix} \\] The partitioned matrix multiplication shows that \\[ A = [U_r \\;\\; U_{m-r}] \\begin{bmatrix} D &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} \\begin{bmatrix} V_r^T \\\\ V_{n-r}^T \\end{bmatrix} = U_rDV_{r}^T \\] This more compact form is called a reduced singular value decomposition. Another way to write this is \\[ A = \\sum_{i=1}^{t}{\\sigma_i}\\bar{u}_i\\bar{v}_i \\] Right multiply the non-compact form \\(A = U\\Sigma V^T\\) by \\(A^T\\) , we get the spectral decomposition of symmetric matrix \\(AA^T\\). \\[ AA^T = (U \\Sigma V^T)(U \\Sigma V^T)^T = U \\Sigma \\Sigma^T VV^TU^T = U (\\Sigma\\Sigma^T) U^T \\tag{1} \\] Therefore, \\([\\bar{u}_1 \\;\\; \\cdots \\;\\; \\bar{u}_n]\\) are revealed as the orthonormal basis for \\(AA^T\\)’s eigenspace, as \\([\\bar{v}_1 \\;\\; \\cdots \\;\\; \\bar{v}_n]\\) are for \\(A^TA\\). Formula (1) echoes the fact that \\(A^TA\\) and \\(AA^T\\) have the same set of nonzero eigenvalues, because \\(\\Sigma\\Sigma^T\\) produces nonzero set \\(\\lambda_1, ..., \\lambda_r\\). In fact, if were to ask for a direction in which \\(A^T\\) has its greatest stretching effect instead of \\(A\\), we would still result in the equivalent decomposition \\(A^T = V\\Sigma U^T\\), with \\(\\bar{v}_i = \\frac{A\\bar{u}_i}{\\sigma_i}\\). It’s also easy to test that \\(\\{A\\bar{u}_1, ..., A\\bar{u}_r\\}\\) produces an orthogonal basis for \\(\\mathcal{R}(A^T)\\) or \\(\\mathcal{R}(A)\\). The process is analogous to theorem 5.1 where \\(\\{A\\bar{v}_1, ..., A\\bar{v}_r\\}\\) are shown to span \\(\\mathcal{R}(A)\\). For any vector \\(\\bar{y}\\) in \\(\\mathcal{R}(A)\\), we have \\[ \\begin{align*} \\bar{y} &amp;= A^T\\bar{x} \\\\ &amp;= A^T(c_1\\bar{u}_1 + \\cdots + c_1\\bar{u}_n) \\\\ &amp;= c_1A\\bar{u}_1 + \\cdots + c_rA\\bar{u}_r + \\bar{0} + \\cdots + \\bar{0} &amp;&amp; (\\text{because }A\\bar{u}_i = \\sigma_i\\bar{v}_i)\\\\ &amp;= c_1A\\bar{u}_1 + \\cdots + c_rA\\bar{u}_r \\end{align*} \\] Thus, SVD can be thought of an connection between two spectral decomposition \\[ A^TA = V (\\Sigma^T\\Sigma)V^T \\\\ AA^T = U (\\Sigma\\Sigma^T) U^T \\] This shed light on the relationship between SVD and the fundamental theorem of linear algebra 2.1 Subspace Columns \\(\\mathcal{R}(A)\\) the first \\(r\\) columns of \\(U\\) \\(\\mathcal{R}(A^T)\\) the first \\(r\\) columns of \\(V\\) \\(\\mathcal{N}(A)\\) the last \\(n - r\\) columns of \\(V\\) \\(\\mathcal{N}(A^T)\\) the last \\(m - r\\) columns of \\(U\\) 5.3 Matrix norms Let \\(A\\) and \\(B\\) be matrices conformable for the operations below, a matrix norm should at first satisfy 3 axioms of norm: \\(\\|A\\| \\ge 0\\) for all \\(x \\in X\\), with equality if and only if all elements of \\(A\\) is zero (nonnegative) \\(\\|\\alpha A\\| = |\\alpha|\\,\\|A\\|\\) (homogeneous) \\(\\|A + B\\| &lt; \\|A\\| + \\|B\\|\\) (triangular inequality) Additionally, in the case of square matrices (thus, m = n), some (but not all) matrix norms satisfy the following condition, which is related to the fact that matrices are more than just vectors \\(\\|AB\\| &lt; \\|A\\|\\,\\|B\\|\\) for \\(A, B \\in \\mathbb{R}^{n \\times n}\\) A matrix norm that satisfies this additional property is called a submultiplicative norm. There are 2 main categories of matrix norms. induced norms (defined in terms of vector norms) entry-wise norms (treat \\(A_{m \\times n}\\) like a long vector with \\(m \\times n\\) elements) 5.3.1 Induced norms Induced norms define matrix norms in terms of vectors, also called operator norm since \\(A\\) acts like an operator in this definition. Note that matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) maps a vector \\(\\bar{x} \\not = 0\\) from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\). In particular, if the p-norm is used for both \\(\\mathbb{R}^n\\) and \\(\\mathbb{R}^m\\), then the induced norm is \\[ \\|A\\|_p = \\max \\frac{\\|A\\bar{x}\\|_p}{\\|\\bar{x}\\|_p} \\] The subscript \\(p\\) can be misleading, because the appropriate name for this matrix norm may not be “p-norm”, but rather “induced norm when p-norm is used in both spaces”. The p-norm of a matrix meaning usually means entry-wise p-norms or the Scatten p-norms defined in subsequent sections. In the special cases where \\(p = 1, 2, ..., \\infty\\), \\(\\|A\\|_p\\) is the maximum absolute column sums, largest singular value, and the maximum absolute row sums \\[ \\begin{aligned} \\|A\\|_1 &amp;= \\max \\sum_{i=1}^{m}{|A_{ij}|} \\\\ \\|A\\|_2 &amp;= \\sigma_1 \\\\ \\|A\\|_{\\infty} &amp;= \\max \\sum_{j=1}^{n}{|A_{ij}|} \\end{aligned} \\] For symmetric matrix A, we have \\[ \\|A\\|_1 = \\|A\\|_{\\infty} \\] and \\[ \\|A\\|_2 = \\lambda_1 \\] The induced 2-norm are also called the spectral norm. By definition, the following inequality holds for induced matrix norms \\[ \\|A\\bar{x}\\|_P \\le \\|A\\|_p\\|\\bar{x}\\|_p \\] Proposition 5.1 Induced matrix norms satisfies the additional submultiplicative property in that \\[ \\|AB\\|_p \\le \\|A\\|_p\\|B\\|_p \\] PROOF For any \\(\\bar{x} \\in \\mathbb{R}^n\\) \\[ \\|AB\\bar{x}\\|_p \\le \\|A\\|_p\\|B\\bar{x}\\|_p \\le \\|A\\|_p\\|B\\|_p\\|\\bar{x}\\|_p \\] si \\[ \\|AB\\|_p = \\max \\frac{\\|A\\bar{x}\\|_p}{\\|\\bar{x}\\|_p} \\le \\max \\frac{\\|A\\|_p\\|B\\|_p\\|\\bar{x}\\|_p}{\\|\\bar{x}\\|_p} = \\|A\\|_p\\|B\\|_p \\] 5.3.2 Entry-wise norm Entry-wise norms treat an \\(m \\times n\\) matrix as a long vector of size \\(m \\times n\\), denoted by \\(\\text{vec}(A)\\). For example, using the p-norm for vectors, we get \\[ \\|A\\|_{p,p} = \\|\\text{vec}(A)\\|_p = \\Bigg(\\sum_{j=1}^n\\sum_{i=1}^{m}{|A_{ij}|}^p \\Bigg)^{\\frac{1}{p}} \\] More generally, the p,q norm is defined by \\[ \\|A\\|_{p, q} = \\Bigg ( \\sum_{j=1}^n \\Big (\\sum_{i=1}^{n}{|A_{ij|}}^p \\Big)^{\\frac{q}{p}} \\Bigg)^{\\frac{1}{q}} \\] Another important member norm of this norm family is the Frobenius norm, or the F-norm. \\[ \\| A\\|_F = \\sqrt{\\sum_{j=1}^n\\sum_{j=1}^{m}{A_{ij}^2}} = \\sqrt{\\text{tr}(A^TA)} = \\sqrt{\\sum_{i=1}^{\\min(m,n)}{\\sigma_i^2}} \\] where \\(\\sigma_i\\) is the nonzero singular value of \\(A\\). Proposition 5.2 The F-norm is a submultiplicative norm. PROOF Let \\(A\\) and \\(B\\) are of appropriate size such that \\[ A = \\begin{bmatrix} a_1^T \\\\ \\vdots \\\\ a_m^T \\end{bmatrix} , \\; B = \\begin{bmatrix} b_1 &amp; \\cdots &amp; b_m \\end{bmatrix} \\\\ \\quad \\\\ \\begin{aligned} \\|AB\\| &amp;= \\sqrt{\\sum_{i, j}{|a_i^Tb_j|^2}} \\\\ &amp; \\le \\sqrt{\\sum_{i, j}{\\| a_i \\|^2 \\| b_j\\|^2}} \\\\ &amp;= \\sqrt{\\sum_{i}{\\|a_i\\|^2}} \\sqrt{\\sum_{j}{\\|b_j\\|^2}} \\\\ &amp;= \\|A\\| \\|B\\| \\end{aligned} \\] The first inequality comes from the Cauchy-Schwarz inequality \\(a \\cdot b \\le \\|a\\| \\|b\\|\\) 5.3.3 Other matrix norms One can pick to one of two ways to generalize the F-norm. One is a generalization of the direct definition that F-norm is the 2-norm of \\(||\\text{vec}()||\\). For \\(p \\ge 1\\), it is the Frobenius-p norm: \\[ \\| A \\|_{F_p} = \\Bigg (\\sum_{i,j}{|A_{ij}|^p} \\Bigg)^{\\frac{1}{p}} \\] The Frobenius p-norm is the ordinary Frobenius norm. Another generalization stems from the relationship between F-norm and singular values of \\(A\\). The Schatten p norm or nuclear p-norm is the p-norm of the vector composed of \\(A\\)’s singular values \\[ \\|A\\|_{S_p} = \\Big( \\sum_{i = 1}^{\\min(m, n)}{\\sigma_i^p}\\Big)^{\\frac{1}{p}} \\] 5.3.4 Unitary invariancy Definition 5.2 (unitary invariant) A matrix norm \\(\\|\\cdot \\|\\) is said to be unitary invariant if for all orthogonal matrices \\(U\\) and \\(V\\) of appropriate size \\[ \\|A\\| = \\|UAV\\| \\] (unitary matrices refers to orthogonal matrices with complex-valued entries, but I focus on real matrices here.) Essentially, if a norm depends only on the singular values of a matrix, it is unitary invariant. Since for such norms: \\[ \\|A\\| = \\|\\Sigma\\| \\] Multiply two orthogonal matrices \\(U\\) and \\(V^T\\) on each side, \\[ \\begin{aligned} \\|UA V^T\\| &amp; = \\| U \\Sigma V^T\\| \\\\ \\|UAV^T\\| &amp;= \\|A\\| \\end{aligned} \\] Does this work for all orthogonal matrices \\(U\\) and \\(V\\)? The spectral norm (induced p-norm), F-norm and Schattan norm are all unitary invariant. 5.4 Low rank approximation "],
["solutions-of-linear-system-ax-b.html", "Chapter 6 Solutions of linear system Ax = b 6.1 Generalized inverse 6.2 Ill-conditioned matrices", " Chapter 6 Solutions of linear system Ax = b A linear system \\(A\\bar{x} = \\bar{b}\\) when \\(\\bar{b} \\not = \\bar{0}\\) may behave in any one of three possible ways: The system has no solution. This occurs when \\(\\bar{b} \\not \\in \\mathcal{R}(A)\\). In such cases we try to find best fits that meets certain optimization requirements. The system has a single unique solution. This occurs when \\(\\bar{b} \\in \\mathcal{R}(A)\\), and \\(A\\) has linearly independent columns, then there exists an unique linear combination \\(a_1x_1 + \\cdots + a_nx_n = \\bar{b}\\). In the special case when \\(A\\) is square, the solution is simply \\(\\bar{x} = A^{-1}\\bar{b}\\) The system has infinitely many solutions. This is the case when \\(\\bar{b} \\in \\mathcal{R}(A)\\) and columns of \\(A\\) are linearly dependent, with free columns. Any linear combination of current solutions would still satisfy \\(A\\bar{x} = \\bar{b}\\) In the first scenario, the linear system is called inconsistent because there is no \\(\\bar{x}\\) that satisfies each equation in the system. Linear systems in the second and third scenario are called consistent because there is at least one solution. 6.1 Generalized inverse 6.2 Ill-conditioned matrices 6.2.1 The condition number \\[ \\delta x \\] "],
["taylor-series-and-expansion.html", "Chapter 7 Taylor series and expansion", " Chapter 7 Taylor series and expansion "],
["matrix-calculus.html", "Chapter 8 Matrix calculus 8.1 Useful identities in matirx calculus", " Chapter 8 Matrix calculus Matrix calculus refers to a number of different notations that use matrices and vectors to collect the derivative of each component of the dependent variable with respect to each component of the independent variable. \\[ \\frac{\\partial{f(x)}}{\\partial{x}} \\] Differentiate a scalar by a vector. \\[ \\frac{\\partial f(x_1, ..., x_n)}{\\partial (x_1, ..., x_n)} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} \\] One important thing here is we define the gradient of \\(f(x_1, ..., x_n)\\) to be a row vector. This is also called the numerator layout, as opposed the denominator layout where gradient is defined as a row vector. Next, we come to cases when we differentiate a vector by a vector (i.e., the gradient of a vector-valued function). The function \\(\\bar{f}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) takes a \\(n\\)-dimensional vector \\(\\bar{x} = [x_1, ..., x_n]\\) as input, and output a \\(m\\)-dimensional vector. The corresponding vector of function value is given as \\[ \\bar{f}(\\bar{x}) = \\begin{bmatrix} f_1(\\bar{x}) \\\\ \\vdots \\\\ f_m(\\bar{x}) \\end{bmatrix} \\] In this way, we think of a vector-valued function \\(\\bar{f}\\), as a vector of functions \\([f_1, ..., f_m]^T, \\; f_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\). Thus, the partial derivative of \\(f_i\\) with respect vector of \\(\\bar{x}\\) is a gradient \\[ \\frac{\\partial f_i}{\\partial \\bar{x}} = \\begin{bmatrix} \\frac{\\partial f_i}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_i}{\\partial x_n} \\end{bmatrix} \\] Further, the partial derivative of \\(\\bar{f}\\) with respect to scalar \\(x_i, \\; i = 1,...,n\\) is simply a column vector of partial derivatives \\(\\frac{\\partial \\bar{f}_j}{\\partial x_i}\\) \\[ \\frac{\\partial \\bar{f}(\\bar{x})}{\\partial x_i} = \\begin{bmatrix} \\frac{\\partial f_1 }{\\partial x_i} \\\\ \\vdots \\\\ \\frac{\\partial f_m }{\\partial x_i} \\end{bmatrix} = \\begin{bmatrix} \\lim _{h \\to \\infty}\\frac{f_1(x_1, ..., x_i + h, ..., x_n)}{h} \\\\ \\vdots \\\\ \\lim _{h \\to \\infty}\\frac{f_m(x_1, ..., x_i + h, ..., x_n)}{h} \\end{bmatrix} \\] We obtain the “full” gradient of \\(\\bar{f}\\) with respect of \\(\\bar{x}\\). Definition 8.1 (Jacobian) \\[ \\begin{split} J = \\nabla_x \\bar{f} &amp;= \\begin{bmatrix} \\frac{\\partial \\bar{f}}{\\partial x_1} &amp; \\cdots &amp;\\frac{\\partial \\bar{f}}{\\partial x_n} \\end{bmatrix} &amp;= \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots &amp; &amp; \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\end{split} \\] 8.1 Useful identities in matirx calculus Assuming \\(A\\) is a \\(m \\times n\\) matrix and \\(\\bar{w}\\) an \\(n \\times 1\\) column vector, Derivative of with respect to output size result \\(A\\bar{w}\\) \\(\\bar{w}\\) matrix \\(m \\times n\\) \\(A\\) \\(\\bar{a}^T\\bar{w}\\) \\(\\bar{w}^T\\bar{a}\\) \\(\\bar{w}\\) row vector \\(1 \\times n\\) \\(\\bar{a}^T\\) \\(\\bar{b}^TB\\bar{w}\\) \\(\\bar{w}^TB^T\\bar{b}\\) \\(\\bar{w}\\) row vector \\(n \\times 1\\) \\(\\bar{b}^TB\\) \\(\\bar{w}^TA\\bar{w}\\) \\(\\bar{w}\\) row vector \\(1 \\times n\\) \\(\\bar{w}^T(A + A^T)\\) if \\(A\\) is symmetric \\(2\\bar{w}^TA\\) "],
["infinite-sequences-and-series.html", "Chapter 9 Infinite sequences and series", " Chapter 9 Infinite sequences and series "],
["probability-basics.html", "Chapter 10 Probability basics 10.1 Probabilty space 10.2 Counting 10.3 Conditional probability 10.4 Random variables 10.5 Properties of expectation, variance, and covariance 10.6 Conditional expectation", " Chapter 10 Probability basics The probability section should be rather applied, without delving into deeper branch of analysis and measure theory. 10.1 Probabilty space A probability space is a triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) where \\(\\Omega\\) is the set of all possible events in an experiment, called the sample space \\(\\mathcal{F}\\) is a subset of \\(\\Omega\\), containing some of the events usually denoted by \\(A\\). Events may be a singleton set \\({\\omega}\\) or contain more than one sample point. The complement of event \\(A\\) is another event, \\(A^c\\). As we can only define probabilities for such subsets of \\(\\Omega\\) that are tied together like a family, \\(\\mathcal{F}\\) is assumed to be a \\(\\sigma\\)-field which satisfy If \\(A \\in \\mathcal{F}\\) then \\(A^c \\in \\mathcal{F}\\) If \\(A_i \\in \\mathcal{F}, \\;i = 1,...,n\\) is a countable sequence of sets, then \\(A_1 \\cup A_2 \\cup ... \\cup A_n \\in \\mathcal{F}\\) \\(\\mathbb{P}\\) is a function that assigns probabilities to events: \\(\\mathbb{P}: \\mathcal{F} \\rightarrow [0 , 1]\\). Axioms of probability are concerned with specific properties of the function \\(\\mathbb{P}\\) $(A) 0 $ for all \\(A \\in \\mathcal{F}\\) \\(\\mathbb{P}(\\Omega) = 1\\) and \\(\\mathbb{P(\\emptyset)} = 0\\) If \\(A_1, ..., A_n\\) are disjoint sets (\\(A_i \\cup A_j = \\emptyset\\)) 10.2 Counting Additionally, there are several useful identities in combination problems. The formal algebraic proof can be cumbersome, instead, here we draw on story proofs from the “Introduction to Probability” book (K. Blitzstein and Hwang 2019). Proposition 10.1 (Vandermonde’s identity) \\[ \\binom{m + n}{k} = \\sum_{j = 0}^{k}\\binom{m}{j} \\binom{n}{k -j} \\] Suppose we want to select a size \\(k\\) committee out of \\(m\\) juniors and \\(n\\) seniors. The left side shows there are obviously \\(\\binom{m + n}{k}\\) probabilities. On the other side of the story, given there are \\(j\\) juniors in the committee, there must be \\(k - j\\) seniors in the committee. The right-hand side sums up the cases varying \\(k\\). Proposition 10.2 (Partnerships Problem) \\[ \\frac{(2n)!}{2^n \\cdot n!} = (2n -1 ) \\cdot (2n -3) \\cdots 3 \\cdot 1 \\] Both side of the identity count the ways to break \\(2n\\) people into \\(n\\) partnerships. According to the left side, we can form partnerships by lining up all \\(2n\\) people and making the first and the second a pair, then the third and the fourth a pair, etc. This overcounts by a factor \\(2^n \\cdot n!\\) since the order of partnerships does not matter, nor does the order inside each pair. Alternatively, there are \\(2n - 1\\) choices for people 1, \\(2n - 3\\) choices for people 2 (or person 3 if people 2 is paired to people 1), and so on. 10.3 Conditional probability Definition 5.1 (Conditional probability) 10.4 Random variables 10.5 Properties of expectation, variance, and covariance 10.6 Conditional expectation "],
["moments.html", "Chapter 11 Moments", " Chapter 11 Moments "],
["univariate-distributions.html", "Chapter 12 Univariate distributions 12.1 Uniform distribution", " Chapter 12 Univariate distributions 12.1 Uniform distribution \\[ f_X(x) = \\begin{cases} \\frac{1}{a - b} &amp; a \\le x \\le b \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] "],
["multivariate-distributions.html", "Chapter 13 Multivariate distributions", " Chapter 13 Multivariate distributions "],
["basics-of-optimization.html", "Chapter 14 Basics of optimization 14.1 Univariate optimization 14.2 Multivariate optimization 14.3 Lagrange multipliers", " Chapter 14 Basics of optimization 14.1 Univariate optimization Theorem 14.1 (Second derivative test for local extrema) Suppose \\(f&#39;&#39;(x)\\) is continuous on an open interval that contains \\(x = c\\) If \\(f&#39;(c) = 0\\) and \\(f&#39;&#39;(c) &gt; 0\\), then \\(f(x)\\) has a local minimum at \\(x = c\\) If \\(f&#39;(c) = 0\\) and \\(f&#39;&#39;(c) &lt; 0\\), then \\(f(x)\\) has a local maximum at \\(x = c\\) If \\(f&#39;(c) = 0\\) and \\(f&#39;&#39;(c) = 0\\), the the test fails. \\(f(x)\\) my have a local maximum, a local minimum, or neither 14.2 Multivariate optimization 14.3 Lagrange multipliers "],
["gradient-descent.html", "Chapter 15 Gradient descent", " Chapter 15 Gradient descent \\[ f(x + \\ksei ) \\] "],
["linear-models.html", "Chapter 16 Linear models 16.1 Ordinary Least Squares 16.2 Weighted least squares 16.3 Partial least squres 16.4 Regularized regression", " Chapter 16 Linear models 16.1 Ordinary Least Squares 16.1.1 Least square estimation From theorem 3.3 we know that \\[ \\bar{\\beta} = (X^TX)^{-1}X^T\\bar{y} \\] thus: \\[ \\hat{\\bar{y}} = X\\bar{\\beta} = X (X^TX)^{-1}X^{T}\\bar{y} \\] When columns of the design matrix \\(A\\) are orthogonal, the orthogonal projection of \\(\\bar{y}\\) onto \\(\\mathcal{R}(X)\\) is given by \\[ \\hat{\\bar{y}} = \\frac{\\bar{x}_0^T\\bar{y}}{\\bar{x}_0^T\\bar{x}_0}\\bar{x}_0 + \\frac{\\bar{x}_1^T\\bar{y}}{\\bar{x}_1^T\\bar{x}_1}\\bar{x}_1 \\cdots + \\frac{\\bar{x}_p^T\\bar{y}}{\\bar{x}_p^T\\bar{x}_p}\\bar{x}_p \\] ### Maximum likelihood estimation The log likelihood function is given by 16.2 Weighted least squares 16.3 Partial least squres 16.4 Regularized regression 16.4.1 Ridge regression 16.4.2 Lasso regression 16.4.3 Elastic net regression "],
["principal-component-analysis.html", "Chapter 17 Principal component analysis", " Chapter 17 Principal component analysis "],
["references.html", "References", " References Aggarwal, Charu C. 2020. Linear Algebra and Optimization for Machine Learning - A Textbook. Springer. https://doi.org/10.1007/978-3-030-40344-7. Bertsekas, D. P., and J. N. Tsitsiklis. 2008. Introduction to Probability. Athena Scientific Optimization and Computation Series. Athena Scientific. https://books.google.com.hk/books?id=yAy-PQAACAAJ. DasGupta, Anirban. 2011. Probability for Statistics and Machine Learning: Fundamentals and Advanced Topics. 1st ed. Springer Publishing Company, Incorporated. Deisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020. Mathematics for Machine Learning. Gentle, James E. 2017. Matrix Algebra: Theory, Computations, and Applications in Statistics. 2bd ed. Springer Publishing Company, Incorporated. K. Blitzstein, Joseph, and Jessica Hwang. 2019. Introduction to Probability Second Edition. https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view. Lay, David. 2006. Linear Algebra and Its Applications. Vols. 3:CD-ROM. Pearson, Addison Wesley. Strang, Gilbert. 2006. Linear Algebra and Its Applications. Belmont, CA: Thomson, Brooks/Cole. http://www.amazon.com/Linear-Algebra-Its-Applications-Edition/dp/0030105676. Xie, Yihui. 2020. Bookdown: Authoring Books and Technical Documents with R Markdown. https://CRAN.R-project.org/package=bookdown. "]
]
