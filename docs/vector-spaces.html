<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Vector Spaces | Mathematical Notes for Machine Learning</title>
  <meta name="description" content="Chapter 2 Vector Spaces | Mathematical Notes for Machine Learning" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Vector Spaces | Mathematical Notes for Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="enixam/math-foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Vector Spaces | Mathematical Notes for Machine Learning" />
  
  
  

<meta name="author" content="Qiushi Yan" />


<meta name="date" content="2020-09-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="basic-matrix-algebra.html"/>
<link rel="next" href="orthogonality.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Math Notes for Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Linear Algebra</b></span></li>
<li class="chapter" data-level="1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html"><i class="fa fa-check"></i><b>1</b> Basic Matrix Algebra</a><ul>
<li class="chapter" data-level="1.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.1</b> Matrix Multiplication</a><ul>
<li class="chapter" data-level="1.1.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#geometric-transformations"><i class="fa fa-check"></i><b>1.1.1</b> Geometric Transformations</a></li>
<li class="chapter" data-level="1.1.2" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#matrix-multiplication-as-linear-transformation"><i class="fa fa-check"></i><b>1.1.2</b> Matrix Multiplication as Linear Transformation</a></li>
<li class="chapter" data-level="1.1.3" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#selector-matrix"><i class="fa fa-check"></i><b>1.1.3</b> Selector Matrix</a></li>
<li class="chapter" data-level="1.1.4" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#discrete-convolution"><i class="fa fa-check"></i><b>1.1.4</b> Discrete Convolution</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#elemetary-matrix-and-row-operations"><i class="fa fa-check"></i><b>1.2</b> Elemetary matrix and row operations</a></li>
<li class="chapter" data-level="1.3" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#lu-factorization"><i class="fa fa-check"></i><b>1.3</b> LU Factorization</a></li>
<li class="chapter" data-level="1.4" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#determinants"><i class="fa fa-check"></i><b>1.4</b> Determinants</a><ul>
<li class="chapter" data-level="1.4.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#cofactor-expansion"><i class="fa fa-check"></i><b>1.4.1</b> Cofactor Expansion</a></li>
<li class="chapter" data-level="1.4.2" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#geometric-interpretation-of-determinant"><i class="fa fa-check"></i><b>1.4.2</b> Geometric Interpretation of Determinant</a></li>
<li class="chapter" data-level="1.4.3" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#properties-of-determinant"><i class="fa fa-check"></i><b>1.4.3</b> Properties of Determinant</a></li>
<li class="chapter" data-level="1.4.4" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#cramers-rule"><i class="fa fa-check"></i><b>1.4.4</b> Cramer’s Rule</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#trace"><i class="fa fa-check"></i><b>1.5</b> Trace</a></li>
<li class="chapter" data-level="1.6" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#matrix-inversion"><i class="fa fa-check"></i><b>1.6</b> Matrix Inversion</a><ul>
<li class="chapter" data-level="1.6.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#the-matrix-inversion-lemma"><i class="fa fa-check"></i><b>1.6.1</b> The Matrix Inversion Lemma</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#complexity-of-matrix-computation"><i class="fa fa-check"></i><b>1.7</b> Complexity of Matrix Computation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="vector-spaces.html"><a href="vector-spaces.html"><i class="fa fa-check"></i><b>2</b> Vector Spaces</a><ul>
<li class="chapter" data-level="2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#vector-space"><i class="fa fa-check"></i><b>2.1</b> Vector Space</a><ul>
<li class="chapter" data-level="2.1.1" data-path="vector-spaces.html"><a href="vector-spaces.html#euclidean-space"><i class="fa fa-check"></i><b>2.1.1</b> Euclidean Space</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="vector-spaces.html"><a href="vector-spaces.html#metric-spaces-normed-spaces-inner-product-spaces"><i class="fa fa-check"></i><b>2.2</b> Metric Spaces, Normed Spaces, Inner Product Spaces</a><ul>
<li class="chapter" data-level="2.2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#metric-and-norm"><i class="fa fa-check"></i><b>2.2.1</b> Metric and Norm</a></li>
<li class="chapter" data-level="2.2.2" data-path="vector-spaces.html"><a href="vector-spaces.html#inner-produc-outer-product-cross-product"><i class="fa fa-check"></i><b>2.2.2</b> Inner Produc, Outer Product, Cross Product</a></li>
<li class="chapter" data-level="2.2.3" data-path="vector-spaces.html"><a href="vector-spaces.html#restricted-definition-of-inner-products-in-rn"><i class="fa fa-check"></i><b>2.2.3</b> Restricted Definition of Inner Products in <span class="math inline">\(R^n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="vector-spaces.html"><a href="vector-spaces.html#subspaces"><i class="fa fa-check"></i><b>2.3</b> Subspaces</a></li>
<li class="chapter" data-level="2.4" data-path="vector-spaces.html"><a href="vector-spaces.html#fundamental-theorem"><i class="fa fa-check"></i><b>2.4</b> Fundamental Theorem of Linear Algebra</a></li>
<li class="chapter" data-level="2.5" data-path="vector-spaces.html"><a href="vector-spaces.html#rank"><i class="fa fa-check"></i><b>2.5</b> Rank</a><ul>
<li class="chapter" data-level="2.5.1" data-path="vector-spaces.html"><a href="vector-spaces.html#effect-of-operations-on-matrix-rank"><i class="fa fa-check"></i><b>2.5.1</b> Effect of Operations on Matrix Rank</a></li>
<li class="chapter" data-level="2.5.2" data-path="vector-spaces.html"><a href="vector-spaces.html#gram-matrix"><i class="fa fa-check"></i><b>2.5.2</b> Gram Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="vector-spaces.html"><a href="vector-spaces.html#bases-and-coordinate-systems"><i class="fa fa-check"></i><b>2.6</b> Bases and Coordinate Systems</a><ul>
<li class="chapter" data-level="2.6.1" data-path="vector-spaces.html"><a href="vector-spaces.html#change-of-basis"><i class="fa fa-check"></i><b>2.6.1</b> Change of Basis</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="vector-spaces.html"><a href="vector-spaces.html#complexity-of-vector-computations"><i class="fa fa-check"></i><b>2.7</b> Complexity of Vector Computations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="orthogonality.html"><a href="orthogonality.html"><i class="fa fa-check"></i><b>3</b> Orthogonality</a><ul>
<li class="chapter" data-level="3.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition"><i class="fa fa-check"></i><b>3.1</b> Orthogonal Decomposition</a><ul>
<li class="chapter" data-level="3.1.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-complements"><i class="fa fa-check"></i><b>3.1.1</b> Orthogonal Complements</a></li>
<li class="chapter" data-level="3.1.2" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-sets-and-orthogonal-basis"><i class="fa fa-check"></i><b>3.1.2</b> Orthogonal Sets and Orthogonal Basis</a></li>
<li class="chapter" data-level="3.1.3" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition-1"><i class="fa fa-check"></i><b>3.1.3</b> Orthogonal Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="orthogonality.html"><a href="orthogonality.html#orthonormal-sets-and-orthogonal-matrices"><i class="fa fa-check"></i><b>3.2</b> Orthonormal Sets and Orthogonal Matrices</a><ul>
<li class="chapter" data-level="3.2.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-matrices"><i class="fa fa-check"></i><b>3.2.1</b> Orthogonal Matrices</a></li>
<li class="chapter" data-level="3.2.2" data-path="orthogonality.html"><a href="orthogonality.html#best-approximation"><i class="fa fa-check"></i><b>3.2.2</b> Best Approximation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="orthogonality.html"><a href="orthogonality.html#projection-and-idempotent-matrices"><i class="fa fa-check"></i><b>3.3</b> Projection and idempotent matrices</a></li>
<li class="chapter" data-level="3.4" data-path="orthogonality.html"><a href="orthogonality.html#gram-schmidt-process"><i class="fa fa-check"></i><b>3.4</b> Gram-Schmidt Process</a></li>
<li class="chapter" data-level="3.5" data-path="orthogonality.html"><a href="orthogonality.html#qr-factorizaiton"><i class="fa fa-check"></i><b>3.5</b> QR Factorizaiton</a></li>
<li class="chapter" data-level="3.6" data-path="orthogonality.html"><a href="orthogonality.html#complexity"><i class="fa fa-check"></i><b>3.6</b> Complexity</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html"><i class="fa fa-check"></i><b>4</b> Eigenthings and Quadratic Forms</a><ul>
<li class="chapter" data-level="4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>4.1</b> Eigenvectors and Eigenvalues</a><ul>
<li class="chapter" data-level="4.1.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#additional-properties-of-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>4.1.1</b> Additional Properties of Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="4.1.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#left-eigenvectors-and-right-eigenvectors"><i class="fa fa-check"></i><b>4.1.2</b> Left Eigenvectors and Right Eigenvectors</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#diagnolization-and-similar-matrices"><i class="fa fa-check"></i><b>4.2</b> Diagnolization and Similar Matrices</a><ul>
<li class="chapter" data-level="4.2.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#similarity"><i class="fa fa-check"></i><b>4.2.1</b> Similarity</a></li>
<li class="chapter" data-level="4.2.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#jordan-matrix"><i class="fa fa-check"></i><b>4.2.2</b> Jordan Matrix</a></li>
<li class="chapter" data-level="4.2.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#simultaneous-diagonalization"><i class="fa fa-check"></i><b>4.2.3</b> Simultaneous Diagonalization</a></li>
<li class="chapter" data-level="4.2.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>4.2.4</b> Cayley-Hamilton Theorem</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#symmetric-matrices"><i class="fa fa-check"></i><b>4.3</b> Symmetric Matrices</a><ul>
<li class="chapter" data-level="4.3.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#spectral-decomposition"><i class="fa fa-check"></i><b>4.3.1</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="4.3.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#a-orthogonality"><i class="fa fa-check"></i><b>4.3.2</b> A-Orthogonality</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#quadratic-forms"><i class="fa fa-check"></i><b>4.4</b> Quadratic Forms</a><ul>
<li class="chapter" data-level="4.4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#change-of-variable"><i class="fa fa-check"></i><b>4.4.1</b> Change of Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#classification-of-quadratic-forms"><i class="fa fa-check"></i><b>4.4.2</b> Classification of Quadratic Forms</a></li>
<li class="chapter" data-level="4.4.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#gershgorin-discs-and-diagonal-dominance"><i class="fa fa-check"></i><b>4.4.3</b> Gershgorin Discs and Diagonal Dominance</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#cholesky-factorization"><i class="fa fa-check"></i><b>4.5</b> Cholesky Factorization</a></li>
<li class="chapter" data-level="4.6" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#rayleigh-quotients"><i class="fa fa-check"></i><b>4.6</b> Rayleigh Quotients</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html"><i class="fa fa-check"></i><b>5</b> Singular Value Decomposition</a><ul>
<li class="chapter" data-level="5.1" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#singular-values"><i class="fa fa-check"></i><b>5.1</b> Singular Values</a></li>
<li class="chapter" data-level="5.2" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#svd-theorem"><i class="fa fa-check"></i><b>5.2</b> SVD</a></li>
<li class="chapter" data-level="5.3" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#matrix-norms"><i class="fa fa-check"></i><b>5.3</b> Matrix Norms</a><ul>
<li class="chapter" data-level="5.3.1" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#induced-norms"><i class="fa fa-check"></i><b>5.3.1</b> Induced Norms</a></li>
<li class="chapter" data-level="5.3.2" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#entry-wise-norm"><i class="fa fa-check"></i><b>5.3.2</b> Entry-wise Norm</a></li>
<li class="chapter" data-level="5.3.3" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#other-matrix-norms"><i class="fa fa-check"></i><b>5.3.3</b> Other Matrix Norms</a></li>
<li class="chapter" data-level="5.3.4" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#unitary-invariant-norms"><i class="fa fa-check"></i><b>5.3.4</b> Unitary Invariant Norms</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#low-rank-approximation"><i class="fa fa-check"></i><b>5.4</b> Low Rank Approximation</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-system.html"><a href="linear-system.html"><i class="fa fa-check"></i><b>6</b> Solutions of Linear System Ax = b</a><ul>
<li class="chapter" data-level="6.1" data-path="linear-system.html"><a href="linear-system.html#lesat-squares-problems"><i class="fa fa-check"></i><b>6.1</b> Lesat Squares Problems</a></li>
<li class="chapter" data-level="6.2" data-path="linear-system.html"><a href="linear-system.html#generalized-inverse"><i class="fa fa-check"></i><b>6.2</b> Generalized Inverse</a><ul>
<li class="chapter" data-level="6.2.1" data-path="linear-system.html"><a href="linear-system.html#left-and-right-inverse"><i class="fa fa-check"></i><b>6.2.1</b> Left and Right Inverse</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="linear-system.html"><a href="linear-system.html#ill-conditioned-matrices"><i class="fa fa-check"></i><b>6.3</b> Ill-Conditioned Matrices</a><ul>
<li class="chapter" data-level="6.3.1" data-path="linear-system.html"><a href="linear-system.html#condition-number"><i class="fa fa-check"></i><b>6.3.1</b> Condition Number</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Multivariate Calculus</b></span></li>
<li class="chapter" data-level="7" data-path="partial-derivatives.html"><a href="partial-derivatives.html"><i class="fa fa-check"></i><b>7</b> Partial Derivatives</a><ul>
<li class="chapter" data-level="7.1" data-path="partial-derivatives.html"><a href="partial-derivatives.html#limit-and-continuity"><i class="fa fa-check"></i><b>7.1</b> Limit and Continuity</a></li>
<li class="chapter" data-level="7.2" data-path="partial-derivatives.html"><a href="partial-derivatives.html#partial-derivative"><i class="fa fa-check"></i><b>7.2</b> Partial Derivative</a><ul>
<li class="chapter" data-level="7.2.1" data-path="partial-derivatives.html"><a href="partial-derivatives.html#gradient-and-directional-derivative"><i class="fa fa-check"></i><b>7.2.1</b> Gradient and Directional Derivative</a></li>
<li class="chapter" data-level="7.2.2" data-path="partial-derivatives.html"><a href="partial-derivatives.html#linearization-of-two-variable-functions"><i class="fa fa-check"></i><b>7.2.2</b> Linearization of Two-variable Functions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="partial-derivatives.html"><a href="partial-derivatives.html#differentials"><i class="fa fa-check"></i><b>7.3</b> Differentials</a><ul>
<li class="chapter" data-level="7.3.1" data-path="partial-derivatives.html"><a href="partial-derivatives.html#continuity-partial-derivatives-and-differentiability"><i class="fa fa-check"></i><b>7.3.1</b> Continuity, Partial Derivatives and Differentiability</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="partial-derivatives.html"><a href="partial-derivatives.html#divergence-curl-and-laplacian"><i class="fa fa-check"></i><b>7.4</b> Divergence, Curl, and Laplacian</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="matrix-calculus.html"><a href="matrix-calculus.html"><i class="fa fa-check"></i><b>8</b> Matrix Calculus</a><ul>
<li class="chapter" data-level="8.1" data-path="matrix-calculus.html"><a href="matrix-calculus.html#the-chain-rule"><i class="fa fa-check"></i><b>8.1</b> The Chain Rule</a></li>
<li class="chapter" data-level="8.2" data-path="matrix-calculus.html"><a href="matrix-calculus.html#useful-identities-in-matirx-calculus"><i class="fa fa-check"></i><b>8.2</b> Useful Identities in Matirx Calculus</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="taylor-series.html"><a href="taylor-series.html"><i class="fa fa-check"></i><b>9</b> Taylor Series</a><ul>
<li class="chapter" data-level="9.1" data-path="taylor-series.html"><a href="taylor-series.html#convergence-of-taylor-series"><i class="fa fa-check"></i><b>9.1</b> Convergence of Taylor Series</a></li>
<li class="chapter" data-level="9.2" data-path="taylor-series.html"><a href="taylor-series.html#taylor-approximation-of-multivariate-functions"><i class="fa fa-check"></i><b>9.2</b> Taylor Approximation of Multivariate Functions</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="multiple-integral.html"><a href="multiple-integral.html"><i class="fa fa-check"></i><b>10</b> Multiple Integral</a></li>
<li class="part"><span><b>III Probability Theory</b></span></li>
<li class="chapter" data-level="11" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html"><i class="fa fa-check"></i><b>11</b> Basics of Probability Theory</a><ul>
<li class="chapter" data-level="11.1" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#probabilty-space"><i class="fa fa-check"></i><b>11.1</b> Probabilty Space</a></li>
<li class="chapter" data-level="11.2" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#counting"><i class="fa fa-check"></i><b>11.2</b> Counting</a></li>
<li class="chapter" data-level="11.3" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#conditional-probability"><i class="fa fa-check"></i><b>11.3</b> Conditional Probability</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html"><i class="fa fa-check"></i><b>12</b> Random variables and moments</a><ul>
<li class="chapter" data-level="12.1" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#properties-of-expectation-and-variance"><i class="fa fa-check"></i><b>12.1</b> Properties of Expectation and Variance</a><ul>
<li class="chapter" data-level="12.1.1" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#random-vectors"><i class="fa fa-check"></i><b>12.1.1</b> Random vectors</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#other-summaries"><i class="fa fa-check"></i><b>12.2</b> Other Summaries</a></li>
<li class="chapter" data-level="12.3" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#moment-generating-functions"><i class="fa fa-check"></i><b>12.3</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="univariate-distributions.html"><a href="univariate-distributions.html"><i class="fa fa-check"></i><b>13</b> Univariate Distributions</a><ul>
<li class="chapter" data-level="13.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#uniform-distribution"><i class="fa fa-check"></i><b>13.1</b> Uniform Distribution</a></li>
<li class="chapter" data-level="13.2" data-path="univariate-distributions.html"><a href="univariate-distributions.html#normal-distribution"><i class="fa fa-check"></i><b>13.2</b> Normal Distribution</a><ul>
<li class="chapter" data-level="13.2.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#log-normal-distribution"><i class="fa fa-check"></i><b>13.2.1</b> Log Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="univariate-distributions.html"><a href="univariate-distributions.html#binomial-distribution-and-beta-distribution"><i class="fa fa-check"></i><b>13.3</b> Binomial Distribution and Beta Distribution</a></li>
<li class="chapter" data-level="13.4" data-path="univariate-distributions.html"><a href="univariate-distributions.html#poisson-distribution"><i class="fa fa-check"></i><b>13.4</b> Poisson Distribution</a><ul>
<li class="chapter" data-level="13.4.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#poisson-process"><i class="fa fa-check"></i><b>13.4.1</b> Poisson Process</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="univariate-distributions.html"><a href="univariate-distributions.html#exponential-distribution-and-gamma-distribution"><i class="fa fa-check"></i><b>13.5</b> Exponential Distribution and Gamma Distribution</a><ul>
<li class="chapter" data-level="13.5.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#properties"><i class="fa fa-check"></i><b>13.5.1</b> Properties</a></li>
<li class="chapter" data-level="13.5.2" data-path="univariate-distributions.html"><a href="univariate-distributions.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>13.5.2</b> Inverse Gamma Distribution</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="univariate-distributions.html"><a href="univariate-distributions.html#beta-distribution"><i class="fa fa-check"></i><b>13.6</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>14</b> Multivariate Distributions</a><ul>
<li class="chapter" data-level="14.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>14.1</b> Multivariate Normal Distribution</a><ul>
<li class="chapter" data-level="14.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#chi-square-distribution"><i class="fa fa-check"></i><b>14.1.1</b> Chi-square Distribution</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#dirichlet-distributon"><i class="fa fa-check"></i><b>14.2</b> Dirichlet Distributon</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="markov-chain.html"><a href="markov-chain.html"><i class="fa fa-check"></i><b>15</b> Markov Chain</a></li>
<li class="part"><span><b>IV Learning Theory</b></span></li>
<li class="chapter" data-level="16" data-path="the-learning-problem-framework.html"><a href="the-learning-problem-framework.html"><i class="fa fa-check"></i><b>16</b> The Learning Problem Framework</a><ul>
<li class="chapter" data-level="16.1" data-path="the-learning-problem-framework.html"><a href="the-learning-problem-framework.html#the-pac-learning-framework"><i class="fa fa-check"></i><b>16.1</b> The PAC Learning Framework</a></li>
</ul></li>
<li class="part"><span><b>V Optimization</b></span></li>
<li class="chapter" data-level="17" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html"><i class="fa fa-check"></i><b>17</b> Basics of Optimization</a><ul>
<li class="chapter" data-level="17.1" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#univariate-optimization"><i class="fa fa-check"></i><b>17.1</b> Univariate Optimization</a></li>
<li class="chapter" data-level="17.2" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#multivariate-optimization"><i class="fa fa-check"></i><b>17.2</b> Multivariate Optimization</a></li>
<li class="chapter" data-level="17.3" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#convex-functions"><i class="fa fa-check"></i><b>17.3</b> Convex Functions</a></li>
<li class="chapter" data-level="17.4" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#method-of-lagrange-multiplier"><i class="fa fa-check"></i><b>17.4</b> Method of Lagrange Multiplier</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="gradient-descent.html"><a href="gradient-descent.html"><i class="fa fa-check"></i><b>18</b> Gradient Descent</a></li>
<li class="part"><span><b>VI Applications</b></span></li>
<li class="chapter" data-level="19" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>19</b> Linear Models</a><ul>
<li class="chapter" data-level="19.1" data-path="linear-models.html"><a href="linear-models.html#ordinary-least-squares"><i class="fa fa-check"></i><b>19.1</b> Ordinary Least Squares</a><ul>
<li class="chapter" data-level="19.1.1" data-path="linear-models.html"><a href="linear-models.html#least-square-estimation"><i class="fa fa-check"></i><b>19.1.1</b> Least Square Estimation</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="linear-models.html"><a href="linear-models.html#weighted-least-squares"><i class="fa fa-check"></i><b>19.2</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="19.3" data-path="linear-models.html"><a href="linear-models.html#partial-least-squares"><i class="fa fa-check"></i><b>19.3</b> Partial Least Squares</a></li>
<li class="chapter" data-level="19.4" data-path="linear-models.html"><a href="linear-models.html#regularized-regression"><i class="fa fa-check"></i><b>19.4</b> Regularized Regression</a><ul>
<li class="chapter" data-level="19.4.1" data-path="linear-models.html"><a href="linear-models.html#ridge-regression"><i class="fa fa-check"></i><b>19.4.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="19.4.2" data-path="linear-models.html"><a href="linear-models.html#lasso"><i class="fa fa-check"></i><b>19.4.2</b> Lasso</a></li>
<li class="chapter" data-level="19.4.3" data-path="linear-models.html"><a href="linear-models.html#elastic-net"><i class="fa fa-check"></i><b>19.4.3</b> Elastic Net</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>20</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="21" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>21</b> Text Mining</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li>
  <a href="https://github.com/rstudio/bookdown" target="blank">Written with bookdown</a>
</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Notes for Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="vector-spaces" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Vector Spaces</h1>
<p>Vector spaces, metric spaces, normed spaces, and inner product spaces are places where computations in linear algebra happen. These spaces are defined more or less to generalize properties of Euclidean space.</p>
<div id="vector-space" class="section level2">
<h2><span class="header-section-number">2.1</span> Vector Space</h2>
<p>A <strong>vector space</strong> <span class="math inline">\(V\)</span> is a nonempty set, also called <em>linear spaces</em>, the elements of which are called vectors,. A vector space comes with two operations predefined: vectors can be added together, and vectors can be multiplied by real numbers called scalars. For all vectors <span class="math inline">\(\bar{u}, \bar{v}\)</span> and <span class="math inline">\(\bar{w}\)</span>, and all scalars <span class="math inline">\(c\)</span> and <span class="math inline">\(d\)</span> in <span class="math inline">\(V\)</span>, the following axioms of vector space must hold:</p>
<ol style="list-style-type: decimal">
<li><p>The sum of <span class="math inline">\(\bar{u}\)</span> and <span class="math inline">\(\bar{v}\)</span> is in <span class="math inline">\(V\)</span></p></li>
<li><p>The scalar multiple of <span class="math inline">\(\bar{u}\)</span> by c, denoted by <span class="math inline">\(c\bar{u}\)</span>, is in <span class="math inline">\(V\)</span></p></li>
<li><p>There exists additive identity (denoted by <span class="math inline">\(\bar{0}\)</span>) such that <span class="math inline">\(\bar{x} + \bar{0} = \bar{x}\)</span>. Similarly, multiplicative identity (written <span class="math inline">\(\bar{1}\)</span>) means <span class="math inline">\(\bar{1}\bar{x} = \bar{x}\)</span></p></li>
<li><p>There exists an additive inverse (written <span class="math inline">\(-\bar{x}\)</span>) such that <span class="math inline">\(-\bar{x} + \bar{x} = \bar{0}\)</span></p></li>
<li><p>Communitivity: <span class="math inline">\(\bar{x} + \bar{y} = \bar{y} + \bar{x}\)</span></p></li>
<li><p>Associativity: <span class="math inline">\((\bar{x} + \bar{y}) + \bar{z} = \bar{x} + (\bar{y} + \bar{z})\)</span>, and <span class="math inline">\(\alpha(\beta\bar{x}) = (\alpha\beta)\bar{x}\)</span></p></li>
<li><p>Distributivity: <span class="math inline">\(\alpha(\bar{x} + \bar{y}) = \alpha\bar{x} + \alpha\bar{y}\)</span> and <span class="math inline">\((\alpha + \beta)\bar{x} = \alpha\bar{x} + \beta\bar{x}\)</span></p></li>
</ol>
<p>A set of vectors <span class="math inline">\(\bar{v}_1, ..., \bar{v}_n \in V\)</span> are set to be <em>linearly independent</em> if the following equation has only a traivial solution</p>
<p><span class="math display">\[
\begin{align*}
c_1\bar{v}_1 + \cdots + c_n\bar{v}_n &amp;= 0 &amp;&amp; \text{implies } c_1 = \cdots = c_n = 0 
\end{align*}
\]</span></p>
<p>The <strong>span</strong> of <span class="math inline">\(\bar{v}_1, ..., \bar{v}_n\)</span> is the set of all vectors that can be expressed as a linear combination of them.</p>
<div id="euclidean-space" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Euclidean Space</h3>
<p>Euclidean space is the quintessential vector space, denoted by <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
<p>The two must-have operations of vector spaces are valid in <span class="math inline">\(\mathbb{R}^n\)</span></p>
<p><span class="math display">\[
\bar{x} = 
\begin{bmatrix}
x_1 \\
\vdots \\
x_n
\end{bmatrix}, \;\;
\bar{y} = 
\begin{bmatrix}
y_1 \\
\vdots \\
y_n
\end{bmatrix}, \;\;
\alpha \in \mathbb{R} \\
\bar{x} + \bar{y} = 
\begin{bmatrix}
x_1 + y_1 \\
\vdots \\
x_n + y_n
\end{bmatrix}, \;\;
\alpha\bar{x} = 
\begin{bmatrix}
\alpha x_1 \\
\vdots \\
\alpha x_n
\end{bmatrix}
\]</span></p>
<p>Euclidean spaces have other structures defined in addition to the plainest vector space. We can calculate <em>dot product</em>, <em>length</em>, <em>distance</em> by</p>
<p><span class="math display">\[
\begin{aligned}
\text{dot product between }  \bar{x} \text{ and } \bar{y}: \bar{x} \cdot\bar{y} &amp;= \sum_{i=1}^{n}{x_iy_i} \\
\text{length of } \bar{x}: \|\bar{x}\| &amp;= \sqrt{\bar{x} \cdot \bar{x}} \\
\text{distance between } \bar{x} \text{ and } \bar{y}: \text{dist}(\bar{x}, \bar{y}) &amp;= \|\bar{x} - \bar{y}\|
\end{aligned}
\]</span></p>
</div>
</div>
<div id="metric-spaces-normed-spaces-inner-product-spaces" class="section level2">
<h2><span class="header-section-number">2.2</span> Metric Spaces, Normed Spaces, Inner Product Spaces</h2>
<div id="metric-and-norm" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Metric and Norm</h3>
<p>Metric spaces, normed spaces and inner product spaces capture important properties of Euclidean space in a more general way (distance, length, angle).</p>
<p>Although metric spaces are not required to be vector spaces, it is always assumed in linear algebra that this is the case. For this reason, metric spaces are short for “metric linear space”. Normed spaces and inner product spaces are defined to be extensions of metric linear spaces, so that they must be vector spaces.</p>
<hr>
<p>Metrics generalize the notion of distance from Euclidean space. A <em>metric space</em> is a set together with a metric on the set (metric spaces don’t have to be vector spaces). The metric is a function that defines a concept of distance <span class="math inline">\(\in \mathbb{R}\)</span> between any two members of the set. A metric must satisfies the following properties:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(d(\bar{x}, \bar{y}) \ge 0\)</span>, with equality if and only if <span class="math inline">\(\bar{x} = \bar{y}\)</span>. Distances are non-negative, and the only point at zero distance from <span class="math inline">\(x\)</span> is <span class="math inline">\(x\)</span> itself<br />
</li>
<li><span class="math inline">\(d(\bar{x}, \bar{y}) = d(\bar{y}, \bar{x})\)</span>. The distance is a symmetric function.</li>
<li><span class="math inline">\(d(\bar{x}, \bar{z}) \le d(\bar{x}, \bar{y}) + d(\bar{y}, \bar{z})\)</span>. Distance satisfies triangular inequality.</li>
</ol>
<p><br></p>
<p>Norms generalize the notion of length from Euclidean space.</p>
<p>A <strong>norm</strong> on a real vector space <span class="math inline">\(X\)</span> is a function: <span class="math inline">\(\|\cdot\|: V \rightarrow \mathbb{R}\)</span> that satisfies:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\|\bar{x}\| \ge 0\)</span> for all <span class="math inline">\(\bar{x} \in X\)</span>, with equality if and only if <span class="math inline">\(\bar{x} = \bar{0}\)</span> (nonnegative)<br />
</li>
<li><span class="math inline">\(\|\lambda \bar{x}\| = \lambda \|\bar{x}\|\)</span>, for all <span class="math inline">\(\bar{x} \in X\)</span> and <span class="math inline">\(\lambda \in \mathbb{R}\)</span> (homogeneous)<br />
</li>
<li><span class="math inline">\(\|\bar{x} + \bar{y}\| \le \|\bar{x}\| + \|\bar{y}\|\)</span> (triangular inequality)</li>
</ol>
<p><strong>A normed space is a metric space with the metric</strong></p>
<p><span class="math display">\[
d(\bar{x}, \bar{y}) = \|\bar{x} - \bar{y}\|
\]</span></p>
<p>So a normed space is a special case of metric spaces, a metric spcae may not necessarily has a norm associated with it. One can verify that <span class="math inline">\(d(\bar{x}, \bar{y}) = \|\bar{x} - \bar{y}\|\)</span> satisfies all properties of a metric.</p>
<p>The most common function for norms on <span class="math inline">\(\mathbb{R}^n\)</span> are listed below, with <span class="math inline">\(\bar{x} = [x_1, x_2, ..., x_n]\)</span>.</p>
<p><span class="math display">\[
\begin{align*}
\text{1-norm}: \|\bar{x}\|_1 &amp;= \sum_{i=1}^{n}{|x_i|}\\
\text{2-norm}: \|\bar{x}\|_2 &amp;= \sqrt{\sum_{i=1}^{n}{x_i}^2} \\
\text{p-norm}: \|\bar{x}\|_p &amp;= (\sum_{i=1}^{n}{|x_i|}^p)^{\frac{1}{p}} \quad (p \ge 1) \\
\text{maximum norm}: \|\bar{x}\|_{\infty} &amp;= \max\{|x_1|, |x_2|, ..., |x_n|\}
\end{align*}
\]</span></p>
<p>1-norm is also called the Manhattan norm.</p>
<p>2-norm is the Euclidean norm, the subscript <span class="math inline">\(2\)</span> can be left out in <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
<p>p-norm is a generalization of 1-norm and 2-norm, requiring <span class="math inline">\(p &gt; 1\)</span>. When <span class="math inline">\(p\)</span> turns infinity, <span class="math inline">\(\|\bar{x}\|_{\infty}\)</span> is called the maximum norm.</p>
<p>The VMLS book <span class="citation">(Boyd and Vandenberghe <a href="references.html#ref-boyd2018introduction" role="doc-biblioref">2018</a>)</span> introduces some other topics related to norm.</p>
<p><strong>Root-mean-square value</strong>. The root-mean-square (RMS) value of an <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\bar{x}\)</span> is defined as</p>
<p><span class="math display">\[
\boldsymbol{\text{rms}}(x) = \sqrt{\frac{x_1^2 + \cdots + x_n^2}{n}} = \frac{\| \bar{x}\|}{\sqrt{n}}
\]</span></p>
<p>RMS is useful when comparing norms of vectors with different dimensions, because the value is independent of <span class="math inline">\(n\)</span>, and it tells what a “typical” value of <span class="math inline">\(|x_i|\)</span> is. The norm of a vector with all entries being the same value of <span class="math inline">\(\alpha\)</span> will be <span class="math inline">\(\sqrt{n}|\alpha|\)</span>, yet its rms value will be <span class="math inline">\(|\alpha|\)</span>.</p>
<p><strong>Norm of a sum</strong>. The norm of the sum of two vectors <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(\bar{y}\)</span> is</p>
<p><span class="math display">\[
\|\bar{x} + \bar{y}\| = \sqrt{\|\bar{x}\|^2 + 2\bar{x}^T\bar{y} +  \|\bar{y} \|^2}
\]</span></p>
<p>This can be proven by expanding <span class="math inline">\((\bar{x} + \bar{y})^T(\bar{x} + \bar{y})\)</span></p>
<p><strong>Norm of block vectors</strong>. The norm-squared of a stacked vector is the sum of norm-square values of its subvectors. For example, if we stack 3 vectors <span class="math inline">\(\bar{a}, \bar{b}, \bar{c}\)</span> to form a longer vector <span class="math inline">\(\bar{d}\)</span></p>
<p><span class="math display">\[
\bar{d} = 
\begin{bmatrix}
\bar{a} \\
\bar{b} \\
\bar{c}
\end{bmatrix}
\]</span></p>
<p>We have</p>
<p><span class="math display">\[
\| d\|^2 = \bar{a}^T\bar{a} + \bar{b}^T\bar{b} + \bar{c}^T\bar{c} = \|\bar{a}\|^2 + \|\bar{b}\|^2 + \|\bar{c}\|^2
\]</span>
Another way of writing this is</p>
<p><span class="math display">\[
\|(\bar{a}, \bar{b}, \bar{c})\| = \sqrt{ \|\bar{a}\|^2 + \|\bar{b}\|^2 + \|\bar{c}\|^2} = \|(\|\bar{a} \|, \|\bar{b} \|, \|\bar{c}\|)\|
\]</span></p>
<p><strong>Chebyshev inequality</strong></p>
<p><span class="math display">\[
\frac{\|\bar{x}\|^2}{k} \ge a^2
\]</span></p>
<p>Note that <span class="math inline">\(\text{rms}(\bar{x}) = \|\bar{x}\| / \sqrt{n}\)</span>, so that <span class="math inline">\(\|\bar{x}\|^2 = n \, \text{rms}(\bar{x})^2\)</span>. Then we have</p>
<p><span class="math display">\[
\Big(\frac{\text{rms}(\bar{x})}{a}\Big)^2 \ge \frac{k}{n}
\]</span></p>
<p><strong>Average, RMS value, and standard deviation</strong>. The mean and standard deviation of vector <span class="math inline">\(\bar{x}\)</span> can be written in the form</p>
<p><span class="math display">\[
\begin{aligned}
\mu &amp;=  \frac{1}{n}\bar{1}^T\bar{x} \\
\sigma &amp;= \frac{\|\bar{x} - \mu\bar{1}\| }{\sqrt{n}}
\end{aligned}
\]</span></p>
<p>Then we have the following relationship</p>
<p><span class="math display">\[
\text{rms}(\bar{x})^2 = \mu^2 + \sigma^2
\]</span></p>

<div class="proof">
Proof
</div>

<p><span class="math display">\[
\begin{split}
\sigma^2 
&amp; = \frac{1}{n} \|\bar{x} -   \frac{1}{n}\bar{1}^T\bar{x}\bar{1}\|^2 \\
&amp;= \frac{1}{n}[\bar{x}^T\bar{x} + \Big((\frac{1}{n}\bar{1}^T\bar{x})\bar{1}\Big)^T\Big((\frac{1}{n}\bar{1}^T\bar{x})\bar{1}\Big) - 2\bar{x}^T(\frac{1}{n}\bar{1}^T\bar{x})\bar{1}]  \qquad \mbox{norm of sum} \\
&amp;= \frac{1}{n}[\bar{x}^T\bar{x} + \frac{1}{n}(\bar{1}^T\bar{x})^2 - \frac{2}{n}(\bar{1}^T\bar{x})^2] \\
&amp;= \frac{1}{n}\bar{x}^T\bar{x} - \frac{1}{n^2}(\bar{1}^T\bar{x})^2 \\
&amp;= \text{rms}(\bar{x}) - \mu^2
\end{split}
\]</span></p>
</div>
<div id="inner-produc-outer-product-cross-product" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Inner Produc, Outer Product, Cross Product</h3>
<p>An <strong>inner product</strong> on a real vector space <span class="math inline">\(X\)</span> is a function <span class="math inline">\(\langle \cdot, \cdot\rangle: X \times X \rightarrow \mathbb{R}\)</span> satisfying</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\langle \bar{x}, \bar{y} \rangle \ge 0\)</span>, with equality if and only if <span class="math inline">\(x = \bar{0}\)</span><br />
</li>
<li><span class="math inline">\(\langle \bar{x}, \bar{y} \rangle = \langle \bar{y}, \bar{x} \rangle\)</span></li>
<li><span class="math inline">\(\langle \bar{x} + \bar{y}, \bar{z}\rangle = \langle \bar{x}, \bar{z}\rangle + \langle \bar{y}, \bar{z}\rangle\)</span> and <span class="math inline">\(\langle \lambda \bar{x}, \bar{y}\rangle = \lambda \langle \bar{x}, \bar{y} \rangle\)</span></li>
</ol>
<p>A vector space equipped with such inner product is called a <strong>inner product space</strong>. Note that <strong>all inner product spaces are normed spaces</strong>, because a inner product induce a norm on a vector space:</p>
<p><span class="math display">\[
\langle \bar{x}, \bar{x} \rangle = \|\bar{x}\|^2  
\]</span></p>
<p>The <em>standard inner product</em> defined on <span class="math inline">\(\mathbb{R}^{n}\)</span> is the dot product, given by</p>
<p><span class="math display">\[
\langle \bar{x}, \bar{y} \rangle = \sum_{i=1}^{n}{x_iy_i} = \bar{x}^T\bar{y}
\]</span></p>

<div class="rmdnote">
The abstract spaces—metric spaces, normed spaces, and inner product spaces—are all examples of what are more generally called “topological spaces” (linear topological space if they are assumed to be vector spaces first). These spaces have been given in order of increasing structure. That is, every inner product space is a normed space, and in turn, every normed space is a metric space.
</div>

<p>The <strong>outer product</strong> of an <span class="math inline">\(m\)</span> vector <span class="math inline">\(\bar{a}\)</span> and an <span class="math inline">\(\bar{b}\)</span> is given by <span class="math inline">\(\bar{a}\bar{b}^T\)</span>, which is an <strong>rank 1</strong> <span class="math inline">\(m \times n\)</span> matrix</p>
<p><span class="math display">\[
\bar{a}\bar{b}^T = 
\begin{bmatrix}
a_1b_1 &amp; a_1b_2 &amp; \cdots &amp; a_1b_n \\
a_2b_1 &amp; a_2b_2 &amp; \cdots &amp; a_2b_n \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
a_nb_1 &amp; a_nb_2 &amp; \cdots &amp; a_nb_n 
\end{bmatrix}
\]</span>
The outer product is not symmetric, i.e. (<span class="math inline">\(\bar{a}\bar{b}^T \not = \bar{b}\bar{a}^T\)</span>)</p>
<p>If we express an <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> in terms of its columns <span class="math inline">\(a_1, ..., a_n\)</span> and the <span class="math inline">\(n \times p\)</span> matrix <span class="math inline">\(B\)</span> in terms of its rows <span class="math inline">\(b_1, ..., b_p\)</span></p>
<p><span class="math display">\[
A = 
\begin{bmatrix}
a_1 &amp; \cdots &amp; a_n
\end{bmatrix}, \quad
B = 
\begin{bmatrix}
b_1^T \\
\vdots \\
b_n^T
\end{bmatrix}
\]</span>
The the matrix multiplication <span class="math inline">\(AB\)</span> can be expressed as the sum of outer products between <span class="math inline">\(a_i\)</span> and <span class="math inline">\(b_i\)</span></p>
<p><span class="math display">\[
AB = a_1b_1^T + \cdots + a_nb_n^T
\]</span>
This echoes theorem <a href="basic-matrix-algebra.html#thm:cr-expansion">1.1</a>, where matrix multiplication is computed via column row expansion.</p>
<p><br></p>
<p>Next, we bring in the concept of cross product which may not be so significant in the linear algebra section, but it will shine in multivariate integral calculus.</p>
<p>The <strong>cross product</strong> between <span class="math inline">\(\bar{u}\)</span> and <span class="math inline">\(\bar{v}\)</span> is a <strong>vector</strong> perpendicular to <span class="math inline">\(\bar{u}\)</span>, <span class="math inline">\(\bar{v}\)</span> and the plane formed by them, with length equal to</p>
<p><span class="math display">\[
\|\bar{u} \times \bar{v} \| = \|\bar{u}\| \|\bar{v}\| \sin \theta
\]</span></p>
<p>Let <span class="math inline">\(\bar{n}\)</span> denote the unit vector perpendicular to the plane containing <span class="math inline">\(\bar{u}\)</span> and <span class="math inline">\(\bar{v}\)</span>, the cross product can be written as</p>
<p><span class="math display">\[
\bar{u} \times \bar{v} = \|\bar{u}\| \|\bar{v}\| \sin (\theta)\bar{n}
\]</span></p>
<p>Though this formula is rarely helpful we because the main usage of cross product is finding the direction <span class="math inline">\(\bar{n}\)</span>.</p>
<p>In most cases, we only consider cross product in 3D spaces, although it can be generalized to higher dimensions.</p>
<p>Since the sines of <span class="math inline">\(0\)</span> and <span class="math inline">\(\pi\)</span>, it makes sense that the cross product of two parallel vectors is zero.</p>
<p>The cross product obeys the following laws</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(r\bar{u} \times s\bar{v} = rs (\bar{u} \times \bar{v})\)</span></p></li>
<li><p>order matters: <span class="math inline">\(\bar{u} \times \bar{v} = -\bar{v} \times \bar{u}\)</span></p></li>
<li><p>Distributive: <span class="math inline">\(\bar{u} \times (\bar{v} + \bar{w})= \bar{u} \times \bar{v} + \bar{u} \times \bar{w}\)</span></p></li>
<li><p>Not associative: <span class="math inline">\(\bar{u} \times (\bar{v} \times \bar{w}) = (\bar{u} \cdot \bar{w})\bar{v} - (\bar{u} \cdot \bar{v})\bar{w}\)</span></p></li>
</ol>
<p>To get a general formula of cross product, we consider the cross product among 3 standard basis <span class="math inline">\(\boldsymbol{i} = [1, 0, 0], \boldsymbol{j} = [0, 1, 0], \boldsymbol{k} = [0, 0, 1]\)</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="images/cross-product.png" alt="Figure 12.30 FROM Thomas Calculus, 14th edition" width="200" />
<p class="caption">
Figure 2.1: Figure 12.30 FROM Thomas Calculus, 14th edition
</p>
</div>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{i} \times \boldsymbol{j} &amp;= \boldsymbol{k} \\
\boldsymbol{j} \times \boldsymbol{k} &amp;= \boldsymbol{i} \\
\boldsymbol{k} \times \boldsymbol{i} &amp;= \boldsymbol{j}
\end{aligned}
\]</span>
and</p>
<p><span class="math display">\[
\boldsymbol{i} \times \boldsymbol{i} = \boldsymbol{j} \times \boldsymbol{j} = \boldsymbol{k} \times \boldsymbol{k} = 0
\]</span></p>
<p>Then the cross product between <span class="math inline">\(\bar{u}\)</span> and <span class="math inline">\(\bar{v}\)</span> can be written as</p>
<p><span class="math display">\[
\begin{split}
\bar{u} \times \bar{v} &amp;= (u_1\boldsymbol{i} + u_2\boldsymbol{j} + u_3\boldsymbol{k}) \times (v_1\boldsymbol{i} + v_2\boldsymbol{j} + v_3\boldsymbol{k}) \\
&amp;= (u_1v_1)\boldsymbol{i} \times \boldsymbol{i} + (u_1v_2)\boldsymbol{i} \times \boldsymbol{j} + (u_1v_3) \boldsymbol{i} \times \boldsymbol{k} \\
&amp;{\quad} + (u_2v_1)\boldsymbol{j} \times \boldsymbol{i} + (u_2v_2)\boldsymbol{j} \times \boldsymbol{j} + (u_2v_3)\boldsymbol{j} \times \boldsymbol{k} \\
&amp;{\quad} + (u_3v_1)\boldsymbol{k} \times \boldsymbol{i} + (u_3v_2)\boldsymbol{k} \times \boldsymbol{j} + (u_3v_3)\boldsymbol{k} \times \boldsymbol{k} \\
&amp;= (u_2v_3 - u_3v_2)\boldsymbol{i} - (u_1v_3 - u_3v_1)\boldsymbol{j} + (u_1v_2 - u_2v_1)\boldsymbol{k} \\
&amp;=  \begin{bmatrix}
u_2v_3 - u_3v_2 \\
u_1v_3 - u_3v_1 \\
u_1v_2 - u_2v_1
\end{bmatrix}
\end{split}
\]</span>
This result is same as the determinant of the following matrix (think of <span class="math inline">\(\boldsymbol{i}, \boldsymbol{j}, \boldsymbol{k}\)</span>) as scalars during the computation)</p>
<p><span class="math display">\[
\bar{u} \times \bar{v} = 
\begin{vmatrix}
\boldsymbol{i} &amp; \boldsymbol{j} &amp; \boldsymbol{k} \\
u_1 &amp; u_2 &amp; u_3 \\
v_1 &amp; v_2 &amp; v_3 
\end{vmatrix}
\]</span></p>
<p>The main use for cross product will be finding a vector which is perpendicular to two given vectors. We can make use of the magnitude as well. Recall the magnitude of the cross product</p>
<p><span class="math display">\[
\| \bar{u} \times \bar{v} \| = \|\bar{u}\| \|v\|\sin\theta
\]</span></p>
<p>This is the area of the following parallelogram defined by <span class="math inline">\(\bar{u}\)</span> and <span class="math inline">\(\bar{v}\)</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="images/cross-product-2.png" alt="Figure 12.31 FROM Thomas Calculus, 14th edition" width="199" />
<p class="caption">
Figure 2.2: Figure 12.31 FROM Thomas Calculus, 14th edition
</p>
</div>
<p>So let’s review what the dot product gives us: its a vector perpendicular to the plane containing <span class="math inline">\(\bar{u}\)</span> and <span class="math inline">\(\bar{v}\)</span>, the length of that vector is equal to the area of the parallelogram formed by <span class="math inline">\(\bar{u}\)</span> and <span class="math inline">\(\bar{v}\)</span>.</p>
<p>As one application of the cross product, we have the following theorem.</p>

<div class="theorem">
<p><span id="thm:volume-cross-product" class="theorem"><strong>Theorem 2.1  (Volume of a Parallelepiped)  </strong></span>The volume of a parallelepiped with adjacent edges given by vectors <span class="math inline">\(\bar{u}, \bar{v}\)</span> and <span class="math inline">\(\bar{w}\)</span> is the absolute value of the <em>triple scalar product</em></p>
<span class="math display">\[
V = |\bar{u} \cdot (\bar{v} \times \bar{w})|
\]</span>
</div>

<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-5"></span>
<img src="images/cross-product-3.png" alt="Figure 2.59 from https://openstax.org/books/calculus-volume-3/pages/2-4-the-cross-product" width="308" />
<p class="caption">
Figure 2.3: Figure 2.59 from <a href="https://openstax.org/books/calculus-volume-3/pages/2-4-the-cross-product" class="uri">https://openstax.org/books/calculus-volume-3/pages/2-4-the-cross-product</a>
</p>
</div>

<div class="proof">
Proof
</div>

<p>The area of the base of the parallelepiped is given by <span class="math inline">\(\|\bar{v} \times \bar{w}\|\)</span>, the height is <span class="math inline">\(\bar{u}\)</span> projected on to the resulting vector from the dot product. So we have</p>
<p><span class="math display">\[
\begin{split}
V &amp;= \|\text{proj}_{\bar{v} \times \bar{w}} \bar{u} \|\|\bar{v} \times \bar{w}\| \\
&amp;= |\frac{\bar{u} \cdot (\bar{v} \times \bar{w})}{\| \bar{v} \times \bar{w} \|} |\| \bar{v} \times \bar{w} \| \\
&amp;= |\bar{u} \cdot (\bar{v} \times \bar{w})|
\end{split}
\]</span></p>
<p>Looking back at the formula for <span class="math inline">\(\bar{v} \times \bar{w}\)</span>, we can see that this term is the absolute value of the following determinant according to cofactor expansion</p>
<p><span class="math display">\[
\begin{vmatrix}
\bar{u}^T \\
\bar{v}^T \\
\bar{w}^T \\
\end{vmatrix}
\]</span></p>
</div>
<div id="restricted-definition-of-inner-products-in-rn" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Restricted Definition of Inner Products in <span class="math inline">\(R^n\)</span></h3>
<p>Sometimes it suffice only to generalize the dot product, with the definition of inner product, in Euclidean space <span class="math inline">\(\mathbb{R}^n\)</span> instead of other inner product spaces. For example, many engineering applications measure similarity between vectors using the dot product after stretching the two vectors in some directions, with linear transformation <span class="math inline">\(A\)</span>. Therefore, we can given a restricted definition of inner product that is meant to be used in <span class="math inline">\(\mathbb{R}^n\)</span>.</p>

<div class="definition">
<span id="def:restricted-inner-product" class="definition"><strong>Definition 2.1  (Restricted definition of inner product)  </strong></span>
The generalized dot product <span class="math inline">\(\langle \bar{x}, \bar{y}\rangle\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span> between two vectors, is the dot product between <span class="math inline">\(A\bar{x}\)</span> and <span class="math inline">\(A\bar{y}\)</span>, for some <span class="math inline">\(n \times n\)</span> <strong>positive definite matrix <span class="math inline">\(S\)</span></strong>. The inner product <span class="math inline">\(\langle \bar{x}, \bar{y}\rangle\)</span> can also be expressed using the Gram matrix <span class="math inline">\(S = A^TA\)</span>
</div>

<p><span class="math display">\[
\langle \bar{x}, \bar{y}\rangle =\bar{x}^TS\bar{y}  = \bar{x}(A^TA)\bar{y} 
\]</span></p>
<p>It’s easy to see that when <span class="math inline">\(S\)</span> is the identity matrix, the inner product is the dot product. <span class="math inline">\(S\)</span> being positive semidefinite ensures <span class="math inline">\(\bar{x}^TS\bar{y}\)</span> satisfies axioms of inner product.</p>
<p>This definition of inner product also induces angles and distances with respect to transformation <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
\cos_A{(\bar{x}, \bar{y})} = \frac{\langle \bar{x}, \bar{y}\rangle}{\sqrt{\langle \bar{x}, \bar{x}\rangle}\sqrt{\langle \bar{y}, \bar{y}\rangle}}
= \frac{\bar{x}S\bar{y}}{\sqrt{\bar{x}S\bar{x}}\sqrt{\bar{y}S\bar{y}}}
= \frac{(A\bar{x})^T(A\bar{y})}{\|A\bar{x}\|_2\|A\bar{y}\|_2} \\
\text{dist}(\bar{x}, \bar{y}) = \sqrt{\langle \bar{x} - \bar{y}, \bar{x} - \bar{y} \rangle}
= \sqrt{(\bar{x} - \bar{y})^TS(\bar{x} - \bar{y})}
= \|A\bar{x} - A\bar{y} \|_2
\]</span></p>
</div>
</div>
<div id="subspaces" class="section level2">
<h2><span class="header-section-number">2.3</span> Subspaces</h2>
<p>If <span class="math inline">\(A\)</span> is a vector space, then <span class="math inline">\(S\)</span> is a subspace of <span class="math inline">\(A\)</span> (denoted <span class="math inline">\(S \subseteq A\)</span>) if</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\bar{0} \in S\)</span></p></li>
<li><p><span class="math inline">\(S\)</span> is closed under addition: if <span class="math inline">\(\bar{x}, \bar{y} \in S\)</span>, then <span class="math inline">\(\bar{x} + \bar{y} \in S\)</span></p></li>
<li><p><span class="math inline">\(S\)</span> is closed under scalar multiplication if <span class="math inline">\(\bar{x} \in S, \alpha \in \mathbb{R}\)</span> then <span class="math inline">\(\alpha\bar{x} \in S\)</span></p></li>
</ol>
<p>According to this definition, and vector space is always a subspace of itself.</p>
<p>If <span class="math inline">\(U\)</span> and <span class="math inline">\(W\)</span> are all subspaces of <span class="math inline">\(V\)</span>, then the sum of these two subspaces are defined as</p>
<p><span class="math display">\[
U + W = \{\bar{u} + \bar{v} \;| \; \bar{w} \in U, \bar{w} \in W \}
\]</span></p>
<p>If <span class="math inline">\(U\)</span> and <span class="math inline">\(W\)</span> are perpendicular, in other words, <span class="math inline">\(U \cap W = \bar{0}\)</span>. Then <span class="math inline">\(U + W\)</span> are said to be a <strong>direct sum</strong> and written <span class="math inline">\(U \oplus W\)</span>.</p>
<p>Dimensions of the sum of subspaces has the following property</p>
<p><span class="math display">\[
\text{dim}(U + W) = \text{dim}(U) + \text{dim}(W) - \text{dim}(U \cap W)
\]</span></p>
<p>It follows that if <span class="math inline">\(U\)</span> is perpendicular to <span class="math inline">\(W\)</span>, <span class="math inline">\(W\)</span>,</p>
<p><span class="math display">\[
\text{dim}(U \oplus W) = \text{dim}(V)  = \text{dim}(U) + \text{dim}(W)
\]</span></p>
</div>
<div id="fundamental-theorem" class="section level2">
<h2><span class="header-section-number">2.4</span> Fundamental Theorem of Linear Algebra</h2>
<p>The <strong>columnspace</strong> (also called <em>range</em>) of matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> is the span of the columns of <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
\mathcal{R}(A) = \{\bar{v} \in \mathbb{R}^m\;|\; \bar{v} = A\bar{x}, \, \bar{x} \in \mathbb{R}^n\}
\]</span></p>
<p>Similarly, the <strong>rowspace</strong> of <span class="math inline">\(A\)</span> is the span of its rows <span class="math inline">\(\mathcal{R}(A^T)\)</span>.</p>
<p>The <strong>nullspace</strong> (also called <em>kernel</em>) of <span class="math inline">\(A\)</span> is the set of solutions to <span class="math inline">\(A\bar{v} = \bar{0}\)</span></p>
<p><span class="math display">\[
\mathcal{N}(A) = \{\bar{x} \in \mathbb{R}^n\;|\;A\bar{x} = \bar{0}\}
\]</span></p>
<p>And the left null space is all <span class="math inline">\(\bar{x}\)</span> that satisfies <span class="math inline">\(A^T\bar{x} = \bar{0}\)</span>. The word “left” in this context stems from the fact that <span class="math inline">\(A^T\bar{x}= \bar{0}\)</span> is equivalent to <span class="math inline">\(\bar{x}^TA=\bar{0}\)</span> where y “acts” on A from the left.</p>
<p>The relationship between these four subspaces present the fundamental theorem of linear algebra</p>

<div class="theorem">
<p><span id="thm:fundamental-theorem" class="theorem"><strong>Theorem 2.2  (The fundamental theorem of Linear Algebra)  </strong></span></p>
<ul>
<li><p><span class="math inline">\(\mathcal{R}(A) = \mathcal{N}(A^T)^{\perp}\)</span>, and <span class="math inline">\(\dim(\mathcal{R}(A)) + \dim(\mathcal{N}(A^T)) = m\)</span></p></li>
<li><p><span class="math inline">\(\mathcal{R}(A^T) = \mathcal{N}(A)^{\perp}\)</span>, and <span class="math inline">\(\dim(\mathcal{R}(A^T)) + \dim(\mathcal{N}(A)) = n\)</span></p>
</div></li>
</ul>
<p>If the rank (defined next section) of <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> is <span class="math inline">\(r\)</span></p>
<ul>
<li><p><span class="math inline">\(m - r\)</span> is the dimension of the left null space of <span class="math inline">\(A\)</span></p></li>
<li><p><span class="math inline">\(n - r\)</span> is the dimension of the null space of <span class="math inline">\(A\)</span></p></li>
</ul>
</div>
<div id="rank" class="section level2">
<h2><span class="header-section-number">2.5</span> Rank</h2>
<p>The <strong>rank</strong> of a vector space is its dimension.</p>

<div class="definition">
<span id="def:unnamed-chunk-7" class="definition"><strong>Definition 2.2  </strong></span>The rank of a matrix is equal to the rank of its column space, which is the same as the rank of its column space.
</div>

<p>I often think of rank of <span class="math inline">\(A\)</span> as the total volume of information that the matrix can offer.</p>
<p>From the definition of matrix rank, we know that <span class="math inline">\(A\)</span>’s row rank (the dimension of <span class="math inline">\(\mathcal{R}(A^T)\)</span>)equals its column rank <span class="math inline">\(\mathcal{R}(A)\)</span>. A way to verify this are presented below.</p>
<p>All matrices can be reduced into a (possibly rectangular) <em>diagonal matrix</em> with elementary row and column operations. First we can row reduce the matrix into row echelon form, then use column operations to convert positions above the pivot to zero.</p>
<p>Thus, any <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> can be expressed in the following form</p>
<p><span class="math display">\[
RAC = \Lambda \tag{1}
\]</span></p>
<p>where <span class="math inline">\(R\)</span> is the product of the elementary matrices that perform row opertations, and <span class="math inline">\(C\)</span> for column operations. Since <span class="math inline">\(C\)</span> is invertible, we can write</p>
<p><span class="math display">\[
RA = \Lambda C^{-1} \tag{2}
\]</span></p>
<p>From (1) we know that row rank of <span class="math inline">\(A\)</span> is identical to that the number of non-zero entries in <span class="math inline">\(\Lambda\)</span>, on the ground that row operations on <span class="math inline">\(A\)</span> does not change its row rank, and <span class="math inline">\(C^{-1}\)</span> only scale diagonal entries of <span class="math inline">\(\Lambda\)</span> to some multiple. Similarly, <span class="math inline">\(AC = R^{-1}\Lambda\)</span> shows that column rank of <span class="math inline">\(A\)</span> is the same as the number of non-zero diagonal entries in <span class="math inline">\(\Lambda\)</span>. Therefore, row rank <span class="math inline">\(=\)</span> column rank.</p>
<div id="effect-of-operations-on-matrix-rank" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Effect of Operations on Matrix Rank</h3>
<p>Let <span class="math inline">\(A, B \in \mathbb{R}^{m \times n}\)</span> have ranks <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span></p>

<div class="corollary">
<p><span id="cor:unnamed-chunk-8" class="corollary"><strong>Corollary 2.1  </strong></span></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(|a - b| \le r(A + B) \le a + b\)</span></p></li>
<li><p><span class="math inline">\(r(AB) &lt; \min(a, b)\)</span></p>
</div></li>
</ol>
<div class="proof">
Proof
</div>
<p>For (1), rows / columns of <span class="math inline">\(A + B\)</span> can be expressed as linear combinations of rows / columns of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p>
<p>For (2), Each column of <span class="math inline">\(AB\)</span> is a linear combination of columns of A, and each row is a linear combination of rows of <span class="math inline">\(B\)</span>. Therefore, <span class="math inline">\(r(AB)\)</span> can not exceed either rank of <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>. There is also a corollary on the lower bound of <span class="math inline">\(r(AB)\)</span>, which is <span class="math inline">\(a + b - n\)</span>. Note that <span class="math inline">\(n\)</span> is the shared dimension. I have not found a concise proof about this, but this property leads to a interesting result: when one of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are square and full rank, <span class="math inline">\(\min(r(AB)) = \max(r(AB))\)</span></p>

<div class="corollary">
<p><span id="cor:unnamed-chunk-9" class="corollary"><strong>Corollary 2.2  </strong></span></p>
<ol start="3" style="list-style-type: decimal">
<li><p>Multiplying <span class="math inline">\(A\)</span> with a square matrix <span class="math inline">\(B\)</span> of full rank does not change the rank of <span class="math inline">\(A\)</span>.</p></li>
<li><p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are both singular, then <span class="math inline">\(AB\)</span> is non-singular if and only if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are both non-singular</p>
</div></li>
</ol>

<div class="proof">
Proof
</div>

<p>Suppose <span class="math inline">\(B\)</span> is <span class="math inline">\(n \times n\)</span>, the minimum rank of <span class="math inline">\(AB\)</span> is <span class="math inline">\(a + n - n = a\)</span>, and the maximum rank is <span class="math inline">\(\min(a, n) = a\)</span>. Thus, multiplying by a full rank matrix preserves rank: <span class="math inline">\(r(AB) = r(A)\)</span>. (4) follows naturally after (3).</p>
</div>
<div id="gram-matrix" class="section level3">
<h3><span class="header-section-number">2.5.2</span> Gram Matrix</h3>

<div class="proposition">
<span id="prp:gram-matrix" class="proposition"><strong>Proposition 2.1  (Gram matrix)  </strong></span>The matrix <span class="math inline">\(A^TA\)</span> is said to be the Gram matrix of column space of <span class="math inline">\(A_{m \times n}\)</span>. The columns of <span class="math inline">\(A\)</span> are linearly independent if and only if <span class="math inline">\(A^TA\)</span> is invertible.
</div>


<div class="proof">
Proof
</div>

<p>When <span class="math inline">\(A^TA\)</span> is invertible, it has rank <span class="math inline">\(n\)</span>. Therefore, each of the factors of <span class="math inline">\(A^TA\)</span> has at least rank <span class="math inline">\(n\)</span>, and this means columns of <span class="math inline">\(A\)</span> are linearly independent (since <span class="math inline">\(r(A) \le \min(m, n)\)</span>).</p>
<p>Similarly, <span class="math inline">\(AA^T\)</span> are called the left Gram matrix of rowspace of <span class="math inline">\(A\)</span>. And <span class="math inline">\(AA^T\)</span> is invertible if and only rows of <span class="math inline">\(A\)</span> are linearly independent.</p>

<div class="proposition">
<p><span id="prp:gram-matrix-rank" class="proposition"><strong>Proposition 2.2  </strong></span>For any matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, <span class="math inline">\(A\)</span>, <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> always have the same rank.</p>
<span class="math display">\[
r(A) = r(A^TA) = r(AA^T)
\]</span>
</div>


<div class="proof">
Proof
</div>

<p>For <span class="math inline">\(r(A) = r(A^TA)\)</span>, suppose <span class="math inline">\(r(A) = r\)</span>, then <span class="math inline">\(\dim(\mathcal{N}(A)) = n - \dim(\mathcal{R}(A^T)) = n - r\)</span>. Note that for any <span class="math inline">\(\bar{x}\)</span> that satisfies <span class="math inline">\(A\bar{x} = 0\)</span>, we have <span class="math inline">\(A^TA\bar{x} = 0\)</span>. It follows that <span class="math inline">\(A\)</span> and <span class="math inline">\(A^TA\)</span> have the same null space, <span class="math inline">\(\mathcal{N}(A) = \mathcal{N}(A^TA)\)</span>. Since <span class="math inline">\(A^TA \in \mathbb{R}^{n \times n}\)</span>, we have <span class="math inline">\(r(A^TA) = \dim(\mathcal{R}((A^TA)^T)) = n - \dim(\mathcal{N}(A^TA)) = r\)</span>.</p>
<p>For <span class="math inline">\(r(A) = R(AA^T)\)</span>, note that <span class="math inline">\(r(A) = r(A^T)\)</span>, and that <span class="math inline">\(\mathcal{N}(A^T) = \mathcal{N}(AA^T)\)</span>, then the conclusion presents itself.</p>
<hr>
<p>From the SVD perspective(Corollary <a href="eigenthings-and-quadratic-forms.html#cor:same-nonzero">4.1</a> and Section <a href="singular-value-decomposition.html#svd-theorem">5.2</a>), one can show that <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> have the same set of nonzero eigenvalues, and <span class="math inline">\(r(A)\)</span> is the same as the number of nonzero eigenvalues of <span class="math inline">\(A^A\)</span> or <span class="math inline">\(AA^T\)</span>, so that <span class="math inline">\(r(A) = r(A^TA) = r(AA^T)\)</span>.</p>
</div>
</div>
<div id="bases-and-coordinate-systems" class="section level2">
<h2><span class="header-section-number">2.6</span> Bases and Coordinate Systems</h2>

<div class="definition">
<span id="def:basis" class="definition"><strong>Definition 2.3  (basis of a vector space)  </strong></span>Let <span class="math inline">\(V\)</span> be a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>. An indexed set of vectors <span class="math inline">\(\mathcal{B} = \{\bar{a}_1, ..., \bar{a}_r\}\)</span> is said to be a basis of <span class="math inline">\(V\)</span>, if <span class="math inline">\(\mathcal{B}\)</span> spans <span class="math inline">\(V\)</span> (these <span class="math inline">\(r\)</span> vectors are linearly independent)
</div>

<p>Note that <span class="math inline">\(\bar{a}_1, ..., \bar{a}_r\)</span> are n-dimensional vectors themselves, though <span class="math inline">\(V\)</span>, the subspace they span, is a hyperplane that has dimension strictly less than <span class="math inline">\(n\)</span> when <span class="math inline">\(r &lt; n\)</span>.</p>

<div class="definition">
<span id="def:coordinates" class="definition"><strong>Definition 2.4  (coordinates)  </strong></span>The coordinates of <span class="math inline">\(\bar{v}\)</span> relative to basis <span class="math inline">\(\mathcal{B}\)</span> (or the <span class="math inline">\(\mathcal{B}\)</span>-coordinates of <span class="math inline">\(\bar{v}\)</span>, denoted by <span class="math inline">\([\bar{v}]_{\mathcal{B}}\)</span>) are the weights <span class="math inline">\(x_1, ..., x_r\)</span> such that <span class="math inline">\(\bar{x} = x_1\bar{a}_1 + \cdots + x_r\bar{a}_n\)</span>
</div>


<div class="theorem">
<p><span id="thm:unique-coordinate" class="theorem"><strong>Theorem 2.3  (Unique coordinate vector)  </strong></span>Let <span class="math inline">\(\mathcal{B} = \{\bar{a}_1, ..., \bar{a}_n\}\)</span> be a basis of <span class="math inline">\(V\)</span>. Then for any vector <span class="math inline">\(\bar{v} \in V\)</span>, there exists a unique coordinate vector <span class="math inline">\(\bar{x} = [x_1, ..., x_r]\)</span> such that</p>
<span class="math display">\[
\bar{v} = x_1\bar{a}_1 + \cdots + x_r\bar{a}_r
\]</span>
</div>

<p>Suppose <span class="math inline">\(\bar{v}\)</span> has another representation</p>
<p><span class="math display">\[
\bar{v} = y_1\bar{a}_1 + \cdots + y_r\bar{a}_r
\]</span>
Then, subtracting we have</p>
<p><span class="math display">\[
\bar{0} = \bar{x} - \bar{x} = (x_1 - y_1)\bar{a}_1 + \cdots + (x_r - y_r)\bar{a}_r
\]</span>
Since <span class="math inline">\(\{\bar{a}_1, ..., \bar{a}_r\}\)</span> is a linearly independent set, we have <span class="math inline">\(x_i = y_i\)</span> for <span class="math inline">\(1 \le i \le r\)</span>. Thus, the coordinates <span class="math inline">\(\bar{x}\)</span> of any vector in <span class="math inline">\(V\)</span> in terms of a certain basis is always unique.</p>
<p>Note that the coordinate vector of the n-dimensional vector <span class="math inline">\(\bar{v}\)</span> contains <span class="math inline">\(r\)</span> components instead of <span class="math inline">\(n\)</span>, each of which corresponds to a weight associated with one basis in <span class="math inline">\(\mathcal{B}\)</span></p>
<p>The default basis in <span class="math inline">\(\mathbb{R}^n\)</span>, is the collection of <span class="math inline">\(n\)</span> n-dimensional vectors <span class="math inline">\(\{\bar{e}_1, ..., \bar{e}_n\}\)</span>. Each of <span class="math inline">\(\bar{e}_i\)</span> contains a <span class="math inline">\(1\)</span> in the <span class="math inline">\(i\)</span>th entry and a value of <span class="math inline">\(0\)</span> in all other entries.</p>
<p><span class="math display">\[
\bar{e}_1 = 
\begin{bmatrix}
1 \\
0 \\
\vdots \\
0
\end{bmatrix}
,\, 
\bar{e}_2 = 
\begin{bmatrix}
0 \\
1 \\
\vdots \\
0 \\
\end{bmatrix}
, \, \cdots
,\, \bar{e}_n = 
\begin{bmatrix}
0 \\ 
0 \\
\vdots \\
1 
\end{bmatrix}
\]</span></p>
<p>The next question is, given <span class="math inline">\(\mathcal{B} = \{\bar{a}_1, ..., \bar{a}_r\}\)</span>, how can we find the unique coordinates of <span class="math inline">\(\bar{v}\)</span>, which is by default expressed in terms of the standard basis.</p>
<p>We start by a special case where <span class="math inline">\(\bar{a}_1, ..., \bar{a}_r\)</span> forms a <strong>orthonormal</strong> basis of <span class="math inline">\(V\)</span>, then the coordinates are simply the dot products of <span class="math inline">\(\bar{x}\)</span> with these vectors. By taking the dot product of both sides of <span class="math inline">\(\bar{v} = \sum x_i\bar{a}_i\)</span> with each <span class="math inline">\(\bar{a}_i\)</span> and using <span class="math inline">\(\bar{a}_i^T\bar{a}_i = 1, \, \bar{a}_i^T\bar{a}_j = 0\)</span>, we can show that <span class="math inline">\(x_i = \bar{a}_i^T\bar{v}\)</span>.</p>
<p>This is a frequently used result that, if <span class="math inline">\(\bar{a}_i\)</span> is a unit vector, <span class="math inline">\(\bar{a}_i^T\bar{v}\)</span> will project <span class="math inline">\(\bar{v}\)</span> onto <span class="math inline">\(\bar{a}_i\)</span> and return a coordinate component of <span class="math inline">\(\bar{v}\)</span> in terms of <span class="math inline">\(\bar{a}_i\)</span>. Moreover, we get the whole coordinate vector of <span class="math inline">\(\bar{v}\)</span> with respect to <span class="math inline">\([\bar{a}_1 \, \cdots \bar{a}_r]\)</span> by</p>
<p><span class="math display">\[
\begin{aligned}
A &amp;= [\bar{a}_1 \, \cdots \bar{a}_r] \\
A^T\bar{v} &amp;=
\begin{bmatrix}
\bar{a}_1^T\bar{v} \\
\vdots  \\
\bar{a}_r\bar{v}
\end{bmatrix} 
= 
\begin{bmatrix}
x_1 \\
\vdots \\
x_r \\
\end{bmatrix}
\end{aligned} 
\]</span></p>
<p>When the set <span class="math inline">\(\bar{a}_1, ..., \bar{a}_r\)</span> are not orthonormal, a general strategy is solving the system of equations <span class="math inline">\(A\bar{x} = \bar{v}\)</span>. This boils down to a problem extensively discussed in Chapter <a href="linear-system.html#linear-system">6</a>.</p>
<p>If <span class="math inline">\(V\)</span> is simply <span class="math inline">\(\mathbb{R}^n\)</span>, then <span class="math inline">\(A\)</span> is square and invertible, then <span class="math inline">\(\bar{x}\)</span> is found by <span class="math inline">\(A^{-1}\bar{v}\)</span>. Difficult cases are <span class="math inline">\(A\)</span> is not square, or <span class="math inline">\(\bar{v}\)</span> does not lie in the hyperplane defined by <span class="math inline">\(\bar{a}_1, ..., \bar{a}_r\)</span> and therefore defies valid coordinates. Finding <span class="math inline">\(\bar{x}\)</span> now becomes a classic least square problem (Section <a href="linear-system.html#lesat-squares-problems">6.1</a>). This results in the following (the best fit solution)</p>
<p><span class="math display">\[
\bar{x} = (A^TA)^{-1}A^T\bar{v}
\]</span></p>
<div id="change-of-basis" class="section level3">
<h3><span class="header-section-number">2.6.1</span> Change of Basis</h3>
<p>Previous discussions involve different representations of <span class="math inline">\(\bar{x}\)</span> with respect to two basis, one of which is standard basis in <span class="math inline">\(\mathbb{R}^n\)</span>. This section deals with transformations between two non-standard basis.</p>
<p>Now suppose <span class="math inline">\(A = [\bar{a}_1, ..., \bar{a}_n]\)</span> and <span class="math inline">\(B = [\bar{b}_1, ..., \bar{b}_n]\)</span> are both bases for <span class="math inline">\(\mathbb{R}^n\)</span>. And the vector <span class="math inline">\(\bar{x}\)</span> has coordinates <span class="math inline">\(\bar{x}_a, \bar{x}_b\)</span> with respect to <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> respectively. The goal is to find an <span class="math inline">\(n \times n\)</span> matrix that transforms one coordinate to another</p>
<p><span class="math display">\[
\bar{x}_b = P_{a \rightarrow b}\,\bar{x}
\]</span></p>
<p>To find <span class="math inline">\(P_{a \rightarrow b}\)</span>, we use the fact that <span class="math inline">\(A\bar{x}_b\)</span> and <span class="math inline">\(B\bar{x}_a\)</span> represents the same vector <span class="math inline">\(x\)</span>. We have the following</p>
<p><span class="math display">\[
A\bar{x}_a = B \bar{x}_b = \bar{x}
\]</span></p>
<p>Since <span class="math inline">\(B\)</span> is invertible, we have</p>
<p><span class="math display">\[
\bar{x}_b = \underbrace{B^{-1}A}_{P_{a \rightarrow b}}\bar{x}_a
\]</span></p>
<p><span class="math inline">\(B^{-1}A\)</span> is called the <strong>change-of-basis/coordinate matrix from <span class="math inline">\(\mathcal{A}\)</span> to <span class="math inline">\(\mathcal{B}\)</span></strong>, because it converts coordinates from one system to another. When <span class="math inline">\(B\)</span> is large, finding its inverse can be challenging. Though, when <span class="math inline">\(\{\bar{b}_1, ..., \bar{b}_n\}\)</span> is an orthonormal basis, the transformation matrix simplify to <span class="math inline">\(B^TA\)</span>. Additionally, if <span class="math inline">\(A\)</span> corresponds to the standard basis, the transformation matrix is <span class="math inline">\(B^T\)</span>, as shown before.</p>
<p>Such change-of-basis transformation can be performed between basis sets that define some other r-dimensional hyperplane <span class="math inline">\(V\)</span> rather than <span class="math inline">\(\mathbb{R}^n\)</span>. Moreover, the two basis set could even represent different hyperplane in <span class="math inline">\(\mathbb{R}^n\)</span>. We demonstrate such a case below.</p>
<p>Let <span class="math inline">\(A = [\bar{a}_1 \, \cdots \, \bar{a}_r]\)</span> and <span class="math inline">\(B = [\bar{b}_1 \, \cdots \, \bar{b}_r]\)</span> be two matrices whose columns are bases from different hyperplanes in <span class="math inline">\(\mathbb{R}^n\)</span>, and let <span class="math inline">\(\bar{v}\)</span> lie in <span class="math inline">\(A\)</span>’s hyperplane. In this case, <span class="math inline">\(A\bar{x}_a \not = B\bar{x}_b\)</span> because <span class="math inline">\(B\bar{x}_b\)</span> is only a best-fit solution of <span class="math inline">\(\bar{v}\)</span>. This is again a least square problem. We have the following</p>
<p><span class="math display">\[
B^T(A\bar{x}_a - B\bar{x}_b) = \bar{0}
\]</span>
Therefore</p>
<p><span class="math display">\[
\bar{x}_b = \underbrace{(B^TB)^{-1}B^TA}_{P_{a \rightarrow b}}\bar{x}_a
\]</span></p>
<p><br></p>
<p>Change of basis has various applications in machine learning, such as discrete wavelet transform (P.60, Ch2, <span class="citation">Aggarwal (<a href="references.html#ref-DBLP:books/sp/Aggarwal20" role="doc-biblioref">2020</a>)</span>). In time series analysis, a time series of length <span class="math inline">\(n\)</span> would result in a vector from <span class="math inline">\(\mathbb{R}^{n}\)</span>, and <span class="math inline">\(n\)</span> can be extremely large. Since consecutive sample points tend to be similar, we may take larger interest in a few variations across time, rather than the whole long vector. The <em>Haar wavelet transformation</em> performs a basis transformation with a view to extracting important variations, making the original time series vector reasonably sparse.</p>
</div>
</div>
<div id="complexity-of-vector-computations" class="section level2">
<h2><span class="header-section-number">2.7</span> Complexity of Vector Computations</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="basic-matrix-algebra.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="orthogonality.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/enixam/math-foundations/edit/master/vector-spaces.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
