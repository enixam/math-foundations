<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Vector spaces | Mathematical Notes for Machine Learning</title>
  <meta name="description" content="Chapter 2 Vector spaces | Mathematical Notes for Machine Learning" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Vector spaces | Mathematical Notes for Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="enixam/math-foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Vector spaces | Mathematical Notes for Machine Learning" />
  
  
  

<meta name="author" content="Qiushi Yan" />


<meta name="date" content="2020-07-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="matrix-basics.html"/>
<link rel="next" href="orthogonality.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Math foundations in Machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Linear Algebra</b></span></li>
<li class="chapter" data-level="1" data-path="matrix-basics.html"><a href="matrix-basics.html"><i class="fa fa-check"></i><b>1</b> Matrix basics</a><ul>
<li class="chapter" data-level="1.1" data-path="matrix-basics.html"><a href="matrix-basics.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.1</b> Matrix multiplication</a></li>
<li class="chapter" data-level="1.2" data-path="matrix-basics.html"><a href="matrix-basics.html#elemetary-matrix-and-row-operations"><i class="fa fa-check"></i><b>1.2</b> Elemetary matrix and row operations</a></li>
<li class="chapter" data-level="1.3" data-path="matrix-basics.html"><a href="matrix-basics.html#lu-factorization"><i class="fa fa-check"></i><b>1.3</b> LU factorization</a></li>
<li class="chapter" data-level="1.4" data-path="matrix-basics.html"><a href="matrix-basics.html#determinants"><i class="fa fa-check"></i><b>1.4</b> Determinants</a><ul>
<li class="chapter" data-level="1.4.1" data-path="matrix-basics.html"><a href="matrix-basics.html#cofactor-expansion"><i class="fa fa-check"></i><b>1.4.1</b> Cofactor expansion</a></li>
<li class="chapter" data-level="1.4.2" data-path="matrix-basics.html"><a href="matrix-basics.html#geometric-interpretation-of-determinant"><i class="fa fa-check"></i><b>1.4.2</b> Geometric interpretation of determinant</a></li>
<li class="chapter" data-level="1.4.3" data-path="matrix-basics.html"><a href="matrix-basics.html#properties-of-determinant"><i class="fa fa-check"></i><b>1.4.3</b> Properties of determinant</a></li>
<li class="chapter" data-level="1.4.4" data-path="matrix-basics.html"><a href="matrix-basics.html#cramers-rule"><i class="fa fa-check"></i><b>1.4.4</b> Cramer’s rule</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="matrix-basics.html"><a href="matrix-basics.html#trace"><i class="fa fa-check"></i><b>1.5</b> Trace</a></li>
<li class="chapter" data-level="1.6" data-path="matrix-basics.html"><a href="matrix-basics.html#inverse-of-a-matrix"><i class="fa fa-check"></i><b>1.6</b> Inverse of a matrix</a></li>
<li class="chapter" data-level="1.7" data-path="matrix-basics.html"><a href="matrix-basics.html#matrix-multiplication-as-linear-transformation"><i class="fa fa-check"></i><b>1.7</b> Matrix multiplication as linear transformation</a></li>
<li class="chapter" data-level="1.8" data-path="matrix-basics.html"><a href="matrix-basics.html#statistics-and-proabability"><i class="fa fa-check"></i><b>1.8</b> Statistics and proabability</a><ul>
<li class="chapter" data-level="1.8.1" data-path="matrix-basics.html"><a href="matrix-basics.html#sample-statistics"><i class="fa fa-check"></i><b>1.8.1</b> Sample statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="vector-spaces.html"><a href="vector-spaces.html"><i class="fa fa-check"></i><b>2</b> Vector spaces</a><ul>
<li class="chapter" data-level="2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#vector-space"><i class="fa fa-check"></i><b>2.1</b> Vector space</a><ul>
<li class="chapter" data-level="2.1.1" data-path="vector-spaces.html"><a href="vector-spaces.html#euclidean-space"><i class="fa fa-check"></i><b>2.1.1</b> Euclidean space</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="vector-spaces.html"><a href="vector-spaces.html#metric-spaces-normed-spaces-inner-product-spaces"><i class="fa fa-check"></i><b>2.2</b> Metric spaces, normed spaces, inner product spaces</a></li>
<li class="chapter" data-level="2.3" data-path="vector-spaces.html"><a href="vector-spaces.html#subspaces"><i class="fa fa-check"></i><b>2.3</b> Subspaces</a></li>
<li class="chapter" data-level="2.4" data-path="vector-spaces.html"><a href="vector-spaces.html#fundamental-theorem"><i class="fa fa-check"></i><b>2.4</b> Fundamental theorem of linear algebra</a></li>
<li class="chapter" data-level="2.5" data-path="vector-spaces.html"><a href="vector-spaces.html#rank"><i class="fa fa-check"></i><b>2.5</b> Rank</a><ul>
<li class="chapter" data-level="2.5.1" data-path="vector-spaces.html"><a href="vector-spaces.html#effect-of-operations-on-matrix-rank"><i class="fa fa-check"></i><b>2.5.1</b> Effect of operations on matrix rank</a></li>
<li class="chapter" data-level="2.5.2" data-path="vector-spaces.html"><a href="vector-spaces.html#gram-matrix"><i class="fa fa-check"></i><b>2.5.2</b> Gram matrix</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="orthogonality.html"><a href="orthogonality.html"><i class="fa fa-check"></i><b>3</b> Orthogonality</a><ul>
<li class="chapter" data-level="3.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition"><i class="fa fa-check"></i><b>3.1</b> Orthogonal decomposition</a><ul>
<li class="chapter" data-level="3.1.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-complements"><i class="fa fa-check"></i><b>3.1.1</b> Orthogonal complements</a></li>
<li class="chapter" data-level="3.1.2" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-sets-and-orthogonal-basis"><i class="fa fa-check"></i><b>3.1.2</b> Orthogonal sets and orthogonal basis</a></li>
<li class="chapter" data-level="3.1.3" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition-1"><i class="fa fa-check"></i><b>3.1.3</b> Orthogonal decomposition</a></li>
<li class="chapter" data-level="3.1.4" data-path="orthogonality.html"><a href="orthogonality.html#best-approximation"><i class="fa fa-check"></i><b>3.1.4</b> Best approximation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="orthogonality.html"><a href="orthogonality.html#idempotent-and-projection-matrices"><i class="fa fa-check"></i><b>3.2</b> Idempotent and Projection Matrices</a></li>
<li class="chapter" data-level="3.3" data-path="orthogonality.html"><a href="orthogonality.html#gram-schmidt-process"><i class="fa fa-check"></i><b>3.3</b> Gram-Schmidt process</a><ul>
<li class="chapter" data-level="3.3.1" data-path="orthogonality.html"><a href="orthogonality.html#qr-factorizaiton"><i class="fa fa-check"></i><b>3.3.1</b> QR factorizaiton</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="orthogonality.html"><a href="orthogonality.html#orthonormal-sets-and-orthogonal-matrices"><i class="fa fa-check"></i><b>3.4</b> Orthonormal sets and orthogonal matrices</a><ul>
<li class="chapter" data-level="3.4.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-matrices"><i class="fa fa-check"></i><b>3.4.1</b> Orthogonal matrices</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="orthogonality.html"><a href="orthogonality.html#lesat-squares-problems"><i class="fa fa-check"></i><b>3.5</b> Lesat squares problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html"><i class="fa fa-check"></i><b>4</b> Eigenthings and quadratic forms</a><ul>
<li class="chapter" data-level="4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>4.1</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#additional-properties-of-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>4.2</b> Additional properties of eigenvalues and eigenvectors</a></li>
<li class="chapter" data-level="4.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#diagnolization-and-similar-matrices"><i class="fa fa-check"></i><b>4.3</b> Diagnolization and similar matrices</a><ul>
<li class="chapter" data-level="4.3.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#similarity"><i class="fa fa-check"></i><b>4.3.1</b> Similarity</a></li>
<li class="chapter" data-level="4.3.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#jordan-matrix"><i class="fa fa-check"></i><b>4.3.2</b> Jordan matrix</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#symmetric-matrices"><i class="fa fa-check"></i><b>4.4</b> Symmetric matrices</a><ul>
<li class="chapter" data-level="4.4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#spectral-decomposition"><i class="fa fa-check"></i><b>4.4.1</b> Spectral decomposition</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#quadratic-forms"><i class="fa fa-check"></i><b>4.5</b> Quadratic forms</a><ul>
<li class="chapter" data-level="4.5.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#change-of-variabele"><i class="fa fa-check"></i><b>4.5.1</b> Change of variabele</a></li>
<li class="chapter" data-level="4.5.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#classification-of-quadratic-forms"><i class="fa fa-check"></i><b>4.5.2</b> Classification of quadratic forms</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#rayleigh-quotients"><i class="fa fa-check"></i><b>4.6</b> Rayleigh quotients</a></li>
<li class="chapter" data-level="4.7" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#svd"><i class="fa fa-check"></i><b>4.7</b> SVD</a><ul>
<li class="chapter" data-level="4.7.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#singular-values-of-m-x-n-matrix"><i class="fa fa-check"></i><b>4.7.1</b> Singular values of m x n matrix</a></li>
<li class="chapter" data-level="4.7.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#svd-theorem"><i class="fa fa-check"></i><b>4.7.2</b> The singular value decomposition</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#matrix-norms"><i class="fa fa-check"></i><b>4.8</b> Matrix norms</a><ul>
<li class="chapter" data-level="4.8.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#induced-norms"><i class="fa fa-check"></i><b>4.8.1</b> Induced norms</a></li>
<li class="chapter" data-level="4.8.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#entry-wise-norm"><i class="fa fa-check"></i><b>4.8.2</b> Entry-wise norm</a></li>
<li class="chapter" data-level="4.8.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#other-matrix-norms"><i class="fa fa-check"></i><b>4.8.3</b> Other matrix norms</a></li>
<li class="chapter" data-level="4.8.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#unitary-invariancy"><i class="fa fa-check"></i><b>4.8.4</b> Unitary invariancy</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#low-rank-optimization"><i class="fa fa-check"></i><b>4.9</b> Low rank optimization</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="solutions-of-linear-system-ax-b.html"><a href="solutions-of-linear-system-ax-b.html"><i class="fa fa-check"></i><b>5</b> Solutions of linear system Ax = b</a><ul>
<li class="chapter" data-level="5.1" data-path="solutions-of-linear-system-ax-b.html"><a href="solutions-of-linear-system-ax-b.html#generalized-inverse"><i class="fa fa-check"></i><b>5.1</b> Generalized inverse</a></li>
<li class="chapter" data-level="5.2" data-path="solutions-of-linear-system-ax-b.html"><a href="solutions-of-linear-system-ax-b.html#ill-conditioned-matrices"><i class="fa fa-check"></i><b>5.2</b> Ill-conditioned matrices</a><ul>
<li class="chapter" data-level="5.2.1" data-path="solutions-of-linear-system-ax-b.html"><a href="solutions-of-linear-system-ax-b.html#the-condition-number"><i class="fa fa-check"></i><b>5.2.1</b> The condition number</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="matrix-calculus.html"><a href="matrix-calculus.html"><i class="fa fa-check"></i><b>6</b> Matrix calculus</a></li>
<li class="part"><span><b>II Calculus</b></span></li>
<li class="chapter" data-level="7" data-path="taylor-series-and-expansion.html"><a href="taylor-series-and-expansion.html"><i class="fa fa-check"></i><b>7</b> Taylor series and expansion</a></li>
<li class="part"><span><b>III Probability Theory</b></span></li>
<li class="chapter" data-level="8" data-path="probability-basics.html"><a href="probability-basics.html"><i class="fa fa-check"></i><b>8</b> Probability basics</a></li>
<li class="chapter" data-level="9" data-path="moments.html"><a href="moments.html"><i class="fa fa-check"></i><b>9</b> Moments</a></li>
<li class="part"><span><b>IV Applications</b></span></li>
<li class="chapter" data-level="10" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>10</b> Linear models</a><ul>
<li class="chapter" data-level="10.1" data-path="linear-models.html"><a href="linear-models.html#least-square-estimation"><i class="fa fa-check"></i><b>10.1</b> Least square estimation</a></li>
<li class="chapter" data-level="10.2" data-path="linear-models.html"><a href="linear-models.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>10.2</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="10.3" data-path="linear-models.html"><a href="linear-models.html#weighted-least-squares"><i class="fa fa-check"></i><b>10.3</b> Weighted least squares</a></li>
<li class="chapter" data-level="10.4" data-path="linear-models.html"><a href="linear-models.html#partial-least-squres"><i class="fa fa-check"></i><b>10.4</b> Partial least squres</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>11</b> Principal component analysis</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li>
  <a href="https://github.com/rstudio/bookdown" target="blank">Written with bookdown</a>
</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Notes for Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="vector-spaces" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Vector spaces</h1>
<p>Vector spaces, metric spaces, normed spaces, and inner product spaces are places where computations in linear algebra happen. These spaces are defined more or less to generalize properties of Euclidean space.</p>
<div id="vector-space" class="section level2">
<h2><span class="header-section-number">2.1</span> Vector space</h2>
<p>A <strong>vector space</strong> <span class="math inline">\(V\)</span> is a nonempty set, the elements of which are called vectors, also called <em>linear spaces</em>. A vector space comes with two operations predefined: vectors can be added together, and vectors can be multiplied by real numbers called scalars. For all vectors <span class="math inline">\(\boldsymbol{u}, \boldsymbol{v}\)</span> and <span class="math inline">\(\boldsymbol{w}\)</span>, and all scalars <span class="math inline">\(c\)</span> and <span class="math inline">\(d\)</span> in <span class="math inline">\(V\)</span>, the following axioms of vector space must hold:</p>
<ol style="list-style-type: decimal">
<li><p>The sum of <span class="math inline">\(\boldsymbol{u}\)</span> and <span class="math inline">\(\boldsymbol{v}\)</span> is in <span class="math inline">\(V\)</span></p></li>
<li><p>The scalar multiple of <span class="math inline">\(\boldsymbol{u}\)</span> by c, denoted by <span class="math inline">\(c\boldsymbol{u}\)</span>, is in <span class="math inline">\(V\)</span></p></li>
<li><p>There exists additive identity (denoted by <span class="math inline">\(\boldsymbol{0}\)</span>) such that <span class="math inline">\(\boldsymbol{x} + \boldsymbol{0} = \boldsymbol{x}\)</span>. Similarly, multiplicative identity (written <span class="math inline">\(\boldsymbol{1}\)</span>) means <span class="math inline">\(\boldsymbol{1}\boldsymbol{x} = \boldsymbol{x}\)</span></p></li>
<li><p>There exists an additive inverse (written <span class="math inline">\(-\boldsymbol{x}\)</span>) such that <span class="math inline">\(-\boldsymbol{x} + \boldsymbol{x} = \boldsymbol{0}\)</span></p></li>
<li><p>Communitivity: <span class="math inline">\(\boldsymbol{x} + \boldsymbol{y} = \boldsymbol{y} + \boldsymbol{x}\)</span></p></li>
<li><p>Associativity: <span class="math inline">\((\boldsymbol{x} + \boldsymbol{y}) + \boldsymbol{z} = \boldsymbol{x} + (\boldsymbol{y} + \boldsymbol{z})\)</span>, and <span class="math inline">\(\alpha(\beta\boldsymbol{x}) = (\alpha\beta)\boldsymbol{x}\)</span></p></li>
<li><p>Distributivity: <span class="math inline">\(\alpha(\boldsymbol{x} + \boldsymbol{y}) = \alpha\boldsymbol{x} + \alpha\boldsymbol{y}\)</span> and <span class="math inline">\((\alpha + \beta)\boldsymbol{x} = \alpha\boldsymbol{x} + \beta\boldsymbol{x}\)</span></p></li>
</ol>
<p>A set of vectors <span class="math inline">\(\boldsymbol{v}_1, ..., \boldsymbol{v}_n \in V\)</span> are set to be <em>linearly independent</em> if</p>
<p><span class="math display">\[
\begin{align*}
c_1\boldsymbol{v}_1 + \cdots + c_n\boldsymbol{v}_n &amp;= 0 &amp;&amp; \text{implies } c_1 = \cdots = c_n
\end{align*}
\]</span></p>
<p>The <strong>span</strong> of <span class="math inline">\(\boldsymbol{v}_1, ..., \boldsymbol{v}_n\)</span> is the set of all vectors that can be expressed as a linear combination of them.</p>
<div id="euclidean-space" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Euclidean space</h3>
<p>Euclidean space is the quintessential vector space, denoted by <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
<p>The two must-have operations of vector spaces are valid in <span class="math inline">\(\mathbb{R}^n\)</span></p>
<p><span class="math display">\[
\boldsymbol{x} = 
\begin{bmatrix}
x_1 \\
\vdots \\
x_n
\end{bmatrix}, \;\;
\boldsymbol{y} = 
\begin{bmatrix}
y_1 \\
\vdots \\
y_n
\end{bmatrix}, \;\;
\alpha \in \mathbb{R} \\
\boldsymbol{x} + \boldsymbol{y} = 
\begin{bmatrix}
x_1 + y_1 \\
\vdots \\
x_n + y_n
\end{bmatrix}, \;\;
\alpha\boldsymbol{x} = 
\begin{bmatrix}
\alpha x_1 \\
\vdots \\
\alpha x_n
\end{bmatrix}
\]</span></p>
<p>Euclidean spaces have other structures defined in addition to the plainest vector space. We can calculate <em>dot product</em>, <em>length</em>, <em>distance</em> by</p>
<p><span class="math display">\[
\begin{aligned}
\text{dot product between }  \boldsymbol{x} \text{ and } \boldsymbol{y}: \boldsymbol{x} \cdot\boldsymbol{y} &amp;= \sum_{i=1}^{n}{x_iy_i} \\
\text{length of } \boldsymbol{x}: \|\boldsymbol{x}\| &amp;= \sqrt{\boldsymbol{x} \cdot \boldsymbol{x}} \\
\text{distance between } \boldsymbol{x} \text{ and } \boldsymbol{y}: \text{dist}(\boldsymbol{x}, \boldsymbol{y}) &amp;= \|\boldsymbol{x} - \boldsymbol{y}\|
\end{aligned}
\]</span></p>
</div>
</div>
<div id="metric-spaces-normed-spaces-inner-product-spaces" class="section level2">
<h2><span class="header-section-number">2.2</span> Metric spaces, normed spaces, inner product spaces</h2>
<p>Metric spaces, normed spaces and inner product spaces capture important properties of Euclidean space in a more general way (distance, length, angle).</p>
<p>Although metric spaces are not required to be vector spaces, it is always assumed in linear algebra that this is the case. For this reason, metric spaces are short for “metric linear space”. Normed spaces and inner product spaces are defined to be extensions of metric linear spaces, so that they must be vector spaces.</p>
<hr>
<p>Metrics generalize the notion of distance from Euclidean space. A <em>metric space</em> is a set together with a metric on the set (metric spaces don’t have to be vector spaces). The metric is a function that defines a concept of distance <span class="math inline">\(\in \mathbb{R}\)</span> between any two members of the set. A metric must satisfies the following properties:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(d(x, y) \ge 0\)</span>, with equality if and only if <span class="math inline">\(x = y\)</span>. Distances are non-negative, and the only point at zero distance from <span class="math inline">\(x\)</span> is <span class="math inline">\(x\)</span> itself<br />
</li>
<li><span class="math inline">\(d(x, y) = d(y, x)\)</span>. The distance is a symmetric function.</li>
<li><span class="math inline">\(d(x, z) \le d(x, y) + d(y, z)\)</span>. Distance satisfies triangular inequality.</li>
</ol>
<p><br></p>
<p>Norms generalize the notion of length from Euclidean space.</p>
<p>A <strong>norm</strong> on a real vector space <span class="math inline">\(X\)</span> is a function: <span class="math inline">\(\|\cdot\|: V \rightarrow \mathbb{R}\)</span> that satisfies:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\|x\| \ge 0\)</span> for all <span class="math inline">\(x \in X\)</span>, with equality if and only if <span class="math inline">\(x = \boldsymbol{0}\)</span> (nonnegative)<br />
</li>
<li><span class="math inline">\(\|\lambda x\| = \lambda \|x\|\)</span>, for all <span class="math inline">\(x \in X\)</span> and <span class="math inline">\(\lambda \in \mathbb{R}\)</span> (homogeneous)<br />
</li>
<li><span class="math inline">\(\|x + y\| \le \|x\| + \|y\|\)</span> (triangular inequality)</li>
</ol>
<p><strong>A normed space is a metric space with the metric</strong></p>
<p><span class="math display">\[
d(\boldsymbol{x}, \boldsymbol{y}) = \|\boldsymbol{x} - \boldsymbol{y}\|
\]</span></p>
<p>So a normed space is a special case of metric spaces, a metric spcae may not necessarily has a norm associated with it. One can verify that <span class="math inline">\(d(x, y) = \|x - y\|\)</span> satisfies all properties of a metric.</p>
<p>The most common function for norms on <span class="math inline">\(\mathbb{R}^n\)</span> are listed below, with <span class="math inline">\(x = [x_1, x_2, ..., x_n]\)</span>.</p>
<p><span class="math display">\[
\begin{align*}
\text{1-norm}: \|\boldsymbol{x}\|_1 &amp;= \sum_{i=1}^{n}{|x_i|}\\
\text{2-norm}: \|\boldsymbol{x}\|_2 &amp;= \sqrt{\sum_{i=1}^{n}{x_i}^2} \\
\text{p-norm}: \|\boldsymbol{x}\|_p &amp;= (\sum_{i=1}^{n}{|x_i|}^p)^{\frac{1}{p}} \quad (p \ge 1) \\
\text{maximum norm}: \|\boldsymbol{x}\|_{\infty} &amp;= \max\{|x_1|, |x_2|, ..., |x_n|\}
\end{align*}
\]</span>
1-norm is also called the Manhattan norm.</p>
<p>2-norm is the Euclidean norm, the subscript <span class="math inline">\(2\)</span> can be left out in <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
<p>p-norm is a generalization of 1-norm and 2-norm, requiring <span class="math inline">\(p &gt; 1\)</span>. When <span class="math inline">\(p\)</span> turns infinity, <span class="math inline">\(\|x\|_{\infty}\)</span> is called the maximum norm.</p>
<p><br></p>
<p>An <strong>inner product</strong> on a real vector space <span class="math inline">\(X\)</span> is a function <span class="math inline">\(\langle \cdot, \cdot\rangle: X \times X \rightarrow \mathbb{R}\)</span> satisfying</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\langle x, x \rangle \ge 0\)</span>, with equality if and only if <span class="math inline">\(x = \boldsymbol{0}\)</span><br />
</li>
<li><span class="math inline">\(\langle x, y \rangle = \langle y, x \rangle\)</span></li>
<li><span class="math inline">\(\langle x + y, z\rangle = \langle x, z\rangle + \langle y, z\rangle\)</span> and <span class="math inline">\(\langle \lambda x, y\rangle = \lambda \langle x, y \rangle\)</span></li>
</ol>
<p>A vector sapce equipped with such inner product is called a <strong>inner product space</strong>. Note that <strong>all inner product spaces are normed spaces</strong>, because a inner product induce a norm on a vector space:</p>
<p><span class="math display">\[
\langle \boldsymbol{x}, \boldsymbol{\boldsymbol{x}} \rangle = \|\boldsymbol{x}\|^2  
\]</span>
The <em>standard inner product</em> defined on <span class="math inline">\(\mathbb{R}^{n}\)</span> is the dot product, given by</p>
<p><span class="math display">\[
\langle \boldsymbol{x}, \boldsymbol{y} \rangle = \sum_{i=1}^{n}{x_iy_i} = \boldsymbol{x}^T\boldsymbol{y}
\]</span></p>

<div class="rmdnote">
The abstract spaces—metric spaces, normed spaces, and inner product spaces—are all examples of what are more generally called “topological spaces” (linear topological space if they are assumed to be vector spaces first). These spaces have been given in order of increasing structure. That is, every inner product space is a normed space, and in turn, every normed space is a metric space.
</div>

</div>
<div id="subspaces" class="section level2">
<h2><span class="header-section-number">2.3</span> Subspaces</h2>
<p>If <span class="math inline">\(V\)</span> is a subspace, then <span class="math inline">\(S \subseteq A\)</span> if</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\boldsymbol{0} \in S\)</span></p></li>
<li><p><span class="math inline">\(S\)</span> is closed under addition: if <span class="math inline">\(\boldsymbol{x}, \boldsymbol{y} \in S\)</span>, then <span class="math inline">\(\boldsymbol{x} + \boldsymbol{y} \in S\)</span></p></li>
<li><p><span class="math inline">\(S\)</span> is closed under scalar multiplication if <span class="math inline">\(\boldsymbol{x} \in S, \alpha \in \mathbb{R}\)</span> then <span class="math inline">\(\alpha\boldsymbol{x} \in S\)</span></p></li>
</ol>
<p><span class="math inline">\(V\)</span> is always a subspace of itself.</p>
<p>If <span class="math inline">\(U\)</span> and <span class="math inline">\(W\)</span> are all subspaces of <span class="math inline">\(V\)</span>, then the sum of these two subspaces are defined as</p>
<p><span class="math display">\[
U + W = \{\boldsymbol{u} + \boldsymbol{v} \;| \; \boldsymbol{w} \in U, \boldsymbol{w} \in W \}
\]</span></p>
<p>If <span class="math inline">\(U\)</span> and <span class="math inline">\(W\)</span> are perpendicular, in other words, <span class="math inline">\(U \cap W = \boldsymbol{0}\)</span>. Then <span class="math inline">\(U + W\)</span> are said to be a <strong>direct sum</strong> and written <span class="math inline">\(U \oplus W\)</span>.</p>
<p>Dimensions of sums of subspaces has the following property</p>
<p><span class="math display">\[
\text{dim}(U + W) = \text{dim}(U) + \text{dim}(W) - \text{dim}(U \cap W)
\]</span></p>
<p>It follows that if <span class="math inline">\(U\)</span> is perpendicular to <span class="math inline">\(W\)</span>, <span class="math inline">\(W\)</span>,</p>
<p><span class="math display">\[
\text{dim}(U \oplus W) = \text{dim}(V)  = \text{dim}(U) + \text{dim}(W)
\]</span></p>
</div>
<div id="fundamental-theorem" class="section level2">
<h2><span class="header-section-number">2.4</span> Fundamental theorem of linear algebra</h2>
<p>The <strong>columnspace</strong> (also called <em>range</em>) of matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> is the span of the columns of <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
\mathcal{R}(A) = \{\boldsymbol{v} \in \mathbb{R}^m\;|\; \boldsymbol{v} = A\boldsymbol{x}, \, \boldsymbol{x} \in \mathbb{R}^n\}
\]</span></p>
<p>Similarly, the <strong>rowspace</strong> of <span class="math inline">\(A\)</span> is the span of its rows <span class="math inline">\(\mathcal{R}(A^T)\)</span>.</p>
<p>The <strong>nullspace</strong> (also called <em>kernel</em>) of <span class="math inline">\(A\)</span> is the set of solutions to <span class="math inline">\(A\boldsymbol{v} = \boldsymbol{0}\)</span></p>
<p><span class="math display">\[
\mathcal{N}(A) = \{\boldsymbol{x} \in \mathbb{R}^n\;|\;A\boldsymbol{x} = \boldsymbol{0}\}
\]</span></p>
<p>And the left null space is all <span class="math inline">\(\boldsymbol{x}\)</span> that satisfies <span class="math inline">\(A^T\boldsymbol{x} = \boldsymbol{0}\)</span>. The word “left” in this context stems from the fact that <span class="math inline">\(A^T\boldsymbol{x}= \boldsymbol{0}\)</span> is equivalent to <span class="math inline">\(\boldsymbol{x}^TA=\boldsymbol{0}\)</span> where y “acts” on A from the left.</p>
<p>The relationship between these four subspaces present the fundamental theorem of linear algebra</p>

<div class="theorem">
<p><span id="thm:fundamental-theorem" class="theorem"><strong>Theorem 2.1  (The fundamental theorem of Linear Algebra)  </strong></span></p>
<ul>
<li><p><span class="math inline">\(\mathcal{R}(A) = \mathcal{N}(A^T)^{\perp}\)</span>, and <span class="math inline">\(\dim(\mathcal{R}(A)) + \dim(\mathcal{N}(A^T)) = m\)</span></p></li>
<li><p><span class="math inline">\(\mathcal{R}(A^T) = \mathcal{N}(A)^{\perp}\)</span>, and <span class="math inline">\(\dim(\mathcal{R}(A^T)) + \dim(\mathcal{N}(A)) = n\)</span></p>
</div></li>
</ul>
<p>If the rank (defined next section) of <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> is <span class="math inline">\(r\)</span></p>
<ul>
<li><p><span class="math inline">\(m - r\)</span> is the dimension of the left null space of <span class="math inline">\(A\)</span></p></li>
<li><p><span class="math inline">\(n - r\)</span> is the dimension of the null space of <span class="math inline">\(A\)</span></p></li>
</ul>
</div>
<div id="rank" class="section level2">
<h2><span class="header-section-number">2.5</span> Rank</h2>
<p>The <strong>rank</strong> of a vector space is its dimension.</p>

<div class="definition">
<span id="def:unnamed-chunk-2" class="definition"><strong>Definition 1.1  </strong></span>The rank of a matrix is equal to the rank of its column space, which is the same as the rank of its column space.
</div>

<p>I often think of rank of <span class="math inline">\(A\)</span> as the total volume of information that the matrix can offer.</p>
<p>From the definition of matrix rank, we know that <span class="math inline">\(A\)</span>’s row rank (the dimension of <span class="math inline">\(\mathcal{R}(A^T)\)</span>)equals its column rank <span class="math inline">\(\mathcal{R}(A)\)</span>. A way to verify this are presented below.</p>
<p>All matrices can be reduced into a (possibly rectangular) <em>diagonal matrix</em> with elementary row and column operations. First we can row reduce the matrix into row echelon form, then use column operations to convert positions above the pivot to zero.</p>
<p>Thus, any <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> can be expressed in the following form</p>
<p><span class="math display">\[
RAC = \Lambda \tag{1}
\]</span></p>
<p>where <span class="math inline">\(R\)</span> is the product of the elementary matrices that perform row opertations, and <span class="math inline">\(C\)</span> for column operations. Since <span class="math inline">\(C\)</span> is invertible, we can write</p>
<p><span class="math display">\[
RA = \Lambda C^{-1} \tag{2}
\]</span></p>
<p>From (1) we know that row rank of <span class="math inline">\(A\)</span> is identical to that the number of non-zero entries in <span class="math inline">\(\Lambda\)</span>, on the ground that row operations on <span class="math inline">\(A\)</span> does not change its row rank, and <span class="math inline">\(C^{-1}\)</span> only scale diagonal entries of <span class="math inline">\(\Lambda\)</span> to some multiple. Similarly, <span class="math inline">\(AC = R^{-1}\Lambda\)</span> shows that column rank of <span class="math inline">\(A\)</span> is the same as the number of non-zero diagonal entries in <span class="math inline">\(\Lambda\)</span>. Therefore, row rank <span class="math inline">\(=\)</span> column rank.</p>
<div id="effect-of-operations-on-matrix-rank" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Effect of operations on matrix rank</h3>
<p>Let <span class="math inline">\(A, B \in \mathbb{R}^{m \times n}\)</span> have ranks <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span></p>

<div class="corollary">
<p><span id="cor:unnamed-chunk-3" class="corollary"><strong>Corollary 2.1  </strong></span></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(|a - b| \le r(A + B) \le a + b\)</span></p></li>
<li><p><span class="math inline">\(r(AB) &lt; \min(a, b)\)</span></p>
</div></li>
</ol>
<p><strong>PROOF</strong></p>
<p>For (1), rows / columns of <span class="math inline">\(A + B\)</span> can be expressed as linear combinations of rows / columns of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p>
<p>For (2), Each column of <span class="math inline">\(AB\)</span> is a linear combination of columns of A, and each row is a linear combination of rows of <span class="math inline">\(B\)</span>. Therefore, <span class="math inline">\(r(AB)\)</span> can not exceed either rank of <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>. There is also a corollary on the lower bound of <span class="math inline">\(r(AB)\)</span>, which is <span class="math inline">\(a + b - n\)</span>. Note that <span class="math inline">\(n\)</span> is the shared dimension. I have not found a concise proof about this, but this property leads to a interesting result: when one of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are square and full rank, <span class="math inline">\(\min(r(AB)) = \max(r(AB))\)</span></p>

<div class="corollary">
<p><span id="cor:unnamed-chunk-4" class="corollary"><strong>Corollary 2.2  </strong></span></p>
<ol start="3" style="list-style-type: decimal">
<li><p>Multiplying <span class="math inline">\(A\)</span> with a square matrix <span class="math inline">\(B\)</span> of full rank does not change the rank of <span class="math inline">\(A\)</span>.</p></li>
<li><p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are both singular, then <span class="math inline">\(AB\)</span> is non-singular if and only if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are both non-singular</p>
</div></li>
</ol>
<p><strong>PROOF</strong></p>
<p>Suppose <span class="math inline">\(B\)</span> is <span class="math inline">\(n \times n\)</span>, the minimum rank of <span class="math inline">\(AB\)</span> is <span class="math inline">\(a + n - n = a\)</span>, and the maximum rank is <span class="math inline">\(\min(a, n) = a\)</span>. Thus, multiplying by a full rank matrix preserves rank: <span class="math inline">\(r(AB) = r(A)\)</span>. (4) follows naturally after (3).</p>
</div>
<div id="gram-matrix" class="section level3">
<h3><span class="header-section-number">2.5.2</span> Gram matrix</h3>

<div class="proposition">
<span id="prp:gram-matrix" class="proposition"><strong>Proposition 2.1  (Gram matrix)  </strong></span>The matrix <span class="math inline">\(A^TA\)</span> is said to be the Gram matrix of column space of <span class="math inline">\(A_{m \times n}\)</span>. The columns of <span class="math inline">\(A\)</span> are linearly independent if and only if <span class="math inline">\(A^TA\)</span> is invertible.
</div>

<p><strong>PROOF</strong></p>
<p>When <span class="math inline">\(A^TA\)</span> is invertible, it has rank <span class="math inline">\(n\)</span>. Therefore, each of the factors of <span class="math inline">\(A^TA\)</span> has at least rank <span class="math inline">\(n\)</span>, and this means columns of <span class="math inline">\(A\)</span> are linearly independent (since <span class="math inline">\(r(A) \le \min(m, n)\)</span>).</p>
<p>Similarly, <span class="math inline">\(AA^T\)</span> are called the left Gram matrix of rowspace of <span class="math inline">\(A\)</span>. And <span class="math inline">\(AA^T\)</span> is invertible if and only rows of <span class="math inline">\(A\)</span> are linearly independent.</p>

<div class="proposition">
<p><span id="prp:gram-matrix-rank" class="proposition"><strong>Proposition 2.2  </strong></span>For any matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, <span class="math inline">\(A\)</span>, <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> always have the same rank.</p>
<span class="math display">\[
r(A) = r(A^TA) = r(AA^T)
\]</span>
</div>

<p><strong>PROOF</strong></p>
<p>For <span class="math inline">\(r(A) = r(A^TA)\)</span>, suppose <span class="math inline">\(r(A) = r\)</span>, then <span class="math inline">\(\dim(\mathcal{N}(A)) = n - \dim(\mathcal{R}(A^T)) = n - r\)</span>. Note that for any <span class="math inline">\(\boldsymbol{x}\)</span> that satisfies <span class="math inline">\(A\boldsymbol{x} = 0\)</span>, we have <span class="math inline">\(A^TA\boldsymbol{x} = 0\)</span>. It follows that <span class="math inline">\(A\)</span> and <span class="math inline">\(A^TA\)</span> have the same null space, <span class="math inline">\(\mathcal{N}(A) = \mathcal{N}(A^TA)\)</span>. Since <span class="math inline">\(A^TA \in \mathbb{R}^{n \times n}\)</span>, we have <span class="math inline">\(r(A^TA) = \dim(\mathcal{R}((A^TA)^T)) = n - \dim(\mathcal{N}(A^TA)) = r\)</span>.</p>
<p>For <span class="math inline">\(r(A) = R(AA^T)\)</span>, note that <span class="math inline">\(r(A) = r(A^T)\)</span>, and that <span class="math inline">\(\mathcal{N}(A^T) = \mathcal{N}(AA^T)\)</span>, then the conclusion presents itself.</p>
<hr>
<p>From the SVD perspective(Corollary <a href="eigenthings-and-quadratic-forms.html#cor:same-nonzero">4.1</a> and Section <a href="eigenthings-and-quadratic-forms.html#svd-theorem">4.7.2</a>), one can show that <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> have the same set of nonzero eigenvalues, and <span class="math inline">\(r(A)\)</span> is the same as the number of nonzero eigenvalues of <span class="math inline">\(A^A\)</span> or <span class="math inline">\(AA^T\)</span>, so that <span class="math inline">\(r(A) = r(A^TA) = r(AA^T)\)</span>.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="matrix-basics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="orthogonality.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": "https://github.com/enixam/math-foundations/edit/master/vector-spaces.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
