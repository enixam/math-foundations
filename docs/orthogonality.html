<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Orthogonality | Mathematical Notes for Machine Learning</title>
  <meta name="description" content="Chapter 3 Orthogonality | Mathematical Notes for Machine Learning" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Orthogonality | Mathematical Notes for Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="enixam/math-foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Orthogonality | Mathematical Notes for Machine Learning" />
  
  
  

<meta name="author" content="Qiushi Yan" />


<meta name="date" content="2020-07-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="vector-spaces.html"/>
<link rel="next" href="eigenthings-and-quadratic-forms.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Math foundations in Machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Linear Algebra</b></span></li>
<li class="chapter" data-level="1" data-path="matrix-basics.html"><a href="matrix-basics.html"><i class="fa fa-check"></i><b>1</b> Matrix basics</a><ul>
<li class="chapter" data-level="1.1" data-path="matrix-basics.html"><a href="matrix-basics.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.1</b> Matrix multiplication</a></li>
<li class="chapter" data-level="1.2" data-path="matrix-basics.html"><a href="matrix-basics.html#elemetary-matrix-and-row-operations"><i class="fa fa-check"></i><b>1.2</b> Elemetary matrix and row operations</a></li>
<li class="chapter" data-level="1.3" data-path="matrix-basics.html"><a href="matrix-basics.html#lu-factorization"><i class="fa fa-check"></i><b>1.3</b> LU factorization</a></li>
<li class="chapter" data-level="1.4" data-path="matrix-basics.html"><a href="matrix-basics.html#determinants"><i class="fa fa-check"></i><b>1.4</b> Determinants</a><ul>
<li class="chapter" data-level="1.4.1" data-path="matrix-basics.html"><a href="matrix-basics.html#cofactor-expansion"><i class="fa fa-check"></i><b>1.4.1</b> Cofactor expansion</a></li>
<li class="chapter" data-level="1.4.2" data-path="matrix-basics.html"><a href="matrix-basics.html#geometric-interpretation-of-determinant"><i class="fa fa-check"></i><b>1.4.2</b> Geometric interpretation of determinant</a></li>
<li class="chapter" data-level="1.4.3" data-path="matrix-basics.html"><a href="matrix-basics.html#properties-of-determinant"><i class="fa fa-check"></i><b>1.4.3</b> Properties of determinant</a></li>
<li class="chapter" data-level="1.4.4" data-path="matrix-basics.html"><a href="matrix-basics.html#cramers-rule"><i class="fa fa-check"></i><b>1.4.4</b> Cramerâ€™s rule</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="matrix-basics.html"><a href="matrix-basics.html#trace"><i class="fa fa-check"></i><b>1.5</b> Trace</a></li>
<li class="chapter" data-level="1.6" data-path="matrix-basics.html"><a href="matrix-basics.html#inverse-of-a-matrix"><i class="fa fa-check"></i><b>1.6</b> Inverse of a matrix</a></li>
<li class="chapter" data-level="1.7" data-path="matrix-basics.html"><a href="matrix-basics.html#matrix-multiplication-as-linear-transformation"><i class="fa fa-check"></i><b>1.7</b> Matrix multiplication as linear transformation</a></li>
<li class="chapter" data-level="1.8" data-path="matrix-basics.html"><a href="matrix-basics.html#statistics-and-proabability"><i class="fa fa-check"></i><b>1.8</b> Statistics and proabability</a><ul>
<li class="chapter" data-level="1.8.1" data-path="matrix-basics.html"><a href="matrix-basics.html#sample-statistics"><i class="fa fa-check"></i><b>1.8.1</b> Sample statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="vector-spaces.html"><a href="vector-spaces.html"><i class="fa fa-check"></i><b>2</b> Vector spaces</a><ul>
<li class="chapter" data-level="2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#vector-space"><i class="fa fa-check"></i><b>2.1</b> Vector space</a><ul>
<li class="chapter" data-level="2.1.1" data-path="vector-spaces.html"><a href="vector-spaces.html#euclidean-space"><i class="fa fa-check"></i><b>2.1.1</b> Euclidean space</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="vector-spaces.html"><a href="vector-spaces.html#metric-spaces-normed-spaces-inner-product-spaces"><i class="fa fa-check"></i><b>2.2</b> Metric spaces, normed spaces, inner product spaces</a></li>
<li class="chapter" data-level="2.3" data-path="vector-spaces.html"><a href="vector-spaces.html#subspaces"><i class="fa fa-check"></i><b>2.3</b> Subspaces</a></li>
<li class="chapter" data-level="2.4" data-path="vector-spaces.html"><a href="vector-spaces.html#fundamental-theorem"><i class="fa fa-check"></i><b>2.4</b> Fundamental theorem of linear algebra</a></li>
<li class="chapter" data-level="2.5" data-path="vector-spaces.html"><a href="vector-spaces.html#rank"><i class="fa fa-check"></i><b>2.5</b> Rank</a><ul>
<li class="chapter" data-level="2.5.1" data-path="vector-spaces.html"><a href="vector-spaces.html#effect-of-operations-on-matrix-rank"><i class="fa fa-check"></i><b>2.5.1</b> Effect of operations on matrix rank</a></li>
<li class="chapter" data-level="2.5.2" data-path="vector-spaces.html"><a href="vector-spaces.html#gram-matrix"><i class="fa fa-check"></i><b>2.5.2</b> Gram matrix</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="orthogonality.html"><a href="orthogonality.html"><i class="fa fa-check"></i><b>3</b> Orthogonality</a><ul>
<li class="chapter" data-level="3.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition"><i class="fa fa-check"></i><b>3.1</b> Orthogonal decomposition</a><ul>
<li class="chapter" data-level="3.1.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-complements"><i class="fa fa-check"></i><b>3.1.1</b> Orthogonal complements</a></li>
<li class="chapter" data-level="3.1.2" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-sets-and-orthogonal-basis"><i class="fa fa-check"></i><b>3.1.2</b> Orthogonal sets and orthogonal basis</a></li>
<li class="chapter" data-level="3.1.3" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition-1"><i class="fa fa-check"></i><b>3.1.3</b> Orthogonal decomposition</a></li>
<li class="chapter" data-level="3.1.4" data-path="orthogonality.html"><a href="orthogonality.html#best-approximation"><i class="fa fa-check"></i><b>3.1.4</b> Best approximation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="orthogonality.html"><a href="orthogonality.html#idempotent-and-projection-matrices"><i class="fa fa-check"></i><b>3.2</b> Idempotent and Projection Matrices</a></li>
<li class="chapter" data-level="3.3" data-path="orthogonality.html"><a href="orthogonality.html#gram-schmidt-process"><i class="fa fa-check"></i><b>3.3</b> Gram-Schmidt process</a><ul>
<li class="chapter" data-level="3.3.1" data-path="orthogonality.html"><a href="orthogonality.html#qr-factorizaiton"><i class="fa fa-check"></i><b>3.3.1</b> QR factorizaiton</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="orthogonality.html"><a href="orthogonality.html#orthonormal-sets-and-orthogonal-matrices"><i class="fa fa-check"></i><b>3.4</b> Orthonormal sets and orthogonal matrices</a><ul>
<li class="chapter" data-level="3.4.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-matrices"><i class="fa fa-check"></i><b>3.4.1</b> Orthogonal matrices</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="orthogonality.html"><a href="orthogonality.html#lesat-squares-problems"><i class="fa fa-check"></i><b>3.5</b> Lesat squares problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html"><i class="fa fa-check"></i><b>4</b> Eigenthings and quadratic forms</a><ul>
<li class="chapter" data-level="4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>4.1</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#additional-properties-of-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>4.2</b> Additional properties of eigenvalues and eigenvectors</a></li>
<li class="chapter" data-level="4.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#diagnolization-and-similar-matrices"><i class="fa fa-check"></i><b>4.3</b> Diagnolization and similar matrices</a><ul>
<li class="chapter" data-level="4.3.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#similarity"><i class="fa fa-check"></i><b>4.3.1</b> Similarity</a></li>
<li class="chapter" data-level="4.3.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#jordan-matrix"><i class="fa fa-check"></i><b>4.3.2</b> Jordan matrix</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#symmetric-matrices"><i class="fa fa-check"></i><b>4.4</b> Symmetric matrices</a><ul>
<li class="chapter" data-level="4.4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#spectral-decomposition"><i class="fa fa-check"></i><b>4.4.1</b> Spectral decomposition</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#quadratic-forms"><i class="fa fa-check"></i><b>4.5</b> Quadratic forms</a><ul>
<li class="chapter" data-level="4.5.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#change-of-variabele"><i class="fa fa-check"></i><b>4.5.1</b> Change of variabele</a></li>
<li class="chapter" data-level="4.5.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#classification-of-quadratic-forms"><i class="fa fa-check"></i><b>4.5.2</b> Classification of quadratic forms</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#rayleigh-quotients"><i class="fa fa-check"></i><b>4.6</b> Rayleigh quotients</a></li>
<li class="chapter" data-level="4.7" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#svd"><i class="fa fa-check"></i><b>4.7</b> SVD</a><ul>
<li class="chapter" data-level="4.7.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#singular-values-of-m-x-n-matrix"><i class="fa fa-check"></i><b>4.7.1</b> Singular values of m x n matrix</a></li>
<li class="chapter" data-level="4.7.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#svd-theorem"><i class="fa fa-check"></i><b>4.7.2</b> The singular value decomposition</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#matrix-norms"><i class="fa fa-check"></i><b>4.8</b> Matrix norms</a><ul>
<li class="chapter" data-level="4.8.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#induced-norms"><i class="fa fa-check"></i><b>4.8.1</b> Induced norms</a></li>
<li class="chapter" data-level="4.8.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#entry-wise-norm"><i class="fa fa-check"></i><b>4.8.2</b> Entry-wise norm</a></li>
<li class="chapter" data-level="4.8.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#other-matrix-norms"><i class="fa fa-check"></i><b>4.8.3</b> Other matrix norms</a></li>
<li class="chapter" data-level="4.8.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#unitary-invariancy"><i class="fa fa-check"></i><b>4.8.4</b> Unitary invariancy</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#low-rank-optimization"><i class="fa fa-check"></i><b>4.9</b> Low rank optimization</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="solutions-of-linear-system-ax-b.html"><a href="solutions-of-linear-system-ax-b.html"><i class="fa fa-check"></i><b>5</b> Solutions of linear system Ax = b</a><ul>
<li class="chapter" data-level="5.1" data-path="solutions-of-linear-system-ax-b.html"><a href="solutions-of-linear-system-ax-b.html#generalized-inverse"><i class="fa fa-check"></i><b>5.1</b> Generalized inverse</a></li>
<li class="chapter" data-level="5.2" data-path="solutions-of-linear-system-ax-b.html"><a href="solutions-of-linear-system-ax-b.html#ill-conditioned-matrices"><i class="fa fa-check"></i><b>5.2</b> Ill-conditioned matrices</a><ul>
<li class="chapter" data-level="5.2.1" data-path="solutions-of-linear-system-ax-b.html"><a href="solutions-of-linear-system-ax-b.html#the-condition-number"><i class="fa fa-check"></i><b>5.2.1</b> The condition number</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="matrix-calculus.html"><a href="matrix-calculus.html"><i class="fa fa-check"></i><b>6</b> Matrix calculus</a></li>
<li class="part"><span><b>II Calculus</b></span></li>
<li class="chapter" data-level="7" data-path="taylor-series-and-expansion.html"><a href="taylor-series-and-expansion.html"><i class="fa fa-check"></i><b>7</b> Taylor series and expansion</a></li>
<li class="part"><span><b>III Probability Theory</b></span></li>
<li class="chapter" data-level="8" data-path="probability-basics.html"><a href="probability-basics.html"><i class="fa fa-check"></i><b>8</b> Probability basics</a></li>
<li class="chapter" data-level="9" data-path="moments.html"><a href="moments.html"><i class="fa fa-check"></i><b>9</b> Moments</a></li>
<li class="part"><span><b>IV Applications</b></span></li>
<li class="chapter" data-level="10" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>10</b> Linear models</a><ul>
<li class="chapter" data-level="10.1" data-path="linear-models.html"><a href="linear-models.html#least-square-estimation"><i class="fa fa-check"></i><b>10.1</b> Least square estimation</a></li>
<li class="chapter" data-level="10.2" data-path="linear-models.html"><a href="linear-models.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>10.2</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="10.3" data-path="linear-models.html"><a href="linear-models.html#weighted-least-squares"><i class="fa fa-check"></i><b>10.3</b> Weighted least squares</a></li>
<li class="chapter" data-level="10.4" data-path="linear-models.html"><a href="linear-models.html#partial-least-squres"><i class="fa fa-check"></i><b>10.4</b> Partial least squres</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>11</b> Principal component analysis</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li>
  <a href="https://github.com/rstudio/bookdown" target="blank">Written with bookdown</a>
</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Notes for Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="orthogonality" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Orthogonality</h1>
<div id="orthogonal-decomposition" class="section level2">
<h2><span class="header-section-number">3.1</span> Orthogonal decomposition</h2>
<div id="orthogonal-complements" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Orthogonal complements</h3>
<p>if vector <span class="math inline">\(\boldsymbol{v}\)</span> is orthogonal to every vector in a subspace <span class="math inline">\(W\)</span> of <span class="math inline">\(\mathbb{R^n}\)</span>, then <span class="math inline">\(\boldsymbol{v}\)</span> is said to be orthogonal to <span class="math inline">\(W\)</span>. The subspace that contains the set of vectors that are orthogonal to <span class="math inline">\(W\)</span> is called the <strong>orthogonal complement</strong>, denoted by <span class="math inline">\(W^{\perp}\)</span>.</p>
<p><span class="math display">\[
W^{\perp} = \{\boldsymbol{v} \in W^{\perp} | \;\boldsymbol{v} \perp \boldsymbol{x} \; \text{for all} \; \boldsymbol{x} \in W\}
\]</span></p>
<p>This corresponds to discussions in Section <a href="vector-spaces.html#fundamental-theorem">2.4</a>, where</p>
<p><span class="math display">\[
(\text{row}\,A)^{\perp} = \text{Nul}\,A \\
(\text{col}\,A)^{\perp} = \text{Nul}\,A^T
\]</span>
<br></p>

<div class="theorem">
<span id="thm:unnamed-chunk-1" class="theorem"><strong>Theorem 1.1  </strong></span>If <span class="math inline">\(W\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>, <span class="math inline">\(W^{\perp}\)</span> is also a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>.
</div>

<p>Itâ€™s easy to verify that <span class="math inline">\(W^{\perp}\)</span> is closed under scalar multiplication, and under vector addition, and that any vector in <span class="math inline">\(W\)</span> has <span class="math inline">\(n\)</span> components. So that <span class="math inline">\(W^{\perp}\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^n\)</span></p>
</div>
<div id="orthogonal-sets-and-orthogonal-basis" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Orthogonal sets and orthogonal basis</h3>
<p>An orthogonal set is a set of vectors
<span class="math inline">\(\{\boldsymbol{u_1}, \dots, \boldsymbol{u_p}\}\)</span> in <span class="math inline">\(\mathbb{R^n}\)</span>, in which each pair of distinct vectors is orthogonal: <span class="math inline">\(\boldsymbol{u_i}^{T} \boldsymbol{u_j} = 0 \quad i\not = j\)</span>. Note that the set do not necessarily span the whole <span class="math inline">\(\mathbb{R^n}\)</span>, but a subspace <span class="math inline">\(W\)</span>.</p>
<p>Since vectors in orthogonal sets is mutually perpendicular, they must also be linearly independent and could form a basis for a subspace <span class="math inline">\(W\)</span>. In such case, they are called <strong>orthogonal basis</strong>.</p>
<p>There is a particular advantage in using orthogonal basis rather than other basis, because we can find a easy representation of any vector in <span class="math inline">\(W\)</span>.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-2" class="theorem"><strong>Theorem 3.1  </strong></span>For each <span class="math inline">\(\boldsymbol{y}\)</span> in <span class="math inline">\(W\)</span>, there exists a linear combination</p>
<p><span class="math display">\[
y = c_1\boldsymbol{u_1} + \cdots + c_p\boldsymbol{u_p}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
c_i = \frac{\boldsymbol{y} \cdot \boldsymbol{u_i}}{\boldsymbol{u_i} \cdot \boldsymbol{u_i}} \quad i = 1, \cdots, p
\]</span></p>
where <span class="math inline">\(\{\boldsymbol{u_1}, \dots, \boldsymbol{u_p}\}\)</span> is an orthogonal basis.
</div>

<p><strong>Proof</strong></p>
<p><span class="math display">\[
\begin{split}
\boldsymbol{u_1} \cdot \boldsymbol{y} &amp;= \boldsymbol{u_1} \cdot (c_1\boldsymbol{u_1} + \cdots + c_p\boldsymbol{u_p}) \\
  &amp;= c_1 \boldsymbol{u_1} \cdot \boldsymbol{u_1}
\end{split}
\]</span>
So:</p>
<p><span class="math display">\[
c_1 = \frac{\boldsymbol{u_1} \cdot \boldsymbol{y}}{\boldsymbol{u_1} \cdot \boldsymbol{u_1}}
\]</span></p>
<p>Derivations for other <span class="math inline">\(c_i\)</span> is similar.</p>
</div>
<div id="orthogonal-decomposition-1" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Orthogonal decomposition</h3>
<p><strong>Orthogonal decomposition</strong> split <span class="math inline">\(\boldsymbol{y}\)</span> in <span class="math inline">\(\mathbb{R^n}\)</span> into two vectors, one in <span class="math inline">\(W\)</span> and one in its orthogonal compliment <span class="math inline">\(W^{\perp}\)</span>.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-3" class="theorem"><strong>Theorem 3.2  </strong></span>Let <span class="math inline">\(\mathbb{R}^n\)</span> be a inner product space and <span class="math inline">\(W\)</span> and subspace of <span class="math inline">\(\mathbb{R}^n\)</span>. Then every <span class="math inline">\(\boldsymbol{v}\)</span> in <span class="math inline">\(W\)</span> can be written uniquely in the form</p>
<p><span class="math display">\[
\boldsymbol{v} = \boldsymbol{v}_w + \boldsymbol{v}_{\perp}
\]</span></p>
where <span class="math inline">\(\boldsymbol{v}_w \in W\)</span> and <span class="math inline">\(\boldsymbol{v}_{\perp} \in W^{\perp}\)</span>
</div>

<p><strong>PROOF</strong></p>
<p>Let <span class="math inline">\(\boldsymbol{u}_1, ..., \boldsymbol{u}_m\)</span> be a orthonormal basis for <span class="math inline">\(W\)</span>, there exists linear combination according to Section <a href="orthogonality.html#orthogonal-sets-and-orthogonal-basis">3.1.2</a></p>
<p><span class="math display">\[
\boldsymbol{v}_w = (\boldsymbol{v} \cdot \boldsymbol{u}_1)\boldsymbol{u}_1 + \cdots + (\boldsymbol{v} \cdot \boldsymbol{u}_m)\boldsymbol{u}_m
\]</span>
and</p>
<p><span class="math display">\[
\boldsymbol{v}_{\perp} = \boldsymbol{v} - \boldsymbol{v}_w
\]</span>
It is clear that <span class="math inline">\(\boldsymbol{v}_W \in W\)</span>. And we can also show that <span class="math inline">\(\boldsymbol{v}_{\perp}\)</span> is perpendicular to <span class="math inline">\(W\)</span></p>
<p><span class="math display">\[
\begin{split}
\boldsymbol{v}_{\perp} \cdot \boldsymbol{u}_i &amp;= [\boldsymbol{v}- (\boldsymbol{v} \cdot \boldsymbol{u}_1)\boldsymbol{u}_1 - \cdots - (\boldsymbol{v} \cdot \boldsymbol{u}_m)\boldsymbol{u}_m] \cdot \boldsymbol{u}_i \\
&amp;= (\boldsymbol{v} \cdot \boldsymbol{u}_1) - [(\boldsymbol{v} \cdot \boldsymbol{u}_i)\boldsymbol{u}_i \cdot \boldsymbol{u}_i] \\
&amp;= 0
\end{split}
\]</span></p>
<p>which implies <span class="math inline">\(\boldsymbol{v}_{\perp} \in W^{\perp}\)</span>.</p>
<p>To prove that <span class="math inline">\(\boldsymbol{v}_w\)</span> and <span class="math inline">\(\boldsymbol{v}_{\perp}\)</span> are unique (does not depend on the choice of basis), let <span class="math inline">\(\boldsymbol{u}_1&#39;, ..., \boldsymbol{u}_m&#39;\)</span> be another orthonormal basis for <span class="math inline">\(W\)</span>, and define <span class="math inline">\(\boldsymbol{v}_w&#39;\)</span> and <span class="math inline">\(\boldsymbol{v}_{\perp}&#39;\)</span> similarly we want to get <span class="math inline">\(\boldsymbol{v}_w&#39; = \boldsymbol{v}_w\)</span> and <span class="math inline">\(\boldsymbol{v}_{\perp}&#39; = \boldsymbol{v}_{\perp}\)</span>.</p>
<p>By definition</p>
<p><span class="math display">\[
\boldsymbol{v}_w + \boldsymbol{v}_{\perp} = \boldsymbol{v} = \boldsymbol{v}_w&#39; + \boldsymbol{v}_{\perp}&#39; 
\]</span></p>
<p>so</p>
<p><span class="math display">\[
\underbrace{\boldsymbol{v}_w - \boldsymbol{v}_w&#39;}_{\in W} = \underbrace{\boldsymbol{v}_{\perp}&#39; - \boldsymbol{v}_{\perp}}_{\in W^{\perp}}
\]</span>
From the orthogonality of these subspaces, we have</p>
<p><span class="math display">\[
0 = (\boldsymbol{v}_w - \boldsymbol{v}_w&#39;) \cdot (\boldsymbol{v}_{\perp}&#39; - \boldsymbol{v}_{\perp}) = (\boldsymbol{v}_w - \boldsymbol{v}_w&#39;) \cdot (\boldsymbol{v}_w - \boldsymbol{v}_w&#39;) = \|\boldsymbol{v}_w - \boldsymbol{v}_w&#39;\|^2
\]</span>
Similarly we have <span class="math inline">\(\|\boldsymbol{v}_{\perp}&#39; - \boldsymbol{v}_{\perp}\|^2 = 0\)</span>.</p>
<p>The existence and uniqueness of the decomposition above mean that</p>
<p><span class="math display">\[
\mathbb{R}^n = W \oplus W^{\perp}
\]</span></p>
<p>whenever <span class="math inline">\(W\)</span> is a subspace.</p>
</div>
<div id="best-approximation" class="section level3">
<h3><span class="header-section-number">3.1.4</span> Best approximation</h3>

<div class="theorem">
<p><span id="thm:best-approximation" class="theorem"><strong>Theorem 3.3  (The Best Approximation)  </strong></span>Given <span class="math inline">\(\boldsymbol{y}\)</span> be any vector in <span class="math inline">\(\mathbb{R^n}\)</span>, with its subspace <span class="math inline">\(W\)</span>, let <span class="math inline">\(\hat{\boldsymbol{y}}\)</span> be the orthogonal projection of <span class="math inline">\(\boldsymbol{y}\)</span> onto <span class="math inline">\(W\)</span>. Then <span class="math inline">\(\hat{\boldsymbol{y}}\)</span> is the closest point in <span class="math inline">\(W\)</span> to <span class="math inline">\(\boldsymbol{y}\)</span> in the sense that</p>
<span class="math display">\[
\|\boldsymbol{y} - \hat{\boldsymbol{y}}\| \le \|\boldsymbol{y} - \boldsymbol{v}\|
\]</span>
</div>

<p><strong>PROOF</strong></p>
<p>Take <span class="math inline">\(\boldsymbol{v}\)</span> distinct from <span class="math inline">\(\hat{\boldsymbol{y}}\)</span> in <span class="math inline">\(W\)</span>, we know that <span class="math inline">\(\boldsymbol{y} - \hat{\boldsymbol{y}}\)</span> is perpendicular to <span class="math inline">\(\boldsymbol{v}\)</span>. According to Pythoagorean theorem, we have</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="images/best-approximation.png" alt="figure from page p352, ch6 [@lay2006-3]" width="120%" />
<p class="caption">
Figure 3.1: figure from page p352, ch6 <span class="citation">(Lay <a href="references.html#ref-lay2006-3" role="doc-biblioref">2006</a>)</span>
</p>
</div>
<p><span class="math display">\[
\|\boldsymbol{y}-  \boldsymbol{v}\|^2 = \|\boldsymbol{\hat{y}} - \boldsymbol{v}\|^2 + \|\boldsymbol{y} -\boldsymbol{\hat{y}}\|^2 
\]</span>
When <span class="math inline">\(\boldsymbol{v}\)</span> is distinct from <span class="math inline">\(\boldsymbol{\hat{y}}\)</span>, <span class="math inline">\(\|\boldsymbol{\hat{y}} - \boldsymbol{v}\|^2\)</span> is non-negative, so the error term of choosing <span class="math inline">\(\boldsymbol{v}\)</span> is always larger than that of the orthogonal projection <span class="math inline">\(\boldsymbol{\hat{y}}\)</span>.</p>
</div>
</div>
<div id="idempotent-and-projection-matrices" class="section level2">
<h2><span class="header-section-number">3.2</span> Idempotent and Projection Matrices</h2>
<p><span class="math display">\[
\begin{split}
P_S\boldsymbol{v} &amp;= (\boldsymbol{v} \cdot \boldsymbol{u}_1)\boldsymbol{u}_1 + \cdots + (\boldsymbol{v} \cdot \boldsymbol{u}_m)\boldsymbol{u}_m \\
&amp;= \boldsymbol{v}^T\boldsymbol{u}_1\boldsymbol{u}_1 + \cdots +  \boldsymbol{v}^T\boldsymbol{u}_m\boldsymbol{u}_m\\
&amp;= (\boldsymbol{u}_1\boldsymbol{u}_1^T)\boldsymbol{v} + \cdots +  (\boldsymbol{u}_m\boldsymbol{u}_m^T)\boldsymbol{v} \\
&amp;= (\sum_{i=1}^{M}{\boldsymbol{u}_i\boldsymbol{u}_i^T})\boldsymbol{v}\\
&amp;= 
\begin{bmatrix}
\boldsymbol{u}_1 &amp; \cdots &amp; \boldsymbol{u}_m
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{u}_1^T \\
\vdots \\
\boldsymbol{u}_m^T
\end{bmatrix}\boldsymbol{x} \\
&amp;= UU^T\boldsymbol{x}
\end{split}
\]</span></p>
<p>In practical problems, there are times when it is more convenient to use matrix at hand rather than producing an orthonormal basis.</p>
<hr>
<p>Another way to derive projection matrices with matrix calculus</p>
<p><span class="math display">\[
\begin{split}
\|A\boldsymbol{x} - \boldsymbol{b}\|_2^2
&amp;= (A\boldsymbol{x} - \boldsymbol{b})^T(A\boldsymbol{x} - \boldsymbol{b}) \\
&amp;= \boldsymbol{x}^TA^TA\boldsymbol{x} - 2\boldsymbol{b}^TA\boldsymbol{x} + \boldsymbol{b}^T\boldsymbol{b}
\end{split}
\]</span></p>
<p>$$
\begin{split}
_{x}(<sup>TA</sup>TA - 2A + ^T) &amp;=_x(<sup>TA</sup>TA) - _x{2^TA} + _x{^T} \
&amp;= 2(A^TA) - 2A^T</p>
<p>\end{split}
$$</p>
<p><span class="math display">\[
\boldsymbol{x} = (A^TA)^{-1}A^T\boldsymbol{b}
\]</span>
<span class="math display">\[
A\boldsymbol{x} = A(A^TA)^{-1}A^T\boldsymbol{b} = \hat{\boldsymbol{b}}
\]</span></p>
</div>
<div id="gram-schmidt-process" class="section level2">
<h2><span class="header-section-number">3.3</span> Gram-Schmidt process</h2>
<p>Then Gram-Schmidt process is a simple algorithm that transforms a set of linearly independent vectors into orthogonal or orthonormal basis for a subspace. In its essence, it is a sequential projection of <span class="math inline">\(\boldsymbol{x}_{i}\)</span> onto the space spanned by the previously created orthogonal set <span class="math inline">\(\{\boldsymbol{v}_{1}, ..., \boldsymbol{v}_{i-1}\}\)</span>, and take the term in the orthogonal compliment to be <span class="math inline">\(\boldsymbol{v}_{i}\)</span>.</p>

<div class="theorem">
<p><span id="thm:gram-schmidt" class="theorem"><strong>Theorem 3.4  (the Gram-Schmidt process)  </strong></span>Given a basis <span class="math inline">\(\{\boldsymbol{x}_1, ..., \boldsymbol{x}_p\}\)</span> for a nonzero subspace <span class="math inline">\(W\)</span> of <span class="math inline">\(\mathbb{R}^n\)</span>, define</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{v}_1 &amp;= \boldsymbol{x}_1 \\
\boldsymbol{v}_2 &amp;= \boldsymbol{x}_2 - \frac{\boldsymbol{x}_2 \cdot \boldsymbol{v}_1}{\boldsymbol{v}_1 \cdot \boldsymbol{v}_1}\boldsymbol{v}_1 \\
\boldsymbol{v}_3 &amp;= \boldsymbol{x}_3 
- \frac{\boldsymbol{x}_3 \cdot \boldsymbol{v}_1}{\boldsymbol{v}_1 \cdot \boldsymbol{v}_1}\boldsymbol{v}_1 
- \frac{\boldsymbol{x}_3 \cdot \boldsymbol{v}_2}{\boldsymbol{v}_2 \cdot \boldsymbol{v}_2}\boldsymbol{v}_2
\\
&amp; \vdots \\
\boldsymbol{v}_p &amp;= \boldsymbol{x}_p 
- \frac{\boldsymbol{x}_p \cdot \boldsymbol{v}_1}{\boldsymbol{v}_1 \cdot \boldsymbol{v}_1}\boldsymbol{v}_1 
- \frac{\boldsymbol{x}_p \cdot \boldsymbol{v}_2}{\boldsymbol{v}_2 \cdot \boldsymbol{v}_2}\boldsymbol{v}_2
- \cdots
- \frac{\boldsymbol{x}_p \cdot \boldsymbol{v}_{p-1}}{\boldsymbol{v}_{p-1} \cdot \boldsymbol{v}_{p-1}}\boldsymbol{v}_{p-1}
\end{aligned}
\]</span></p>
<p>Then <span class="math inline">\(\{\boldsymbol{v}_1, ..., \boldsymbol{v}_p\}\)</span> is an orthogonal basis for <span class="math inline">\(W\)</span>. In addition</p>
<span class="math display">\[
\text{Span}\{\boldsymbol{v}_1, ..., \boldsymbol{v}_p\} = \text{Span}\{\boldsymbol{x}_1, ..., \boldsymbol{x}_p\} 
\]</span>
</div>

<p>To make <span class="math inline">\(\{\boldsymbol{v}_1, ..., \boldsymbol{v}_p\}\)</span> an <em>orthonormal</em> basis, there is simply one more step of normalization</p>
<p><span class="math display">\[
\{\boldsymbol{q}_i = \frac{\boldsymbol{v}_i}{\|\boldsymbol{v}_i\|}, \;i = 1, ... p\} 
\]</span></p>
<div id="qr-factorizaiton" class="section level3">
<h3><span class="header-section-number">3.3.1</span> QR factorizaiton</h3>
<p>For <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> with linearly independent columns <span class="math inline">\(\boldsymbol{x}_1, ..., \boldsymbol{x}_n\)</span>, apply the Gram-Schmidt process to <span class="math inline">\(\boldsymbol{x}_1, ..., \boldsymbol{x}_n\)</span> amounts to <em>factorizing</em> <span class="math inline">\(A\)</span>.</p>

<div class="theorem">
<span id="thm:qr" class="theorem"><strong>Theorem 3.5  (QR factorization)  </strong></span>if <span class="math inline">\(A\)</span> is an <span class="math inline">\(m \times n\)</span> matrix with full column rank, then <span class="math inline">\(A\)</span> can be factored as <span class="math inline">\(A = QR\)</span>, where <span class="math inline">\(Q\)</span> is an <span class="math inline">\(m \times n\)</span> matrix whose columns form an orthonormal basis of <span class="math inline">\(\text{Col}\;A\)</span> and <span class="math inline">\(R\)</span> is an <span class="math inline">\(n \times n\)</span> upper triangular invertible matrix with positive entries on its diagonal.
</div>

<p><strong>PROOF</strong></p>
<p>Because <span class="math inline">\(A_{m \times n}\)</span> is full column rank, we can transform its column vector <span class="math inline">\(\{\boldsymbol{x}_{1}, ..., \boldsymbol{x}_{n}\}\)</span> into a new set of orthonormal basis <span class="math inline">\(\{\boldsymbol{q}_{1}, ..., \boldsymbol{q}_{n}\}\)</span> with Gram-Schmidt process. Let</p>
<p><span class="math display">\[
Q = [\boldsymbol{q}_{1} \;\; \cdots \;\; \boldsymbol{q}_{n}]
\]</span>
For <span class="math inline">\(\boldsymbol{x}_i, \; i = {1, ..., n}\)</span> in Span<span class="math inline">\(\{\boldsymbol{x}_1, ... \boldsymbol{x}_k\}\)</span>, there exists a set of constant <span class="math inline">\(r_{1k}, ..., r_{kk}\)</span> such that<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p><span class="math display">\[
\boldsymbol{x}_k = r_{1k}\boldsymbol{q}_{1} + \cdots + r_{kk}\boldsymbol{q}_{k} + 0 \cdot\boldsymbol{q}_{k+1} + \cdots + 0 \cdot \boldsymbol{q}_{n}
\]</span></p>
<p>So</p>
<p><span class="math display">\[
A = [\boldsymbol{x}_{1} \;\; \boldsymbol{x}_{2} \;\; \cdots \;\; \boldsymbol{x}_{n}] = [\boldsymbol{q}_{1} \;\; \boldsymbol{q}_{2} \;\; \cdots \;\; \boldsymbol{q}_{n}] 
\begin{bmatrix}
r_{11} &amp; r_{12} &amp; \cdots &amp; r_{1n} \\
0 &amp; r_{22} &amp; \cdots &amp; r_{2n} \\
\vdots &amp; \vdots &amp; &amp; \vdots\\
0 &amp; 0 &amp; \cdots &amp; r_{nn}
\end{bmatrix} 
= QR
\]</span>
We could assume that <span class="math inline">\(r_{kk} \ge 0\)</span>. (if <span class="math inline">\(r_{kk} &lt; 0\)</span>, multiply both <span class="math inline">\(r_{kk}\)</span> and <span class="math inline">\(\boldsymbol{u}_k\)</span> by <span class="math inline">\(-1\)</span>)</p>
</div>
</div>
<div id="orthonormal-sets-and-orthogonal-matrices" class="section level2">
<h2><span class="header-section-number">3.4</span> Orthonormal sets and orthogonal matrices</h2>
<p>An orthogonal set whose components are all unit vectors is said to be <strong>orthonormal</strong> sets.</p>
<div id="orthogonal-matrices" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Orthogonal matrices</h3>
<p>An <em>orthogonal matrix</em> is a square matrix <span class="math inline">\(Q\)</span> whose inverse is its transpose:</p>
<p><span class="math display" id="eq:orthogonal-matrix">\[
\tag{3.1}
QQ^T = Q^TQ = I
\]</span></p>
<p>Another way of defining it is that an orthogonal matrix has both <strong>orthonormal columns</strong> and <strong>orthonormal rows</strong>.</p>
<p>Orthogonal matrices have a nice property that they preserve inner products:</p>
<p><span class="math display">\[
(Q\boldsymbol{x})^T(Q\boldsymbol{y}) = \boldsymbol{x}^TQ^TQ\boldsymbol{y} = \boldsymbol{x}^TI\boldsymbol{y} = \boldsymbol{x}^T\boldsymbol{y}
\]</span></p>
<p>A direct result is that <span class="math inline">\(Q\)</span> preserves L2 norms</p>
<p><span class="math display">\[
\|Q\boldsymbol{x}\|_2 = \sqrt{(Q\boldsymbol{x})^T(Q\boldsymbol{x})} = \sqrt{\boldsymbol{x}^T\boldsymbol{x}} = \|\boldsymbol{x}\|_2
\]</span></p>
<p>Therefore multiplication by an orthogonal matrix can be considered as a transformation that preserves length, but may rotate or reflect the vector about the origin.</p>
<p>Note that <span class="math inline">\(Q\)</span> may not necessarily be a square matrix to satisfy <span class="math inline">\(Q^TQ = I\)</span>. For exmaple <span class="math inline">\(Q \in \mathbb{R}^{m \times n}, n &lt; m\)</span>, but its columns and rows can still be orthonormal, then <span class="math inline">\(QQ^T = I\)</span>. But in most cases the term orthogonal implies a square matrix <span class="math inline">\(Q\)</span>.</p>
</div>
</div>
<div id="lesat-squares-problems" class="section level2">
<h2><span class="header-section-number">3.5</span> Lesat squares problems</h2>

<div class="definition">
<span id="def:unnamed-chunk-5" class="definition"><strong>Definition 3.1  </strong></span>the <strong>normal equation</strong>
<span class="math display">\[
A^TA\boldsymbol{x} = A^T\boldsymbol{b}
\]</span>
</div>

<p><span class="math display">\[
\begin{aligned}
A^T(\boldsymbol{b} - A\hat{\boldsymbol{x}}) &amp;= \boldsymbol{0}
\end{aligned} \\
A^T\boldsymbol{b} - A^TA\hat{\boldsymbol{x}} = 0 \\
\hat{\boldsymbol{x}} = (A^TA)^{-1}A^T\boldsymbol{b}
\]</span></p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>because Span<span class="math inline">\(\{\boldsymbol{x}_1, ... \boldsymbol{x}_k\}\)</span> is the same as Span<span class="math inline">\(\{\boldsymbol{u}_1, ... \boldsymbol{u}_k\}\)</span><a href="orthogonality.html#fnref1" class="footnote-back">â†©ï¸Ž</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="vector-spaces.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="eigenthings-and-quadratic-forms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": "https://github.com/enixam/math-foundations/edit/master/orthogonality.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
