<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Matrix basics | Mathematical Notes for Machine Learning</title>
  <meta name="description" content="Chapter 1 Matrix basics | Mathematical Notes for Machine Learning" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Matrix basics | Mathematical Notes for Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="enixam/math-foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Matrix basics | Mathematical Notes for Machine Learning" />
  
  
  

<meta name="author" content="Qiushi Yan" />


<meta name="date" content="2020-07-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="vector-spaces.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Math foundations in Machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Linear Algebra</b></span></li>
<li class="chapter" data-level="1" data-path="matrix-basics.html"><a href="matrix-basics.html"><i class="fa fa-check"></i><b>1</b> Matrix basics</a><ul>
<li class="chapter" data-level="1.1" data-path="matrix-basics.html"><a href="matrix-basics.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.1</b> Matrix multiplication</a></li>
<li class="chapter" data-level="1.2" data-path="matrix-basics.html"><a href="matrix-basics.html#elemetary-matrix-and-row-operations"><i class="fa fa-check"></i><b>1.2</b> Elemetary matrix and row operations</a></li>
<li class="chapter" data-level="1.3" data-path="matrix-basics.html"><a href="matrix-basics.html#lu-factorization"><i class="fa fa-check"></i><b>1.3</b> LU factorization</a></li>
<li class="chapter" data-level="1.4" data-path="matrix-basics.html"><a href="matrix-basics.html#determinants"><i class="fa fa-check"></i><b>1.4</b> Determinants</a><ul>
<li class="chapter" data-level="1.4.1" data-path="matrix-basics.html"><a href="matrix-basics.html#cofactor-expansion"><i class="fa fa-check"></i><b>1.4.1</b> Cofactor expansion</a></li>
<li class="chapter" data-level="1.4.2" data-path="matrix-basics.html"><a href="matrix-basics.html#geometric-interpretation-of-determinant"><i class="fa fa-check"></i><b>1.4.2</b> Geometric interpretation of determinant</a></li>
<li class="chapter" data-level="1.4.3" data-path="matrix-basics.html"><a href="matrix-basics.html#properties-of-determinant"><i class="fa fa-check"></i><b>1.4.3</b> Properties of determinant</a></li>
<li class="chapter" data-level="1.4.4" data-path="matrix-basics.html"><a href="matrix-basics.html#cramers-rule"><i class="fa fa-check"></i><b>1.4.4</b> Cramer’s rule</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="matrix-basics.html"><a href="matrix-basics.html#trace"><i class="fa fa-check"></i><b>1.5</b> Trace</a></li>
<li class="chapter" data-level="1.6" data-path="matrix-basics.html"><a href="matrix-basics.html#inverse-of-a-matrix"><i class="fa fa-check"></i><b>1.6</b> Inverse of a matrix</a></li>
<li class="chapter" data-level="1.7" data-path="matrix-basics.html"><a href="matrix-basics.html#matrix-multiplication-as-linear-transformation"><i class="fa fa-check"></i><b>1.7</b> Matrix multiplication as linear transformation</a></li>
<li class="chapter" data-level="1.8" data-path="matrix-basics.html"><a href="matrix-basics.html#statistics-and-proabability"><i class="fa fa-check"></i><b>1.8</b> Statistics and proabability</a><ul>
<li class="chapter" data-level="1.8.1" data-path="matrix-basics.html"><a href="matrix-basics.html#sample-statistics"><i class="fa fa-check"></i><b>1.8.1</b> Sample statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="vector-spaces.html"><a href="vector-spaces.html"><i class="fa fa-check"></i><b>2</b> Vector spaces</a><ul>
<li class="chapter" data-level="2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#four-subspaces"><i class="fa fa-check"></i><b>2.1</b> Four subspaces</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="orthogonality.html"><a href="orthogonality.html"><i class="fa fa-check"></i><b>3</b> Orthogonality</a><ul>
<li class="chapter" data-level="3.1" data-path="orthogonality.html"><a href="orthogonality.html#metric-spaces-normed-spaces-inner-product-spaces"><i class="fa fa-check"></i><b>3.1</b> Metric spaces, normed spaces, inner product spaces</a></li>
<li class="chapter" data-level="3.2" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition"><i class="fa fa-check"></i><b>3.2</b> Orthogonal decomposition</a><ul>
<li class="chapter" data-level="3.2.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-complements"><i class="fa fa-check"></i><b>3.2.1</b> Orthogonal complements</a></li>
<li class="chapter" data-level="3.2.2" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-sets-and-orthogonal-basis"><i class="fa fa-check"></i><b>3.2.2</b> Orthogonal sets and orthogonal basis</a></li>
<li class="chapter" data-level="3.2.3" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition-1"><i class="fa fa-check"></i><b>3.2.3</b> Orthogonal decomposition</a></li>
<li class="chapter" data-level="3.2.4" data-path="orthogonality.html"><a href="orthogonality.html#best-approximation"><i class="fa fa-check"></i><b>3.2.4</b> Best approximation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="orthogonality.html"><a href="orthogonality.html#gram-schmidt-process"><i class="fa fa-check"></i><b>3.3</b> Gram-Schmidt process</a><ul>
<li class="chapter" data-level="3.3.1" data-path="orthogonality.html"><a href="orthogonality.html#qr-factorizaiton"><i class="fa fa-check"></i><b>3.3.1</b> QR factorizaiton</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="orthogonality.html"><a href="orthogonality.html#orthonormal-sets-and-orthogonal-matrices"><i class="fa fa-check"></i><b>3.4</b> Orthonormal sets and orthogonal matrices</a><ul>
<li class="chapter" data-level="3.4.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-matrices"><i class="fa fa-check"></i><b>3.4.1</b> Orthogonal matrices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html"><i class="fa fa-check"></i><b>4</b> Eigenthings and quadratic forms</a><ul>
<li class="chapter" data-level="4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>4.1</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#diagnolization-and-similar-matrices"><i class="fa fa-check"></i><b>4.2</b> Diagnolization and similar matrices</a><ul>
<li class="chapter" data-level="4.2.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#similarity"><i class="fa fa-check"></i><b>4.2.1</b> Similarity</a></li>
<li class="chapter" data-level="4.2.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#jordan-matrix"><i class="fa fa-check"></i><b>4.2.2</b> Jordan matrix</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#symmetric-matrices"><i class="fa fa-check"></i><b>4.3</b> Symmetric matrices</a><ul>
<li class="chapter" data-level="4.3.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#spectral-decomposition"><i class="fa fa-check"></i><b>4.3.1</b> Spectral decomposition</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#quadratic-forms"><i class="fa fa-check"></i><b>4.4</b> Quadratic forms</a><ul>
<li class="chapter" data-level="4.4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#change-of-variabele"><i class="fa fa-check"></i><b>4.4.1</b> Change of variabele</a></li>
<li class="chapter" data-level="4.4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#classification-of-quadratic-forms"><i class="fa fa-check"></i><b>4.4.2</b> Classification of quadratic forms</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#rayleigh-quotients"><i class="fa fa-check"></i><b>4.5</b> Rayleigh quotients</a></li>
<li class="chapter" data-level="4.6" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#svd"><i class="fa fa-check"></i><b>4.6</b> SVD</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="matrix-calculus.html"><a href="matrix-calculus.html"><i class="fa fa-check"></i><b>5</b> Matrix calculus</a></li>
<li class="part"><span><b>II Calculus</b></span></li>
<li class="chapter" data-level="6" data-path="taylor-series-and-expansion.html"><a href="taylor-series-and-expansion.html"><i class="fa fa-check"></i><b>6</b> Taylor series and expansion</a></li>
<li class="part"><span><b>III Probability Theory</b></span></li>
<li class="chapter" data-level="7" data-path="probability-basics.html"><a href="probability-basics.html"><i class="fa fa-check"></i><b>7</b> Probability basics</a></li>
<li class="part"><span><b>IV Applications</b></span></li>
<li class="chapter" data-level="8" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>8</b> Linear models</a><ul>
<li class="chapter" data-level="8.1" data-path="linear-models.html"><a href="linear-models.html#least-square-estimation"><i class="fa fa-check"></i><b>8.1</b> Least square estimation</a></li>
<li class="chapter" data-level="8.2" data-path="linear-models.html"><a href="linear-models.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>8.2</b> Maximum likelihood estimation</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html"><i class="fa fa-check"></i><b>9</b> Principle component analysis</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li>
  <a href="https://github.com/rstudio/bookdown" target="blank">Written with bookdown</a>
</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Notes for Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="matrix-basics" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Matrix basics</h1>
<div id="matrix-multiplication" class="section level2">
<h2><span class="header-section-number">1.1</span> Matrix multiplication</h2>
<p>A common way of looking at matrix-vector multiplication <span class="math inline">\(A\boldsymbol{x}\)</span> is to think of as a linear combination of column vectors in <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
A\boldsymbol{x} &amp;= [\boldsymbol{a}_1 \;\; \cdots \;\; \boldsymbol{a}_n] 
\begin{bmatrix}
x_1 \\
\vdots \\
x_n
\end{bmatrix} \\
&amp;= x_1\boldsymbol{a}_1 + \cdots + x_n\boldsymbol{a}_n
\end{aligned}
\]</span></p>
<p>Likewise, for any <span class="math inline">\(\boldsymbol{x}^{T} = [x_1, \cdots, x_n]^T\)</span> and matrix <span class="math inline">\(A_{m \times n}\)</span>, <span class="math inline">\(x^{T}A\)</span> can be thought of as a linear combination of rows in <span class="math inline">\(A\)</span> to produce a new row vector:</p>
<p><span class="math display">\[
[x_1 \;\; \cdots \;\; x_n] 
\begin{bmatrix}
\boldsymbol{a}_1^T \\
\vdots  \\
\boldsymbol{a}_n^T
\end{bmatrix} 
= x_1\boldsymbol{a}_1^T + \dots + x_n\boldsymbol{a}_n^T
\]</span>
For matrix-matrix multiplication <span class="math inline">\(AB\)</span>, besides the dot product definition we can see it as <strong>column row expansion</strong>.</p>
<p><br></p>


<div class="theorem">
<p><span id="thm:unnamed-chunk-1" class="theorem"><strong>Theorem 1.1  (Column-row expansion of <span class="math inline">\(AB\)</span>)  </strong></span>
if <span class="math inline">\(A\)</span> is <span class="math inline">\(m \times n\)</span> and <span class="math inline">\(B\)</span> is <span class="math inline">\(n \times p\)</span>, then</p>
<span class="math display">\[
\begin{aligned}
AB &amp;= [\text{col}_1(A) \cdots \text{col}_n(A)] 
\begin{bmatrix}
\text{row}_1(B) \\
\vdots \\ 
\text{row}_n(B)
\end{bmatrix} \\ 
&amp;= \text{col}_1(A)\text{row}_1(B) + \cdots +  \text{col}_n(A)\text{row}_n(B)
\end{aligned}
\]</span>
</div>

<p>Note that each <span class="math inline">\(\text{col}_1(A)\text{row}_1(B)\)</span> is a rank 1 <span class="math inline">\(m \times p\)</span> matrix.</p>
</div>
<div id="elemetary-matrix-and-row-operations" class="section level2">
<h2><span class="header-section-number">1.2</span> Elemetary matrix and row operations</h2>
<p>An <em>elementary matrix</em> is one that is obtained by performing a single elementary row
operation on an identity matrix <span class="math inline">\(I\)</span>. Each elementary matrix <span class="math inline">\(E\)</span> is invertible. The inverse of <span class="math inline">\(E\)</span> is the elementary matrix of the same type that transforms <span class="math inline">\(E\)</span> back into <span class="math inline">\(I\)</span>.</p>
<p>Left multiplication by a elementary matrix has a nice illustration. There are 3 primary types of elementary matrices (example for <span class="math inline">\(3 \times 3\)</span>):</p>
<p><span class="math display">\[
E_1 = 
\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
-4 &amp; 0 &amp; 1
\end{bmatrix} 
, 
E_2 = 
\begin{bmatrix}
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} 
,
E_3 = 
\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 5
\end{bmatrix} 
\]</span>
<span class="math inline">\(E_1, E_2, E_3\)</span> represents 3 types of elementary row operations applicable to a <span class="math inline">\(3 \times 3\)</span> matrix, (1) <em>replacement</em>; (2) <em>interchange</em> and (3) <em>scaling</em>. We can verify this by right multiply them with an arbitrary matrix <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
\begin{aligned}
E_1A &amp;= 
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
- 4a_{11} + a_{31} &amp; -4a_{12} + a_{32} &amp; -4a_{13} + a_{33}
\end{bmatrix}  \\ \\  
E_2A &amp;= 
\begin{bmatrix}
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{31} &amp; a_{32} &amp; a_{33}
\end{bmatrix} \\ \\
E_3A &amp;= 
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
5a_{31} &amp; 5a_{32} &amp; 5a_{33} \\
\end{bmatrix}
\end{aligned}
\]</span>
Thus, any row operation on <span class="math inline">\(A\)</span> is equivalent to left multiply a corresponding elementary matrix <span class="math inline">\(E\)</span>.</p>
<p>Since row operation are invertible, elementary matrices are invertible. This gives a general way of finding the inverse matrix of <span class="math inline">\(A\)</span>.</p>

<div class="theorem">
<span id="thm:find-inverse" class="theorem"><strong>Theorem 1.2  (an algorithm for finding inverse matrices)  </strong></span>Row reduce the augmented matrix <span class="math inline">\([A \;\; I]\)</span>, when <span class="math inline">\(A\)</span> becomes <span class="math inline">\(I\)</span>, <span class="math inline">\(I\)</span> becomes <span class="math inline">\(A^{-1}\)</span>. Otherwise <span class="math inline">\(A^{-1}\)</span> is not invertible.
</div>

</div>
<div id="lu-factorization" class="section level2">
<h2><span class="header-section-number">1.3</span> LU factorization</h2>
<p><a href="https://fml-fam.github.io/blog/2020/07/03/matrix-factorizations-for-data-analysis/" class="uri">https://fml-fam.github.io/blog/2020/07/03/matrix-factorizations-for-data-analysis/</a></p>
<p>A <em>factorization</em> a matrix <span class="math inline">\(A\)</span> is an equation that expresses <span class="math inline">\(A\)</span> as a product of two or more matrices.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-2" class="definition"><strong>Definition 1.1  (LU factorization)  </strong></span><br />
Suppose <span class="math inline">\(A\)</span> can be reduced to an echelon form <span class="math inline">\(U\)</span> using row operations that add a multiple ofo oone row to another row <em>below</em> it, there exist a set of unit lower trangular matrices <span class="math inline">\(E_1, \dots, E_p\)</span> such that</p>
<p><span class="math display">\[
E_p \cdots E_1A = U  
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
A = (E_p \cdots E_1)^{-1}U = LU
\]</span></p>
where
<span class="math display">\[
L = (E_p \cdots E_1)^{-1}  
\]</span>
</div>

</div>
<div id="determinants" class="section level2">
<h2><span class="header-section-number">1.4</span> Determinants</h2>
<div id="cofactor-expansion" class="section level3">
<h3><span class="header-section-number">1.4.1</span> Cofactor expansion</h3>
<p>The <span class="math inline">\((i, j)\text{-cofactor}\)</span> of <span class="math inline">\(A\)</span> is a number <span class="math inline">\(C_{ij}\)</span> in <span class="math inline">\(\mathbb{R}\)</span> given by</p>
<p><span class="math display">\[
C_{ij} = (-1)^{i + j} \det A_{ij}
\]</span>
where <span class="math inline">\(A_{ij}\)</span> denotes the submatrix formed by deleting the <span class="math inline">\(i\)</span>h row and <span class="math inline">\(j\)</span>th column of <span class="math inline">\(A\)</span>.</p>

<div class="theorem">
<p><span id="thm:cofactor-expansion" class="theorem"><strong>Theorem 1.3  (cofactor expansion)  </strong></span>The determinant of an <span class="math inline">\(n \times n\)</span> matrix is given by a <strong>cofactor expasion</strong> across any row or column. For example, the expansion across the <span class="math inline">\(i\)</span>th row is:</p>
<p><span class="math display">\[
\det A = a_{i1}C_{i1} + a_{i2}C_{i2} + \cdots + a_{in}C_{in} 
\]</span></p>
<p>and cross the <span class="math inline">\(j\)</span>th column is</p>
<span class="math display">\[
\det A = a_{1j}C_{1j} + a_{2j}C_{2j} + \cdots + a_{nj}C_{nj} 
\]</span>
</div>

</div>
<div id="geometric-interpretation-of-determinant" class="section level3">
<h3><span class="header-section-number">1.4.2</span> Geometric interpretation of determinant</h3>
<p>Given matrix <span class="math inline">\(A_{n \times n}\)</span></p>
<p><span class="math display">\[
\begin{bmatrix}
a_1^{T} \\
a_2^{T} \\
\vdots \\
a_n^{T}
\end{bmatrix}
\]</span>
where <span class="math inline">\(a_1, ..., a_n\)</span> are row vectors of A. Then <span class="math inline">\(|\det A|\)</span> is the volume of parallelotope constrained by <span class="math inline">\(a_1, ..., a_n\)</span>. When <span class="math inline">\(A\)</span> is <span class="math inline">\(2\times2\)</span>, <span class="math inline">\(|\det A|\)</span> is simply the area of the parallelogram defined by two side <span class="math inline">\(a_1, a_2\)</span></p>
</div>
<div id="properties-of-determinant" class="section level3">
<h3><span class="header-section-number">1.4.3</span> Properties of determinant</h3>
<p>A list of arithmetic properties of determinants, A is an <span class="math inline">\(n\times n\)</span> matrix:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\det(A^T) = \det(A)\)</span></li>
<li><span class="math inline">\(\det(kA) = k^n \det(A)\)</span></li>
<li><span class="math inline">\(\det(AB) = \det(A)\det(B)\)</span> (although <span class="math inline">\(AB \not = BA\)</span> in general), it follows that <span class="math inline">\(\det(A^n) = \det(A)^n\)</span><br />
</li>
<li><span class="math inline">\(\det(A^{-1}) = 1 / \det(A)\)</span>, if <span class="math inline">\(A\)</span> is invertible</li>
<li>determinant is equal to the product of eigenvalues(counting multiplicity) <span class="math inline">\(\det(A) = \prod_{i=1}^n{\lambda_i}\)</span><br />
</li>
<li>If the <span class="math inline">\(i\)</span>-th row (column) in A is a sum of the <span class="math inline">\(i\)</span>-th row (column) of a matrix <span class="math inline">\(B\)</span> and the <span class="math inline">\(i\)</span>-th row (column) of a matrix <span class="math inline">\(C\)</span> and all other rows in <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> are equal to the corresponding rows in <span class="math inline">\(A\)</span> (that is <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> differ from <span class="math inline">\(A\)</span> by one row only), then <span class="math inline">\(\det(A)=\det(B)+\det(C)\)</span>. This can be proven by cofactor expansion across the <span class="math inline">\(i\)</span>th row. The same applies to columns.</li>
</ol>
<p>Row operations on <span class="math inline">\(A\)</span> has the following effect on <span class="math inline">\(\det A\)</span></p>
<ul>
<li><p>if we multiply a single row in <span class="math inline">\(A\)</span> by a scalar <span class="math inline">\(k \in \mathbb{R}\)</span>, then the determinant of the new matrix is <span class="math inline">\(k\det A\)</span></p></li>
<li><p>if we exchange two rows <span class="math inline">\(a_i^T\)</span> and <span class="math inline">\(a_j^T\)</span> of <span class="math inline">\(A\)</span>, determinant becomes <span class="math inline">\(-\det A\)</span></p></li>
<li><p>Add a multiple of one row to another row has <strong>no</strong> effect on determinant</p></li>
</ul>
<p>The first two effects can be easily understood in connection with geometric meaning of determinant. As for the third one, let us represent A with row vectors</p>
<p><span class="math display">\[
A = 
\begin{vmatrix}
a_1^T \\ 
\vdots \\
a_i^T \\
\vdots \\
a_j^T \\ 
\vdots \\
a_n^T
\end{vmatrix}
\]</span>
Then <span class="math inline">\(B\)</span>, after performing replacing (add a multiple of the <span class="math inline">\(j\)</span>th row to the <span class="math inline">\(i\)</span>th row) on <span class="math inline">\(A\)</span>, becomes</p>
<p><span class="math display">\[
B = 
\begin{vmatrix}
a_1^T \\ 
\vdots \\
a_i^T + ka_j^T \\
\vdots \\
a_j^T \\ 
\vdots \\
a_n^T
\end{vmatrix}
\]</span>
By property 6 <span class="math inline">\(\det(A) = \det(B) + \det(C)\)</span> when <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> only differs from <span class="math inline">\(A\)</span> by the same row. So <span class="math inline">\(\det A\)</span> can be broke down into two parts</p>
<p><span class="math display">\[
|A| = 
\begin{vmatrix}
a_1^T \\ 
\vdots \\
a_i^T \\
\vdots \\
a_j^T \\ 
\vdots \\
a_n^T
\end{vmatrix} 
+ 
\begin{vmatrix}
a_1^T \\ 
\vdots \\
ka_j^T \\
\vdots \\
a_j^T \\ 
\vdots \\
a_n^T
\end{vmatrix} 
\]</span>
The second matrix on the right side has determinant <span class="math inline">\(0\)</span>, and <span class="math inline">\(\det A\)</span> stays the same after replacing.</p>
<p>Note that all row operations don’t change whether or not a determinant is 0, only change it by a non-zero factor or change its sign.</p>
</div>
<div id="cramers-rule" class="section level3">
<h3><span class="header-section-number">1.4.4</span> Cramer’s rule</h3>
<p>Given an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> and <span class="math inline">\(\boldsymbol{b}\)</span> in <span class="math inline">\(\mathbb{\mathbb{R^n}}\)</span>, denote <span class="math inline">\(A_i(\boldsymbol{b})\)</span> as the matrix derived by <span class="math inline">\(A\)</span> by <strong>replacing</strong> column <span class="math inline">\(i\)</span> by vector <span class="math inline">\(\boldsymbol{b}\)</span>:</p>
<p><span class="math display">\[
A_i(\boldsymbol{b}) = [\boldsymbol{a}_1 \cdots \underbrace{\boldsymbol{b}}_{\text{column} \,i} \cdots \boldsymbol{a}_n]
\]</span></p>

<div class="theorem">
<p><span id="thm:cramer" class="theorem"><strong>Theorem 1.4  (Cramer’s rule)  </strong></span>Let <span class="math inline">\(A\)</span> be an invertible <span class="math inline">\(n \times n\)</span> matrix. For any <span class="math inline">\(\boldsymbol{b}\)</span> in <span class="math inline">\(\mathbb{R^n}\)</span>, the unique solution <span class="math inline">\(\boldsymbol{x}\)</span> of <span class="math inline">\(A\boldsymbol{x} = \boldsymbol{b}\)</span> has entries given by:</p>
<span class="math display">\[
x_i = \frac{\det A_i(\boldsymbol{b})}{\det A}
\]</span>
</div>

</div>
</div>
<div id="trace" class="section level2">
<h2><span class="header-section-number">1.5</span> Trace</h2>
<p>The <em>trace</em> of square matrix <span class="math inline">\(A\)</span> is the sum of its diagonal entries <span class="math inline">\(\sum_{i = 1}^{n}A_{ii}\)</span>.</p>
<p>The trace has the following properties:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\text{tr}(A + B) = \text{tr}A + \text{tr}B\)</span><br />
</li>
<li><span class="math inline">\(\text{tr}(kA) = k\text{tr}A\)</span>, <span class="math inline">\(k\)</span> is a scalar<br />
</li>
<li><span class="math inline">\(\text{tr}(A^T) = \text{tr}(A)\)</span><br />
</li>
<li>For <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> such that <span class="math inline">\(AB\)</span> is square, <span class="math inline">\(\text{tr}(AB) = \text{tr}(BA)\)</span><br />
</li>
<li>Trace of product of multiple matrices is invariant to <em>cyclic permutations</em>, <span class="math inline">\(\text{tr}(ABC) = \text{tr}(BCA) = \text{tr}(CAB)\)</span>. Note that the reordering cannot be done arbitrarily, fro example <span class="math inline">\(\text{tr}(ABC) \not= \text{tr}(ACG)\)</span> in general.</li>
<li>Trace is equal to the sum of its eigenvalues (repeated accordin gto multiplicity) <span class="math inline">\(\text{tr}(A) = \sum_{i = 1}^n{\lambda_i}\)</span><br />
</li>
<li><span class="math inline">\(\text{tr}(\boldsymbol{a}\boldsymbol{a}^T) = \boldsymbol{a}^T\boldsymbol{a}\)</span>, a is a column vector</li>
</ol>
</div>
<div id="inverse-of-a-matrix" class="section level2">
<h2><span class="header-section-number">1.6</span> Inverse of a matrix</h2>

<div class="rmdnote">
<p>Note that the inverse of a matrix is only defined for square matrces, so is determinants in Section <a href="matrix-basics.html#determinants">1.4</a>.</p>
In practice <span class="math inline">\(A^{-1}\)</span> is seldom computed, because computing both <span class="math inline">\(A^{-1}\)</span> and <span class="math inline">\(A^{-1}\boldsymbol{b}\)</span> to solve linear equations takes about 3 times as many arithmetic operations as solving <span class="math inline">\(A\boldsymbol{x} = \boldsymbol{b}\)</span> by row reduction.
</div>

Assume that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are both non-singular

<div class="theorem">
<p><span id="thm:unnamed-chunk-4" class="theorem"><strong>Theorem 1.5  </strong></span>If A and B are both invertible matrces, we have</p>
<p><span class="math display">\[
(A^{-1})^{-1} = A  
\]</span></p>
<p><span class="math display">\[
(AB)^{-1} = B^{-1}A^{-1}
\]</span></p>
<span class="math display">\[
(A^T)^{-1} = (A^{-1})^T
\]</span>
</div>

<p>In <a href="matrix-basics.html#thm:find-inverse">1.2</a>, we know an algorithm of finding inverse matrices by row reducions on the augmented matrix <span class="math inline">\([A \;\; I]\)</span>. However, Cramer’s rule <a href="matrix-basics.html#thm:cramer">1.4</a> leads to a general formula of calculating <span class="math inline">\(A^{-1}\)</span>, if it exists.</p>
<p>The <span class="math inline">\(j\)</span>th column of <span class="math inline">\(A^{-1}\)</span> is a vector <span class="math inline">\(\boldsymbol{x}\)</span> that satisfies:</p>
<p><span class="math display">\[
A\boldsymbol{x} = \boldsymbol{e}_j
\]</span>
By Cramer’s rule
<span class="math display">\[
\{(i,j) \text{ entry of } A^{-1}\} = x_i = \frac{\det A_i{(\boldsymbol{e}_j)}}{\det A}
\]</span></p>
<p>A cofactor expansion <a href="matrix-basics.html#thm:cofactor-expansion">1.3</a> down column <span class="math inline">\(i\)</span> of <span class="math inline">\(A_i{(\boldsymbol{e}_j)}\)</span> shows that:</p>
<p><span class="math display">\[
\det A_i{(\boldsymbol{e}_j)} = (-1)^{i + j}\det A_{ji} = C_{ji}
\]</span>
where <span class="math inline">\(C_{ji}\)</span> is a cofactor of <span class="math inline">\(A\)</span>. Note that the (<span class="math inline">\(i\)</span>, <span class="math inline">\(j\)</span>)th entry of <span class="math inline">\(A^{-1}\)</span> is the cofactor <span class="math inline">\(C_{ji}\)</span> divided by <span class="math inline">\(\det A\)</span> (the subscript is reversed). Thus</p>
<p><span class="math display" id="eq:inverse-adjugate">\[\begin{equation}
\tag{1.1}
A^{-1} = \frac{1}{\det A} 
\begin{bmatrix}
C_{11} &amp; C_{21} &amp; \cdots &amp; C_{n1} \\ 
C_{12} &amp; C_{22} &amp; \cdots &amp; C_{n2} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
C_{1n} &amp; C_{2n} &amp; \cdots &amp; C_{nn} \\
\end{bmatrix}
\end{equation}\]</span></p>
<p>The right side of Eq <a href="matrix-basics.html#eq:inverse-adjugate">(1.1)</a> is called the <em>adjugate</em> of <span class="math inline">\(A\)</span>, often denoted by <span class="math inline">\(\text{adj}\, A\)</span>.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-5" class="theorem"><strong>Theorem 1.6  (An Inverse Formula)  </strong></span>Let <span class="math inline">\(A\)</span> be an invertible <span class="math inline">\(n \times n\)</span> matrix. Then</p>
<span class="math display">\[
A^{-1} = \frac{1}{\det A}\text{adj}\, A
\]</span>
</div>

</div>
<div id="matrix-multiplication-as-linear-transformation" class="section level2">
<h2><span class="header-section-number">1.7</span> Matrix multiplication as linear transformation</h2>
<p>The equation <span class="math inline">\(A_{m \times n} \, x _{ n \times 1} = b_{m \times 1}\)</span> can arise in a way that is not directly connected with linear combination of column vectors. That is, we think of the matrix <span class="math inline">\(A\)</span> as an force that “acts” on a vector <span class="math inline">\(x\)</span> in <span class="math inline">\(\mathbb{R^n}\)</span> by multiplication to produce a new vector called <span class="math inline">\(b\)</span> in <span class="math inline">\(\mathbb{\mathbb{R^m}}\)</span>.</p>
<p>A transformation <span class="math inline">\(T\)</span> from <span class="math inline">\(\mathbb{R^n}\)</span> to <span class="math inline">\(\mathbb{R^m}\)</span> is a rule that assigns each vector  in <span class="math inline">\(\mathbb{R^n}\)</span> a vector <span class="math inline">\(T(x)\)</span> in <span class="math inline">\(\mathbb{R^m}\)</span>, which is called the <strong>image</strong> of  (under the action of <span class="math inline">\(T\)</span>).</p>
<p>It can be show that such transformations induced by multiplying a matrix is a type of <strong>linear transformation</strong>, because it satisfies all required properties to be linear:</p>
<span class="math display">\[
\begin{aligned}
\text{vector addition} \quad A(\boldsymbol{u} + \boldsymbol{v}) &amp;= A\boldsymbol{u} + A\boldsymbol{v}  \\ 
\text{scalar multiplication} \quad A(c\boldsymbol{u}) &amp;= cA\boldsymbol{u}
\end{aligned}
\]</span>
<hr>

<div class="theorem">
<p><span id="thm:unnamed-chunk-6" class="theorem"><strong>Theorem 1.7  (left multiplication as linear transformation)  </strong></span>There is a one to one relationship between a linear transformation and a matrix. Let <span class="math inline">\(T: \mathbb{R^n} \rightarrow \mathbb{R^m}\)</span> be a linear transformation. Then there exists a <strong>unique</strong> matrix <span class="math inline">\(A\)</span> such that:</p>
<p><span class="math display">\[
T(x) = Ax \quad \text{for all} \; x \; \text{in} \; \mathbb{R^n}  
\]</span></p>
In fact, <span class="math inline">\(A\)</span> is a <span class="math inline">\(m \times n\)</span> matrix whose <span class="math inline">\(j\)</span>th column is the vector <span class="math inline">\(T(\boldsymbol{e_j})\)</span>, where <span class="math inline">\(\boldsymbol{e_j}\)</span> is the <span class="math inline">\(j\)</span>th basis of <span class="math inline">\(\mathbb{R^n}\)</span>
</div>

<p><strong>Proof</strong>:</p>
<p><span class="math display">\[
\boldsymbol{x} = x_1\boldsymbol{e_1} + \dots + x_n{\boldsymbol{e_n}}
\]</span>
And because <span class="math inline">\(T(\boldsymbol{x})\)</span> is a linear transformation:</p>
<p><span class="math display">\[
\begin{split}
T(\boldsymbol{x}) &amp;= x_1T(\boldsymbol{e_1}) + \dots + x_nT(\boldsymbol{e_n}) \\
&amp;= [T(\boldsymbol{e_1}) \, \cdots \, T(\boldsymbol{e_n})]\boldsymbol{x} \\
&amp;= (A\boldsymbol{x})_{m \times 1}
\end{split}
\]</span></p>
<p>In other words, the transformation is specified once we know what all basis in <span class="math inline">\(\mathbb{R^n}\)</span> become in <span class="math inline">\(\mathbb{R^m}\)</span>.</p>
<p>The matrix <span class="math inline">\(A\)</span> is called the <strong>standard matrix for the linear transformation</strong> <span class="math inline">\(T\)</span>.</p>
<hr>


<div class="definition">
<span id="def:unnamed-chunk-7" class="definition"><strong>Definition 1.2  (A mapping is onto <span class="math inline">\(\mathbb{R^m}\)</span>)  </strong></span>A mapping <span class="math inline">\(T: \mathbb{R^n} \rightarrow \mathbb{R^m}\)</span> is said to be <strong>onto</strong>  if each <span class="math inline">\(\boldsymbol{b}\)</span> in <span class="math inline">\(\mathbb{R^m}\)</span> is the image of at least one <span class="math inline">\(\boldsymbol{x}\)</span> in 
</div>

<p>Equivalently, <span class="math inline">\(T\)</span> is onto <span class="math inline">\(\mathbb{R^m}\)</span> means that there exists at least one solution of <span class="math inline">\(T(\boldsymbol{x}) = \boldsymbol{b}\)</span></p>
<p><br></p>

<div class="definition">
<span id="def:unnamed-chunk-8" class="definition"><strong>Definition 1.3  (one-to-one mapping)  </strong></span>A mapping T: <span class="math inline">\(\mathbb{R^n} \rightarrow \mathbb{R^m}\)</span> is said to be <strong>one-to-one</strong> if each <span class="math inline">\(\boldsymbol{b}\)</span> in <span class="math inline">\(\mathbb{R^m}\)</span> is the image of <em>at most</em> one <span class="math inline">\(\boldsymbol{x}\)</span> in <span class="math inline">\(\mathbb{R^n}\)</span>
</div>

<p>Equivalently, <span class="math inline">\(T\)</span> is one-to-one if, for each <span class="math inline">\(\boldsymbol{b}\)</span> in <span class="math inline">\(\mathbb{R^m}\)</span>, the equation <span class="math inline">\(T(\boldsymbol{x}) = \boldsymbol{b}\)</span> has either a unique solution or none at all.</p>
</div>
<div id="statistics-and-proabability" class="section level2">
<h2><span class="header-section-number">1.8</span> Statistics and proabability</h2>
<div id="sample-statistics" class="section level3">
<h3><span class="header-section-number">1.8.1</span> Sample statistics</h3>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="vector-spaces.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": "https://github.com/enixam/math-foundations/edit/master/matrix-algebra.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
