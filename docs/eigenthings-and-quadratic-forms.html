<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Eigenthings and quadratic forms | Mathematical Notes for Machine Learning</title>
  <meta name="description" content="Chapter 4 Eigenthings and quadratic forms | Mathematical Notes for Machine Learning" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Eigenthings and quadratic forms | Mathematical Notes for Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="enixam/math-foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Eigenthings and quadratic forms | Mathematical Notes for Machine Learning" />
  
  
  

<meta name="author" content="Qiushi Yan" />


<meta name="date" content="2020-08-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="orthogonality.html"/>
<link rel="next" href="solutions-of-linear-system-ax-b.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Math Notes for Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Linear Algebra</b></span></li>
<li class="chapter" data-level="1" data-path="matrix-basics.html"><a href="matrix-basics.html"><i class="fa fa-check"></i><b>1</b> Matrix basics</a><ul>
<li class="chapter" data-level="1.1" data-path="matrix-basics.html"><a href="matrix-basics.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.1</b> Matrix multiplication</a></li>
<li class="chapter" data-level="1.2" data-path="matrix-basics.html"><a href="matrix-basics.html#elemetary-matrix-and-row-operations"><i class="fa fa-check"></i><b>1.2</b> Elemetary matrix and row operations</a></li>
<li class="chapter" data-level="1.3" data-path="matrix-basics.html"><a href="matrix-basics.html#lu-factorization"><i class="fa fa-check"></i><b>1.3</b> LU factorization</a></li>
<li class="chapter" data-level="1.4" data-path="matrix-basics.html"><a href="matrix-basics.html#determinants"><i class="fa fa-check"></i><b>1.4</b> Determinants</a><ul>
<li class="chapter" data-level="1.4.1" data-path="matrix-basics.html"><a href="matrix-basics.html#cofactor-expansion"><i class="fa fa-check"></i><b>1.4.1</b> Cofactor expansion</a></li>
<li class="chapter" data-level="1.4.2" data-path="matrix-basics.html"><a href="matrix-basics.html#geometric-interpretation-of-determinant"><i class="fa fa-check"></i><b>1.4.2</b> Geometric interpretation of determinant</a></li>
<li class="chapter" data-level="1.4.3" data-path="matrix-basics.html"><a href="matrix-basics.html#properties-of-determinant"><i class="fa fa-check"></i><b>1.4.3</b> Properties of determinant</a></li>
<li class="chapter" data-level="1.4.4" data-path="matrix-basics.html"><a href="matrix-basics.html#cramers-rule"><i class="fa fa-check"></i><b>1.4.4</b> Cramer’s rule</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="matrix-basics.html"><a href="matrix-basics.html#trace"><i class="fa fa-check"></i><b>1.5</b> Trace</a></li>
<li class="chapter" data-level="1.6" data-path="matrix-basics.html"><a href="matrix-basics.html#matrix-inversion"><i class="fa fa-check"></i><b>1.6</b> Matrix inversion</a><ul>
<li class="chapter" data-level="1.6.1" data-path="matrix-basics.html"><a href="matrix-basics.html#the-matrix-inversion-lemma"><i class="fa fa-check"></i><b>1.6.1</b> The matrix inversion lemma</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="matrix-basics.html"><a href="matrix-basics.html#matrix-multiplication-as-linear-transformation"><i class="fa fa-check"></i><b>1.7</b> Matrix multiplication as linear transformation</a><ul>
<li class="chapter" data-level="1.7.1" data-path="matrix-basics.html"><a href="matrix-basics.html#matrix-multiplication-as-geometric-operators"><i class="fa fa-check"></i><b>1.7.1</b> Matrix multiplication as geometric operators</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="matrix-basics.html"><a href="matrix-basics.html#statistics-and-proabability"><i class="fa fa-check"></i><b>1.8</b> Statistics and proabability</a><ul>
<li class="chapter" data-level="1.8.1" data-path="matrix-basics.html"><a href="matrix-basics.html#sample-statistics"><i class="fa fa-check"></i><b>1.8.1</b> Sample statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="vector-spaces.html"><a href="vector-spaces.html"><i class="fa fa-check"></i><b>2</b> Vector spaces</a><ul>
<li class="chapter" data-level="2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#vector-space"><i class="fa fa-check"></i><b>2.1</b> Vector space</a><ul>
<li class="chapter" data-level="2.1.1" data-path="vector-spaces.html"><a href="vector-spaces.html#euclidean-space"><i class="fa fa-check"></i><b>2.1.1</b> Euclidean space</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="vector-spaces.html"><a href="vector-spaces.html#metric-spaces-normed-spaces-inner-product-spaces"><i class="fa fa-check"></i><b>2.2</b> Metric spaces, normed spaces, inner product spaces</a><ul>
<li class="chapter" data-level="2.2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#restricted-definition-of-inner-products-in-rn"><i class="fa fa-check"></i><b>2.2.1</b> Restricted definition of inner products in <span class="math inline">\(R^n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="vector-spaces.html"><a href="vector-spaces.html#subspaces"><i class="fa fa-check"></i><b>2.3</b> Subspaces</a></li>
<li class="chapter" data-level="2.4" data-path="vector-spaces.html"><a href="vector-spaces.html#fundamental-theorem"><i class="fa fa-check"></i><b>2.4</b> Fundamental theorem of linear algebra</a></li>
<li class="chapter" data-level="2.5" data-path="vector-spaces.html"><a href="vector-spaces.html#rank"><i class="fa fa-check"></i><b>2.5</b> Rank</a><ul>
<li class="chapter" data-level="2.5.1" data-path="vector-spaces.html"><a href="vector-spaces.html#effect-of-operations-on-matrix-rank"><i class="fa fa-check"></i><b>2.5.1</b> Effect of operations on matrix rank</a></li>
<li class="chapter" data-level="2.5.2" data-path="vector-spaces.html"><a href="vector-spaces.html#gram-matrix"><i class="fa fa-check"></i><b>2.5.2</b> Gram matrix</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="orthogonality.html"><a href="orthogonality.html"><i class="fa fa-check"></i><b>3</b> Orthogonality</a><ul>
<li class="chapter" data-level="3.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition"><i class="fa fa-check"></i><b>3.1</b> Orthogonal decomposition</a><ul>
<li class="chapter" data-level="3.1.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-complements"><i class="fa fa-check"></i><b>3.1.1</b> Orthogonal complements</a></li>
<li class="chapter" data-level="3.1.2" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-sets-and-orthogonal-basis"><i class="fa fa-check"></i><b>3.1.2</b> Orthogonal sets and orthogonal basis</a></li>
<li class="chapter" data-level="3.1.3" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition-1"><i class="fa fa-check"></i><b>3.1.3</b> Orthogonal decomposition</a></li>
<li class="chapter" data-level="3.1.4" data-path="orthogonality.html"><a href="orthogonality.html#best-approximation"><i class="fa fa-check"></i><b>3.1.4</b> Best approximation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="orthogonality.html"><a href="orthogonality.html#idempotent-and-projection-matrices"><i class="fa fa-check"></i><b>3.2</b> Idempotent and Projection Matrices</a></li>
<li class="chapter" data-level="3.3" data-path="orthogonality.html"><a href="orthogonality.html#gram-schmidt-process"><i class="fa fa-check"></i><b>3.3</b> Gram-Schmidt process</a><ul>
<li class="chapter" data-level="3.3.1" data-path="orthogonality.html"><a href="orthogonality.html#qr-factorizaiton"><i class="fa fa-check"></i><b>3.3.1</b> QR factorizaiton</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="orthogonality.html"><a href="orthogonality.html#orthonormal-sets-and-orthogonal-matrices"><i class="fa fa-check"></i><b>3.4</b> Orthonormal sets and orthogonal matrices</a><ul>
<li class="chapter" data-level="3.4.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-matrices"><i class="fa fa-check"></i><b>3.4.1</b> Orthogonal matrices</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="orthogonality.html"><a href="orthogonality.html#lesat-squares-problems"><i class="fa fa-check"></i><b>3.5</b> Lesat squares problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html"><i class="fa fa-check"></i><b>4</b> Eigenthings and quadratic forms</a><ul>
<li class="chapter" data-level="4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>4.1</b> Eigenvectors and eigenvalues</a><ul>
<li class="chapter" data-level="4.1.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#additional-properties-of-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>4.1.1</b> Additional properties of eigenvalues and eigenvectors</a></li>
<li class="chapter" data-level="4.1.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#left-eigenvectors-and-right-eigenvectors"><i class="fa fa-check"></i><b>4.1.2</b> Left eigenvectors and right eigenvectors</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#diagnolization-and-similar-matrices"><i class="fa fa-check"></i><b>4.2</b> Diagnolization and similar matrices</a><ul>
<li class="chapter" data-level="4.2.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#similarity"><i class="fa fa-check"></i><b>4.2.1</b> Similarity</a></li>
<li class="chapter" data-level="4.2.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#jordan-matrix"><i class="fa fa-check"></i><b>4.2.2</b> Jordan matrix</a></li>
<li class="chapter" data-level="4.2.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#simultaneous-diagonalization"><i class="fa fa-check"></i><b>4.2.3</b> Simultaneous Diagonalization</a></li>
<li class="chapter" data-level="4.2.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>4.2.4</b> Cayley-Hamilton theorem</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#symmetric-matrices"><i class="fa fa-check"></i><b>4.3</b> Symmetric matrices</a><ul>
<li class="chapter" data-level="4.3.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#spectral-decomposition"><i class="fa fa-check"></i><b>4.3.1</b> Spectral decomposition</a></li>
<li class="chapter" data-level="4.3.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#a-rrthogonality"><i class="fa fa-check"></i><b>4.3.2</b> A-Rrthogonality</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#quadratic-forms"><i class="fa fa-check"></i><b>4.4</b> Quadratic forms</a><ul>
<li class="chapter" data-level="4.4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#change-of-variabele"><i class="fa fa-check"></i><b>4.4.1</b> Change of variabele</a></li>
<li class="chapter" data-level="4.4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#classification-of-quadratic-forms"><i class="fa fa-check"></i><b>4.4.2</b> Classification of quadratic forms</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#cholesky-factorization"><i class="fa fa-check"></i><b>4.5</b> Cholesky factorization</a></li>
<li class="chapter" data-level="4.6" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#rayleigh-quotients"><i class="fa fa-check"></i><b>4.6</b> Rayleigh quotients</a></li>
<li class="chapter" data-level="4.7" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#svd"><i class="fa fa-check"></i><b>4.7</b> SVD</a><ul>
<li class="chapter" data-level="4.7.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#singular-values-of-m-x-n-matrix"><i class="fa fa-check"></i><b>4.7.1</b> Singular values of m x n matrix</a></li>
<li class="chapter" data-level="4.7.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#svd-theorem"><i class="fa fa-check"></i><b>4.7.2</b> The singular value decomposition</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#matrix-norms"><i class="fa fa-check"></i><b>4.8</b> Matrix norms</a><ul>
<li class="chapter" data-level="4.8.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#induced-norms"><i class="fa fa-check"></i><b>4.8.1</b> Induced norms</a></li>
<li class="chapter" data-level="4.8.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#entry-wise-norm"><i class="fa fa-check"></i><b>4.8.2</b> Entry-wise norm</a></li>
<li class="chapter" data-level="4.8.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#other-matrix-norms"><i class="fa fa-check"></i><b>4.8.3</b> Other matrix norms</a></li>
<li class="chapter" data-level="4.8.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#unitary-invariancy"><i class="fa fa-check"></i><b>4.8.4</b> Unitary invariancy</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#low-rank-approximation"><i class="fa fa-check"></i><b>4.9</b> Low rank approximation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="solutions-of-linear-system-ax-b.html"><a href="solutions-of-linear-system-ax-b.html"><i class="fa fa-check"></i><b>5</b> Solutions of linear system Ax = b</a><ul>
<li class="chapter" data-level="5.1" data-path="solutions-of-linear-system-ax-b.html"><a href="solutions-of-linear-system-ax-b.html#generalized-inverse"><i class="fa fa-check"></i><b>5.1</b> Generalized inverse</a></li>
<li class="chapter" data-level="5.2" data-path="solutions-of-linear-system-ax-b.html"><a href="solutions-of-linear-system-ax-b.html#ill-conditioned-matrices"><i class="fa fa-check"></i><b>5.2</b> Ill-conditioned matrices</a><ul>
<li class="chapter" data-level="5.2.1" data-path="solutions-of-linear-system-ax-b.html"><a href="solutions-of-linear-system-ax-b.html#the-condition-number"><i class="fa fa-check"></i><b>5.2.1</b> The condition number</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Calculus</b></span></li>
<li class="chapter" data-level="6" data-path="taylor-series-and-expansion.html"><a href="taylor-series-and-expansion.html"><i class="fa fa-check"></i><b>6</b> Taylor series and expansion</a></li>
<li class="chapter" data-level="7" data-path="matrix-calculus.html"><a href="matrix-calculus.html"><i class="fa fa-check"></i><b>7</b> Matrix calculus</a></li>
<li class="chapter" data-level="8" data-path="infinite-sequences-and-series.html"><a href="infinite-sequences-and-series.html"><i class="fa fa-check"></i><b>8</b> Infinite sequences and series</a></li>
<li class="part"><span><b>III Probability Theory</b></span></li>
<li class="chapter" data-level="9" data-path="probability-basics.html"><a href="probability-basics.html"><i class="fa fa-check"></i><b>9</b> Probability basics</a><ul>
<li class="chapter" data-level="9.1" data-path="probability-basics.html"><a href="probability-basics.html#probabilty-space"><i class="fa fa-check"></i><b>9.1</b> Probabilty space</a></li>
<li class="chapter" data-level="9.2" data-path="probability-basics.html"><a href="probability-basics.html#counting"><i class="fa fa-check"></i><b>9.2</b> Counting</a></li>
<li class="chapter" data-level="9.3" data-path="probability-basics.html"><a href="probability-basics.html#conditional-probability"><i class="fa fa-check"></i><b>9.3</b> Conditional probability</a></li>
<li class="chapter" data-level="9.4" data-path="probability-basics.html"><a href="probability-basics.html#random-variables"><i class="fa fa-check"></i><b>9.4</b> Random variables</a></li>
<li class="chapter" data-level="9.5" data-path="probability-basics.html"><a href="probability-basics.html#properties-of-expectation-variance-and-covariance"><i class="fa fa-check"></i><b>9.5</b> Properties of expectation, variance, and covariance</a></li>
<li class="chapter" data-level="9.6" data-path="probability-basics.html"><a href="probability-basics.html#conditional-expectation"><i class="fa fa-check"></i><b>9.6</b> Conditional expectation</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="moments.html"><a href="moments.html"><i class="fa fa-check"></i><b>10</b> Moments</a></li>
<li class="chapter" data-level="11" data-path="univariate-distributions.html"><a href="univariate-distributions.html"><i class="fa fa-check"></i><b>11</b> Univariate distributions</a><ul>
<li class="chapter" data-level="11.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#uniform-distribution"><i class="fa fa-check"></i><b>11.1</b> Uniform distribution</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>12</b> Multivariate distributions</a></li>
<li class="part"><span><b>IV Optimization</b></span></li>
<li class="chapter" data-level="13" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html"><i class="fa fa-check"></i><b>13</b> Basics of optimization</a><ul>
<li class="chapter" data-level="13.1" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#univariate-optimization"><i class="fa fa-check"></i><b>13.1</b> Univariate optimization</a></li>
<li class="chapter" data-level="13.2" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#multivariate-optimization"><i class="fa fa-check"></i><b>13.2</b> Multivariate optimization</a></li>
<li class="chapter" data-level="13.3" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#lagrange-multipliers"><i class="fa fa-check"></i><b>13.3</b> Lagrange multipliers</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="gradient-descent.html"><a href="gradient-descent.html"><i class="fa fa-check"></i><b>14</b> Gradient descent</a></li>
<li class="part"><span><b>V Applications</b></span></li>
<li class="chapter" data-level="15" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>15</b> Linear models</a><ul>
<li class="chapter" data-level="15.1" data-path="linear-models.html"><a href="linear-models.html#ordinary-least-squares"><i class="fa fa-check"></i><b>15.1</b> Ordinary Least Squares</a><ul>
<li class="chapter" data-level="15.1.1" data-path="linear-models.html"><a href="linear-models.html#least-square-estimation"><i class="fa fa-check"></i><b>15.1.1</b> Least square estimation</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="linear-models.html"><a href="linear-models.html#weighted-least-squares"><i class="fa fa-check"></i><b>15.2</b> Weighted least squares</a></li>
<li class="chapter" data-level="15.3" data-path="linear-models.html"><a href="linear-models.html#partial-least-squres"><i class="fa fa-check"></i><b>15.3</b> Partial least squres</a></li>
<li class="chapter" data-level="15.4" data-path="linear-models.html"><a href="linear-models.html#regularized-regression"><i class="fa fa-check"></i><b>15.4</b> Regularized regression</a><ul>
<li class="chapter" data-level="15.4.1" data-path="linear-models.html"><a href="linear-models.html#ridge-regression"><i class="fa fa-check"></i><b>15.4.1</b> Ridge regression</a></li>
<li class="chapter" data-level="15.4.2" data-path="linear-models.html"><a href="linear-models.html#lasso-regression"><i class="fa fa-check"></i><b>15.4.2</b> Lasso regression</a></li>
<li class="chapter" data-level="15.4.3" data-path="linear-models.html"><a href="linear-models.html#elastic-net-regression"><i class="fa fa-check"></i><b>15.4.3</b> Elastic net regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>16</b> Principal component analysis</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li>
  <a href="https://github.com/rstudio/bookdown" target="blank">Written with bookdown</a>
</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Notes for Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="eigenthings-and-quadratic-forms" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Eigenthings and quadratic forms</h1>
<div id="eigenvectors-and-eigenvalues" class="section level2">
<h2><span class="header-section-number">4.1</span> Eigenvectors and eigenvalues</h2>

<div class="definition">
<p><span id="def:eigen" class="definition"><strong>Definition 4.1  (Eigenvectors and eigenvalues)  </strong></span>An <strong>eigenvector</strong> of an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is a <em>nonzero</em> vector <span class="math inline">\(\bar{x}\)</span> such that <span class="math inline">\(A\bar{x} = \lambda\bar{x}\)</span>.</p>
<span class="math inline">\(\lambda\)</span> is the <strong>eigenvalue</strong> of <span class="math inline">\(A\)</span> if there is a nontrivial solution <span class="math inline">\(\bar{x}\)</span> of <span class="math inline">\(A\bar{x} = \lambda \bar{x}\)</span>; such an <span class="math inline">\(\bar{x}\)</span> is called an <em>eigenvector corresponding to <span class="math inline">\(\lambda\)</span></em>
</div>

<p>To find eigenvalues and corresponding eigenvectors of <span class="math inline">\(A\)</span>, we look at the equation</p>
<p><span class="math display">\[
(A - \lambda I)\bar{x}= 0
\]</span></p>
<p>Since eigenvector <span class="math inline">\(\bar{x}\)</span> must be nonzero, <span class="math inline">\((A - \lambda I)\)</span> is a singular matrix</p>
<p><span class="math display" id="eq:characteristic-equation">\[\begin{equation}
\tag{4.1}
\det (A - \lambda I) = 0
\end{equation}\]</span></p>
<p>Eq <a href="eigenthings-and-quadratic-forms.html#eq:characteristic-equation">(4.1)</a> is called the <strong>characteristic equation</strong> of matrix <span class="math inline">\(A\)</span>. This is a scalar equation containing information about eigenvalues and eigenvectors of a square matrix <span class="math inline">\(A\)</span>.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-1" class="theorem"><strong>Theorem 1.1  </strong></span>Eigenvalues of a trangular matrix are its diagonal entries.
</div>

<p><strong>PROOF</strong></p>
<p>Consider the <span class="math inline">\(3 \times 3\)</span> case. If <span class="math inline">\(A\)</span> is upper triangular, then <span class="math inline">\(A - \lambda I\)</span> has the form</p>
<p><span class="math display">\[
\begin{bmatrix}
a_{11} - \lambda &amp; a_{12} &amp; a_{13} \\
0 &amp; a_{22} - \lambda &amp; a_{23}  \\
0 &amp; 0 &amp; a_{33} - \lambda
\end{bmatrix}
\]</span>
So the roots of characteristic are <span class="math inline">\(a_{11}, a_{22}, a_{33}\)</span> respectively.</p>
<p>There are some useful results about how eigenvalues change after various manipulations.</p>
<ol style="list-style-type: decimal">
<li><p>For any <span class="math inline">\(k, b \in \mathbb{R}\)</span>, <span class="math inline">\(\bar{x}\)</span> is an eigenvector of <span class="math inline">\(kA + bI\)</span> with eigenvalue <span class="math inline">\(k\lambda + b\)</span></p></li>
<li><p>If <span class="math inline">\(A\)</span> is invertible, then <span class="math inline">\(\bar{x}\)</span> is an eigenvector of <span class="math inline">\(A^{-1}\)</span> with eigenvalue <span class="math inline">\(1/\lambda\)</span></p></li>
<li><p><span class="math inline">\(A^{k}\bar{x} = \lambda^{k}\bar{x}\)</span></p></li>
</ol>
<p><strong>PROOF</strong></p>
<p>For (1)
<span class="math display">\[
(kA + bI)\bar{x} = kA\bar{x} + bI\bar{x} = k \lambda\bar{x} + b\bar{x} = (k\lambda + b)\bar{x} 
\]</span></p>
<p>For(2)</p>
<p><span class="math display">\[
\bar{x} = A^{-1}A\bar{x} =  A^{-1}\lambda \bar{x} = \lambda A^{-1}\bar{x}
\]</span></p>
<p>The next theorem is important in terms of diagonalization and spectral decomposition</p>

<div class="theorem">
<span id="thm:distinct-eigenvalue" class="theorem"><strong>Theorem 4.1  </strong></span>For distinct eigenvalues <span class="math inline">\(\lambda_1, \cdots, \lambda_r\)</span> of an <span class="math inline">\(n \times n\)</span> matrix A, their corresponding eigenvectors <span class="math inline">\(\bar{v_1}, ..., \bar{v_r}\)</span> are linearly independent.
</div>

<p><strong>PROOF</strong></p>
<p>Suppose for r distinct eigenvalue <span class="math inline">\(\lambda_1, \cdots, \lambda_r\)</span>, the set <span class="math inline">\(\{\bar{v_1}, ..., \bar{v_r}\}\)</span> is not linearly independent, and <span class="math inline">\(p\)</span> is the least index such that <span class="math inline">\(\bar{v}_{p+1}\)</span> is a linear combination of the preceding vectors. Then there exists scalars <span class="math inline">\(c_1, \cdots, c_p\)</span> such that</p>
<p><span class="math display">\[
c_1\bar{v}_1 + \cdots + c_p\bar{v}_p = \bar{v}_{p+1} \tag{1}
\]</span>
Left multiply by <span class="math inline">\(A\)</span>, and note we have <span class="math inline">\(A\bar{v}_i = \lambda_i\bar{v}_i\)</span> for <span class="math inline">\(i = 1, ..., n\)</span></p>
<p><span class="math display">\[
c_1\lambda_1\bar{v}_1 + \cdots + c_p\lambda_p\bar{v}_p = \lambda_{p+1}\bar{v}_{p+1} \tag{2}
\]</span>
Multiplying both sides of (2) by <span class="math inline">\(\lambda_{p+1}\)</span> and subtracting (2) from the result</p>
<p><span class="math display">\[
c_1(\lambda_1 - \lambda_{p+1})\bar{v}_1 +\cdots + c_p(\lambda_p - \lambda_{p+1})\bar{v}_p = 0 \tag{3}
\]</span>
Since <span class="math inline">\(\bar{v}_1, ..., \bar{v}_p\)</span> are linearly independent, weights in (3) must be all zero. Since <span class="math inline">\(\lambda_1, \cdots, \lambda_p\)</span> are distinct, hence <span class="math inline">\(c_i = 0, \, i = 1, ..., p\)</span>. But then (5) says that eigenvector <span class="math inline">\(\bar{v}_{p+1}\)</span> is zero vector, which contradicts definition <a href="eigenthings-and-quadratic-forms.html#def:eigen">4.1</a></p>
<hr>

<div class="corollary">
<span id="cor:same-nonzero" class="corollary"><strong>Corollary 4.1  </strong></span>Let <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> has the same set of <em>nonzero</em> eigenvalues.
</div>

<p><strong>PROOF</strong></p>
<p>Let <span class="math inline">\(\lambda\)</span> be a nonzero eigenvalue of <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(\bar{x}\)</span> its eigenvector</p>
<p><span class="math display">\[
\begin{split}
(A^TA)\bar{x} &amp;= \lambda\bar{x} \\
\end{split}
\]</span>
Left multiply by <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
AA^T(A\bar{x}) = \lambda (A\bar{x})
\]</span>
We will have to verify that <span class="math inline">\(A\bar{x}\)</span> is no zero vector before concluding <span class="math inline">\(\lambda\)</span> is also an eigenvector of <span class="math inline">\(AA^T\)</span>. Suppose <span class="math inline">\(A\bar{x} = 0\)</span>, then <span class="math inline">\(A^TA\bar{x} =\lambda\bar{x} = 0\)</span>. Since <span class="math inline">\(\bar{x}\)</span> is a eigenvector which is nonzero, <span class="math inline">\(\lambda = 0\)</span>, which contradicts our former statement. Thus, any nonzero eigenvalue of <span class="math inline">\(A^TA\)</span> is also an eigenvalue of <span class="math inline">\(AA^T\)</span>.</p>
<p><span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> are known as Gram matrix and left Gram matrix in corollary <a href="vector-spaces.html#prp:gram-matrix">2.1</a></p>
<div id="additional-properties-of-eigenvalues-and-eigenvectors" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Additional properties of eigenvalues and eigenvectors</h3>
<p>Let <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> with eigenvalues <span class="math inline">\(\lambda_1, ..., \lambda_n\)</span>. Here are some additional properties of this matrix and its eigenvlaues:</p>
<ul>
<li>The trace of <span class="math inline">\(A\)</span> is the sum of all eigenvalues</li>
</ul>
<p><span class="math display">\[
\text{tr}(A) = \sum_{i=1}^{n}{\lambda_i}
\]</span></p>
<ul>
<li>The determinant of <span class="math inline">\(A\)</span> is the product of all its eigenvalues.</li>
</ul>
<p><span class="math display">\[
\det(A) = \prod_{i=1}^{n}{\lambda_i}
\]</span></p>
<ul>
<li><p>The eigenvalues of <span class="math inline">\(k\)</span>th power of <span class="math inline">\(A\)</span>, i.e. <span class="math inline">\(A^k\)</span>, is <span class="math inline">\(\lambda_1^k, ..., \lambda_n^k\)</span></p></li>
<li><p>If <span class="math inline">\(A\)</span> is invertible, then eigenvalues of <span class="math inline">\(A^{-1}\)</span> are <span class="math inline">\(\frac{1}{\lambda_1}, ..., \frac{1}{\lambda_n}\)</span></p></li>
<li><p>For a polynomial function <span class="math inline">\(P\)</span> the eigenvalues of <span class="math inline">\(P(A)\)</span> are <span class="math inline">\(P(\lambda_1), ..., P(\lambda_n)\)</span></p></li>
</ul>
</div>
<div id="left-eigenvectors-and-right-eigenvectors" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Left eigenvectors and right eigenvectors</h3>
<p><span class="math display">\[
\bar{x}A = \lambda\bar{x}
\]</span></p>
</div>
</div>
<div id="diagnolization-and-similar-matrices" class="section level2">
<h2><span class="header-section-number">4.2</span> Diagnolization and similar matrices</h2>

<div class="definition">
<p><span id="def:diagonalization" class="definition"><strong>Definition 4.2  (Diagonalization thoerem)  </strong></span>An <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is diagnolizable <strong>if and only if</strong> A has <span class="math inline">\(n\)</span> independent linearly independent eigenvectors.</p>
<p>In such case, in <span class="math inline">\(A = P \Lambda P^{-1}\)</span>, the diagonal entries of <span class="math inline">\(D\)</span> are eigenvalues that correpond, respectively, to the eigenvectors of in <span class="math inline">\(P\)</span></p>
In other words, <span class="math inline">\(A\)</span> is diagnolizable if and only if there are enough eigenvectors in form a basis of <span class="math inline">\(R^n\)</span>, called an <strong>eigenvector basis</strong> of <span class="math inline">\(R^n\)</span>
</div>

<p><strong>Proof</strong></p>
<p><span class="math display">\[
\begin{split}
AP &amp;= A[\bar{v}_1 \cdots \bar{v}_n] \\
   &amp;= [A\bar{v}_1 \cdots A\bar{v}_n] \\ 
   &amp;= [\lambda_1\bar{v}_1 \cdots \lambda_n\bar{v}_n]
\end{split}
\]</span>
while on the other side of the equation:</p>
<p><span class="math display">\[
\begin{aligned}
DP &amp;= 
[\bar{v}_1 \cdots \bar{v}_n]
\begin{bmatrix}
\lambda_1 &amp; 0 &amp; \cdots &amp; 0\\
0  &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_n 
\end{bmatrix} 
 \\
&amp;= [\lambda_1\bar{v}_1 \cdots \lambda_n\bar{v}_n]
\end{aligned}
\]</span></p>
<p>So that</p>
<p><span class="math display">\[
\begin{aligned}
AP &amp;= PD \\
A &amp;= P \Lambda P^{-1}
\end{aligned}
\]</span>
Because <span class="math inline">\(P\)</span> contains <span class="math inline">\(n\)</span> independent columns so it’s invertible.</p>
<p>According to theorem <a href="eigenthings-and-quadratic-forms.html#thm:distinct-eigenvalue">4.1</a>, an <span class="math inline">\(n \times n\)</span> matrix with <span class="math inline">\(n\)</span> distinct eigenvalues is diagonalizable. This is a sufficient condition.</p>
<p>For matrices whose eigenvalues are not distinct, there is still a change that it is diagonalizable. For any matrix <span class="math inline">\(A_{n\times n}\)</span>, as long as the sum of the dimensions of the eigenspaces equals <span class="math inline">\(n\)</span> then <span class="math inline">\(P\)</span> is invertible. This could happen in the following two scenarios</p>
<ol style="list-style-type: decimal">
<li><p>The characteristic polynomial factors completely into linear factors. This is the case when <span class="math inline">\(A\)</span> has n distinct eigenvalues.</p></li>
<li><p>The dimension of the eigenspace for each <span class="math inline">\(\lambda_k\)</span> equals the multiplicity of <span class="math inline">\(\lambda_k\)</span>. Thus <span class="math inline">\(A\)</span> with repeated eigenvalues can still be diagonalizable.<br />
Repeated eigenvalues create the possibility that a diagonalization might not exist. Particularly, if less than <span class="math inline">\(r_i\)</span> eigenvectors exist for an eigenvalue with multiplicity <span class="math inline">\(r_i\)</span>, a diagonalization does not exist. Such a matrix is said to be <strong>deflective</strong>.</p></li>
</ol>
<div id="similarity" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Similarity</h3>
<p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are both <span class="math inline">\(n \times n\)</span> matrices, then <span class="math inline">\(A\)</span> <strong>is similar to</strong> <span class="math inline">\(B\)</span> if there is an invertible matrix <span class="math inline">\(P\)</span> such that <span class="math inline">\(P^{-1}AP = B\)</span>, or equivalently if we write <span class="math inline">\(Q\)</span> for <span class="math inline">\(P^{-1}\)</span>, <span class="math inline">\(Q^{-1}BQ = A\)</span>. Changing <span class="math inline">\(A\)</span> into <span class="math inline">\(P^{-1}AP\)</span> is called a <strong>similarity transformation</strong>.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-2" class="theorem"><strong>Theorem 3.1  </strong></span>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are similar, they have the same eigenvalues.
</div>

<p><strong>PROOF</strong><br />
If <span class="math inline">\(B = P^{-1}AP\)</span>, then</p>
<p><span class="math display">\[
B - \lambda I = P^{-1}AP - \lambda P^{-1}P = P^{-1}(AP - \lambda P) =  P^{-1}(A - \lambda I) P
\]</span>
so that</p>
<p><span class="math display">\[
\det (B - \lambda I ) = \det(P) \cdot \det(A - \lambda I ) \cdot \det(P^{-1})
\]</span>
since <span class="math inline">\(\det(P) \cdot \det(P^{-1}) = \det (I) = 1\)</span>, we have</p>
<p><span class="math display">\[
\det (B - \lambda I)  = \det(A - \lambda I)
\]</span></p>
<p>As a result of their identical characteristic polynomial, <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span> have the same eigenvalues. We can also show that eigenvector of <span class="math inline">\(B\)</span> is <span class="math inline">\(P\bar{v}\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
A\bar{v} &amp;= \lambda\bar{v} \\
(P^{-1}BP)\bar{v} &amp;= \lambda\bar{v} \\
P(P^{-1}BP)\bar{v} &amp;= \lambda P\bar{v} \\
B(P\bar{v}) = \lambda P \bar{v}
\end{aligned}
\]</span></p>
<p>The similarity theorem leads to a interesting result.</p>

<div class="corollary">
<span id="cor:unnamed-chunk-3" class="corollary"><strong>Corollary 2.1  </strong></span>For <span class="math inline">\(A, B \in \mathbb{R}^{n \times n}\)</span>, <span class="math inline">\(AB\)</span> and <span class="math inline">\(BA\)</span> are similar matrices and therefore share the same set of eigenvalues.
</div>

<p>To prove this, we need to show that there exists a invertible matrix <span class="math inline">\(A\)</span> such that <span class="math inline">\(P^{-1}(AB)P = BA\)</span>. Take <span class="math inline">\(P = A\)</span> and the equation holds.</p>
<p>It is easy to show that similarity is <strong>transitive</strong>: if <span class="math inline">\(A\)</span> is similar to <span class="math inline">\(B\)</span>, <span class="math inline">\(B\)</span> is similar to <span class="math inline">\(C\)</span>, then <span class="math inline">\(A\)</span> is similar to <span class="math inline">\(C\)</span>. So similarity means a family of matrices with the same set of eigenvalues, the most special and simplest of which is the diagonal matrix (if this is an diagonalizable family). Some computer algorithms calculate eigenvalues of <span class="math inline">\(A\)</span> in this manner: with a sequential choices of <span class="math inline">\(P\)</span>, the off-diagonal elements of <span class="math inline">\(A\)</span> become smaller and smaller until <span class="math inline">\(A\)</span> becomes a triangular matrix or diagonal matrix, whose eigenvalues are simply diagonal entries and is the same as <span class="math inline">\(A\)</span>.</p>
<p>It is obvious that a diagonalizable matrix <span class="math inline">\(A\)</span> is similar to diagonal matrix <span class="math inline">\(D\)</span>, whose diagonal entries are <span class="math inline">\(A\)</span>’s eigenvalues <span class="math inline">\(\lambda_i\)</span>, and <span class="math inline">\(P = [\bar{v}_1 \;\; \cdots \;\; \bar{v}_n]^{-1}\)</span> where <span class="math inline">\(\bar{v}_i, \;i = 1,..., n\)</span> are eigenvectors corresponding to <span class="math inline">\(\lambda_i\)</span>.</p>
<p>But square matrix <span class="math inline">\(A\)</span> can still be similar to matrices other than <span class="math inline">\(D\)</span> with other choices of <span class="math inline">\(P\)</span>, and non-diagonal matrices can also have similar matrices of their own. In fact, <strong>every square matrix is similar to a matrix in Jordan matrix</strong> <a href="eigenthings-and-quadratic-forms.html#jordan-matrix">4.2.2</a>.</p>
<hr>
<p>Similarity is only a <em>sufficient</em> condition for identical eigenvalues. The matrices</p>
<p><span class="math display">\[
\begin{bmatrix}
2 &amp; 1 \\
0 &amp; 2 
\end{bmatrix}
\;\text{and}\;
\begin{bmatrix}
2 &amp; 0 \\
0 &amp; 2 
\end{bmatrix}
\]</span>
are not similar even though they have the same eigenvalues.</p>
</div>
<div id="jordan-matrix" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Jordan matrix</h3>
<p>For non-diagonalizable square matrix <span class="math inline">\(A_{n \times n}\)</span>, the goal is to with similar transformation <span class="math inline">\(P^{-1}AP\)</span> construct a matrix that is as nearest to a diagonal matrix as possible.</p>

<div class="definition">
<span id="def:unnamed-chunk-4" class="definition"><strong>Definition 4.3  </strong></span>The <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(J_{\lambda, n}\)</span> with <span class="math inline">\(\lambda\)</span>s on the diagonal, <span class="math inline">\(1\)</span>s on the superdiagonal and <span class="math inline">\(0\)</span>s elsewhere is called a Jordan matrix. A Jordan matrix in Jordan normal form is a block matrix that has Jordan blocks down its block diagonal and is zero elsewhere
</div>

<p>An example of Jordan matrix, the appearance of <span class="math inline">\(\lambda_i\)</span> on the diagonal is equal to its multiplicity as <span class="math inline">\(A\)</span>’s eigenvalue.
<span class="math display">\[
\begin{bmatrix}
\lambda_1 &amp; 1  &amp; \\
&amp; \lambda_1 &amp; 1 &amp; \\
&amp; &amp; \lambda_1 &amp; \\ 
&amp; &amp; &amp; \lambda_2 &amp; 1 \\
&amp; &amp; &amp; &amp; \lambda_2  \\ 
&amp; &amp; &amp; &amp; &amp; \lambda_3 &amp; 1  \\ 
&amp; &amp; &amp; &amp; &amp; &amp; \ddots \\
&amp; &amp; &amp; &amp; &amp; &amp; &amp; \lambda_n &amp; 1 \\
&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \lambda_n
\end{bmatrix}
\]</span>
An illustration from <a href="https://en.wikipedia.org/wiki/Jordan_normal_form">wikipedia</a>, the circled area is the Jordan blcok.
<img src="images/jordan-blocks.png" width="40%" style="display: block; margin: auto;" /></p>
<p>Though the purpose of this section was not the computation details of Jordan matrices, it helps to give a concrete example. Consider <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
A = 
\begin{bmatrix}
5 &amp; 4 &amp; 2 &amp; 1 \\
0 &amp; 1 &amp; -1 &amp; -1 \\
-1 &amp; -1 &amp; 3 &amp; 0 \\
1 &amp; 1 &amp; -1 &amp; 2
\end{bmatrix}
\]</span></p>
<p>Including multiplicity, the eigenvalues of <span class="math inline">\(A\)</span> are <span class="math inline">\(\lambda = 1, 2, 4, 4\)</span>. And for <span class="math inline">\(\lambda = 4\)</span>, the eigenspace is 1 dimensional instead of 2, meaning <span class="math inline">\(A\)</span> is not diagonalizable. Nonetheless, <span class="math inline">\(A\)</span> is similar to the following Jordan matrix</p>
<p><span class="math display">\[
J = 
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 2 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 4 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 4
\end{bmatrix}
\]</span></p>
<p>To obtain <span class="math inline">\(P\)</span>, recall that <span class="math inline">\(P^{-1}AP = J\)</span>. Let <span class="math inline">\(P\)</span> have column vectors <span class="math inline">\(p_i, \; i = 1,...,4\)</span>, then:</p>
<p><span class="math display">\[
A[\bar{p}_1 \; \; \bar{p}_2 \;\; \bar{p}_3 \;\; \bar{p}_4] = [\bar{p}_1 \; \; \bar{p}_2 \;\; \bar{p}_3 \;\; \bar{p}_4]
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 2 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 4 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 4
\end{bmatrix}
= [\bar{p}_1 \;\; 2\bar{p}_2 \;\; 4\bar{p}_3 \;\; \bar{p}_3 + 4\bar{p}_4]
\]</span></p>
<p>We see that</p>
<p><span class="math display">\[
\begin{aligned}
(A - 1I)\bar{p}_1 &amp;= \bar{0} \\
(A - 2I)\bar{p}_2 &amp;= \bar{0} \\
(A - 4I)\bar{p}_3 &amp;= \bar{0} \\
(A - 1I)\bar{p}_4 &amp;= \bar{p}_3 
\end{aligned}
\]</span>
The solutions <span class="math inline">\(\bar{p}_i\)</span> are called <strong>generalized eigenvectors</strong> of <span class="math inline">\(A\)</span>.</p>
</div>
<div id="simultaneous-diagonalization" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Simultaneous Diagonalization</h3>
<p>A diagonlizable matrix family that share the <strong>same eigenvectors</strong> is called <em>simultaneously diagonalizable</em>. This notion is complimentary to a family of similar matrices that are diagonalizable, share eigenvalues but not eigenvectors.</p>

<div class="definition">
<p><span id="def:simultaneous-diagonalizable" class="definition"><strong>Definition 4.4  (Simultaneously diagonalizable)  </strong></span>Two diagonalizable matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are said to be <em>simultaneously diagonalizable</em> if a <span class="math inline">\(n \ times n\)</span> matrix <span class="math inline">\(P\)</span> exists, such that <span class="math inline">\(P^{-1}AP\)</span> and <span class="math inline">\(P^{-1}BP\)</span> are diagonal matrices. In other words</p>
<span class="math display">\[
A =  P\Lambda_1P^{-1} \\
B = P\Lambda_2P^{-1}
\]</span>
</div>

<p>The geometric interpretation of simultaneously diagonalizable matrices is that they perform scaling in the same set of directions.</p>
<hr>

<div class="theorem">
<span id="thm:unnamed-chunk-6" class="theorem"><strong>Theorem 4.2  </strong></span>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are diagonalizable matrices, they are simultaneously diagonalizable if and only if they commute, such that <span class="math inline">\(AB\)</span> = <span class="math inline">\(BA\)</span>.
</div>

<p>I don’t know how to prove this yet. But this theorem is useful in identifying diagonalizable matrices with the same eigenvectors.</p>
</div>
<div id="cayley-hamilton-theorem" class="section level3">
<h3><span class="header-section-number">4.2.4</span> Cayley-Hamilton theorem</h3>
<p>For any square matrix <span class="math inline">\(A_{n \times n}\)</span>, the characteristic polynomial of <span class="math inline">\(\lambda\)</span> is defined as</p>
<p><span class="math display">\[
\det(A - \lambda I)
\]</span></p>
<p>We can obtain a polynomial of matrix <span class="math inline">\(A\)</span> by substituting <span class="math inline">\(A\)</span> for <span class="math inline">\(\lambda\)</span>, and <span class="math inline">\(kI\)</span> for constant terms. For example, the matrix form of the polynomial <span class="math inline">\(3\lambda^2 + 2\lambda + 2\)</span> is <span class="math inline">\(3A^2 + 2A + 2I\)</span>.</p>

<div class="theorem">
<span id="thm:cayley-hamilton" class="theorem"><strong>Theorem 4.3  (Cayley-Hamilton Theorem)  </strong></span>
Let <span class="math inline">\(f(\lambda)\)</span> be the polynomial function of the characteristic polynomial <span class="math inline">\(\det(A - \lambda I)\)</span>, where <span class="math inline">\(A\)</span> is a square matrix. Then <span class="math inline">\(f(A)\)</span> evaluates to a zero matrix.
</div>

<p><strong>PROOF</strong></p>
<p>Though the Caley Hamilton theorem <a href="eigenthings-and-quadratic-forms.html#thm:cayley-hamilton">4.3</a> applies to any square matrix <span class="math inline">\(A\)</span>. Our proof only address the case for diagonalizable matrices.</p>
<p>When <span class="math inline">\(A\)</span> is digonalizable, the polynomial of <span class="math inline">\(A\)</span> takes the form</p>
<p><span class="math display">\[
f(A) = Pf(\Lambda)P^{-1}
\]</span>
Since <span class="math inline">\(f(\lambda) = \det(A - \lambda I)\)</span>, and the diagonal entries of <span class="math inline">\(\Lambda\)</span> are the eigenvalues of <span class="math inline">\(A\)</span>. Evaluate <span class="math inline">\(f(\lambda)\)</span> at each diagonal entry of <span class="math inline">\(\Lambda\)</span> will be zero. Thus <span class="math inline">\(f(A)\)</span> is a zero matrix.</p>
<p>A direct result derived from the Cayley Hamilton theorem is that for every intertible matrix <span class="math inline">\(A\)</span>, its inverse <span class="math inline">\(A^{-1}\)</span> can be represented as a polynomial of <span class="math inline">\(A\)</span> with degree <span class="math inline">\(d - 1\)</span>.</p>
<p><br></p>

<div class="proposition">
<span id="prp:unnamed-chunk-7" class="proposition"><strong>Proposition 4.1  (Polynomial representation of matrix inverse)  </strong></span>The <em>inverse</em> of an invertible square matrix <span class="math inline">\(A\)</span> is a polynomial of <span class="math inline">\(A\)</span> with degree at most <span class="math inline">\(d -1\)</span>.
</div>

<p>Since the constant term in the characteristic polynomial is the product of eigenvalues, which is nonzero for nonsingular matrices, we can write the Cayley-Hamilton matrix polynomial <span class="math inline">\(f(A)\)</span> in the form <span class="math inline">\(f(A) = A \cdot g(A) + kI\)</span>. <span class="math inline">\(A \cdot g(A)\)</span> is obtained by factoring out <span class="math inline">\(A\)</span> from the d-degree matrix polynomial, leaving <span class="math inline">\(g(A)\)</span> with degree of <span class="math inline">\(d - 1\)</span>. Since <span class="math inline">\(f(A)\)</span> evaluates to zero, we have</p>
<p><span class="math display">\[
A \underbrace{\Big( - g(A) / k\Big)}_{A^{-1}} = I
\]</span>
Therefore, <span class="math inline">\(A^{-1}\)</span> is shown to be a polynomial of <span class="math inline">\(A\)</span>.</p>
</div>
</div>
<div id="symmetric-matrices" class="section level2">
<h2><span class="header-section-number">4.3</span> Symmetric matrices</h2>
<p>A <em>square</em> matrix <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> is <em>symmetric</em> if <span class="math inline">\(A = A^{T}\)</span>, and <em>anti-symmetric</em> if <span class="math inline">\(A = - A^{T}\)</span>.</p>
<p>It can be shown that for any <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>, <span class="math inline">\(A + A^T\)</span> is symmetric and <span class="math inline">\(A - A^T\)</span> anti-symmetric. So any square matrix <span class="math inline">\(A\)</span> can be wrote as a sum of a symmetric matrix and an anti-symmetric matrix</p>
<p><span class="math display">\[
A = \frac{1}{2}(A + A^T) + \frac{1}{2}(A - A^T)
\]</span></p>
<p>It is common to denote the set of all symmetric matrices of size <span class="math inline">\(n\)</span> as <span class="math inline">\(\mathbb{S}^n\)</span>, and <span class="math inline">\(A \in \mathbb{S}^n\)</span> means <span class="math inline">\(A\)</span> is a symmetric <span class="math inline">\(n \times n\)</span> matrix.</p>
<p>Symmetric matrices have some nice properties about diagonalization.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-8" class="theorem"><strong>Theorem 4.4  </strong></span>If <span class="math inline">\(A\)</span> is symmetric, eigenvectors from distinct eigenvalues are <strong>orthogonal</strong>.
</div>

<p><strong>PROOF</strong></p>
<p>Let <span class="math inline">\(\bar{v}_1\)</span> and <span class="math inline">\(\bar{v}_2\)</span> be eigenvectors that correspond to distinct eigenvalues <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>. Compute</p>
<p><span class="math display">\[
\begin{split}
\lambda_1\bar{v}_1 \cdot \bar{v}_2 &amp;= (\lambda_1\bar{v}_1)^T\bar{v}_2 \\
&amp;= (\bar{v}_1^TA^T)\bar{v}_2 \\
&amp;= \bar{v}_1^T(A\bar{v}_2) \\
&amp;= \bar{v}_1^T(\lambda_2\bar{v}_2) \\
&amp;= \lambda_2\bar{v}_1 \cdot \bar{v}_2
\end{split}
\]</span>
because <span class="math inline">\(\lambda_1 \not = \lambda_2\)</span>, <span class="math inline">\(\bar{v}_1 \cdot \bar{v}_2 = 0\)</span>.</p>
<p>For symmetric matrices <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> without <span class="math inline">\(n\)</span> distinct eigenvalues, it turns out that the dimension of the eigenspace for each <span class="math inline">\(\lambda_k\)</span> always equals the multiplicity of <span class="math inline">\(\lambda_k\)</span>. For this reason, if <span class="math inline">\(A\)</span> is a symmetric matrix we can always construct a orthonormal set <span class="math inline">\(\{\bar{q}_1 \;\; \cdots \;\; \bar{q}_n\}\)</span> from <span class="math inline">\(\{\bar{v}_1 \;\; \cdots \;\; \bar{v}_n\}\)</span> such that</p>
<p><span class="math display">\[
Q^{T} = 
\begin{bmatrix}
\bar{q}_1^T \\
\vdots \\ 
\bar{q}_n^T
\end{bmatrix}
= Q^{-1}
\]</span>
Recall that matrix <span class="math inline">\(A\)</span> with <span class="math inline">\(n\)</span> linearly independent eigenvectors is diagonalizable and can be written as</p>
<p><span class="math display">\[
A = P \Lambda P^{-1}
\]</span>
where <span class="math inline">\(P = [\bar{v}_1 \;\; \cdots \;\; \bar{v}_n]\)</span> and <span class="math inline">\(\Lambda\)</span> is a diagonal matrix with eigenvalues on its diagonal entries.</p>
<p>With symmetric matrices, <span class="math inline">\(\{\bar{v}_1, \cdots, \bar{v}_n\}\)</span> must be linearly independent and can be transformed into a orthonormal basis <span class="math inline">\(\{\bar{q}_1, \cdots, \bar{q}_n\}\)</span>. With orthogonal matrix <span class="math inline">\(Q =[\bar{q}_1 \;\; \cdots \;\; \bar{q}_n]\)</span>, we have</p>
<p><span class="math display" id="eq:orthogonal-diagonalization">\[\begin{equation}
\tag{4.2}
A = Q \Lambda Q^{T}
\end{equation}\]</span></p>
<p>Such matrix <span class="math inline">\(A\)</span> is said to be <strong>orthogonally diagonalizable</strong>.</p>
<p>We have seen that for symmetric matrix <span class="math inline">\(A\)</span>, Eq <a href="eigenthings-and-quadratic-forms.html#eq:orthogonal-diagonalization">(4.2)</a> always holds. We can also also verify that if <span class="math inline">\(A\)</span> is orthogonally diagonalizable then it is a symmetric matrix</p>
<p><span class="math display">\[
A^T = (Q \Lambda Q^{T})^T = (Q^T)^T\Lambda^TQ^T= Q \Lambda Q^{T}  = A
\]</span></p>

<div class="theorem">
<span id="thm:unnamed-chunk-9" class="theorem"><strong>Theorem 4.5  </strong></span>An <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is orthogonally diagonalizable if an only if <span class="math inline">\(A\)</span> is a symmetric matrix.
</div>

<div id="spectral-decomposition" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Spectral decomposition</h3>
<p>For orthogonally diagonalizable matrix <span class="math inline">\(A\)</span>, we have</p>
<p><span class="math display">\[
A = Q \Lambda Q^{T} = [\bar{q}_1 \;\; \cdots \;\; \bar{q}_n] 
\begin{bmatrix}
\lambda_1 &amp; &amp; \\
 &amp; \ddots \\
 &amp; &amp; \lambda_n
\end{bmatrix}
\begin{bmatrix}
\bar{q}_1^T \\
\vdots \\
\bar{q}_n
\end{bmatrix}
\]</span></p>
<p>It follows that</p>
<p><span class="math display" id="eq:spectral-decomposition">\[\begin{equation}
\tag{4.3}
A = \lambda_1\bar{q}_1\bar{q}_1^T + \cdots + \lambda_1\bar{q}_n\bar{q}_n^T
\end{equation}\]</span></p>
<p>Eq <a href="eigenthings-and-quadratic-forms.html#eq:spectral-decomposition">(4.3)</a> is called the <strong>spectral decomposition</strong>, breaking <span class="math inline">\(A\)</span> into pieces of rank 1 matrix. It got this name because he set of eigenvalues of a matrix <span class="math inline">\(A\)</span> is sometimes called its <em>spectrum</em>.</p>
</div>
<div id="a-rrthogonality" class="section level3">
<h3><span class="header-section-number">4.3.2</span> A-Rrthogonality</h3>

<div class="definition">
<span id="def:unnamed-chunk-10" class="definition"><strong>Definition 4.5  (A-Orthogonality)  </strong></span>Column vector <span class="math inline">\(\bar{v}_i\)</span> and <span class="math inline">\(\bar{v}_j\)</span> are said to be <em>A-orthogonal</em> if <span class="math inline">\(\bar{v}_i^TA\bar{v}_j\)</span> for some <span class="math inline">\(n \times n\)</span> invertible matrix <span class="math inline">\(A\)</span>.
</div>

<p>Similarly, a set of column vectors <span class="math inline">\(\bar{v}_1, ..., \bar{v}_n\)</span> is A-orthogonal, if and only if <span class="math inline">\(\bar{v}_i^TA\bar{v}_i = 0\)</span> for each pair of vectors.</p>
</div>
</div>
<div id="quadratic-forms" class="section level2">
<h2><span class="header-section-number">4.4</span> Quadratic forms</h2>

<div class="definition">
<span id="def:unnamed-chunk-11" class="definition"><strong>Definition 4.6  (Quadratic form)  </strong></span>A <strong>quadratic form</strong> on <span class="math inline">\(\mathbb{R}^n\)</span> is a function <span class="math inline">\(Q\)</span> defined on <span class="math inline">\(\mathbb{R}^n\)</span> whose value at a vector <span class="math inline">\(\bar{x}\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span> can be computed by an expression of the form <span class="math inline">\(Q(\bar{x}) = \bar{x}^TA\bar{x}\)</span>, where <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> is a <strong>symmetric</strong> matrix. <span class="math inline">\(A\)</span> is called the matrix of the quadraticc form.
</div>

<p>There exists a one-to-one mapping between symmetric matrix <span class="math inline">\(A\)</span> and the quadratic form. Consider the <span class="math inline">\(3 \times 3\)</span> case:</p>
<p><span class="math display">\[
\bar{x} =
\begin{bmatrix}
x_1 \\
x_3 \\
x_3 \\
\end{bmatrix}
, \;\; A = 
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33} 
\end{bmatrix} \\
\]</span></p>
<p><span class="math display">\[
\begin{split}
\bar{x}^TA\bar{x} &amp;= 
[x_1 \;\; x_2 \;\; x_3]
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33} 
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_3 \\
x_3 \\
\end{bmatrix} \\
&amp;= a_{11}x_1^2 + a_{22}x_2^2 + a_{33}x_3^2 + \\
&amp; \quad(a_{12} + a_{21})x_1x_2 + (a_{13} + a_{31})x_1x_3 + (a_{23} + a_{32})x_2x_3 
\end{split} 
\tag{1}
\]</span>
Since <span class="math inline">\(A\)</span> is symmetric, we have <span class="math inline">\(a_{ij} = a_{ji}\)</span>, thus</p>
<p><span class="math display">\[
\bar{x}^TA\bar{x}  = a_{11}x_1^2 + a_{22}x_2^2 + a_{33}x_3^2 + 2a_{12}x_1x_2 + 2a_{13}x_1x_3 + 2a_{23}x_2x_3 \tag{2} 
\]</span>
This verifies that <span class="math inline">\(\bar{x}^TA\bar{x}\)</span> when <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> is symmetric does result in a quadratic function of <span class="math inline">\(n\)</span> variables. Conversely, any quadratic function of <span class="math inline">\(n\)</span> variables, like shown in <span class="math inline">\((2)\)</span>, can be expressed in terms of <span class="math inline">\(\bar{x}^TA\bar{x}\)</span> with unique choice of symmetric matrix <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>.</p>
<div id="change-of-variabele" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Change of variabele</h3>
<p>If <span class="math inline">\(\bar{x}\)</span> is a variable vector in <span class="math inline">\(\mathbb{R}^n\)</span>, then a <em>change of variable</em> is an equation of the form</p>
<p><span class="math display">\[
\begin{aligned}
\bar{x} &amp;= P\bar{y} \\
\text{or equivalently} \quad \bar{y} &amp;= P^{-1}\bar{x}
\end{aligned}
\]</span>
where <span class="math inline">\(P\)</span> is any invertible matrix <span class="math inline">\(\in \mathbb{R}^{n \times n}\)</span></p>

<div class="theorem">
<p><span id="thm:principal-axes" class="theorem"><strong>Theorem 4.6  (The Principal Axes Theorem)  </strong></span>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n \times n\)</span> symmetric matrix. Then there exists an orthogonal change of variable, <span class="math inline">\(\bar{x} = Q\bar{y}\)</span>, this transform the quadratic form <span class="math inline">\(\bar{x}^TA\bar{x}\)</span> into a quadratic form <span class="math inline">\(\bar{y}^T\Lambda\bar{y}\)</span> with no cross-product term.</p>
<p><span class="math inline">\(Q\)</span> is constructed with <span class="math inline">\(A\)</span>’s orthonormal eigenvectors <span class="math inline">\(\bar{q}_1, ..., \bar{q}_n\)</span>. According to theorem <a href="eigenthings-and-quadratic-forms.html#eq:orthogonal-diagonalization">(4.2)</a>:</p>
<span class="math display">\[
\bar{x}^TA\bar{x} = (Q\bar{y})^TA(Q\bar{y}) = \bar{y}^TQ^{T}AQ\bar{y} = \bar{y}^T \Lambda \bar{y}
\]</span>
</div>

<p>The principal axes theorem <a href="eigenthings-and-quadratic-forms.html#thm:principal-axes">4.6</a> shows that if <span class="math inline">\(A\)</span> is diagonalizable, quadratic form <span class="math inline">\(\bar{x}^TA\bar{x}\)</span> can be reexpressed into the form <span class="math inline">\(\lambda_1y_1^2 + \lambda_2y_2^2 + \cdots + \lambda_ny_n^2\)</span> with change of variables <span class="math inline">\(\bar{x} = Q\bar{y}\)</span>.</p>
</div>
<div id="classification-of-quadratic-forms" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Classification of quadratic forms</h3>
<ul>
<li><p>A symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is <strong>positive definite</strong> (PD) if for all non-zero vectors <span class="math inline">\(\bar{x} \in \mathbb{R}^n,\; \bar{x}^TA\bar{x} &gt; 0\)</span>. We can denote positive definite matrix <span class="math inline">\(A\)</span> as <span class="math inline">\(A \succ 0\)</span> (or <span class="math inline">\(A &gt; 0\)</span>). The set of all positive definite matrices is denoted as <span class="math inline">\(\mathbb{S}_{++}^n\)</span></p></li>
<li><p>A symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is <strong>positive semidefinite</strong> (PSD) if for all non-zero vectors <span class="math inline">\(\bar{x} \in \mathbb{R}^n,\; \bar{x}^TA\bar{x} \ge 0\)</span>. We can denote positive definite matrix <span class="math inline">\(A\)</span> as <span class="math inline">\(A \succeq 0\)</span> (or <span class="math inline">\(A \ge 0\)</span>). The set of all positive semidefinite matrices is denoted as <span class="math inline">\(\mathbb{S}_{+}^n\)</span></p></li>
<li><p>A symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is <strong>negative definite</strong> (ND), denoted by <span class="math inline">\(A \prec 0\)</span> (or <span class="math inline">\(A &lt; 0\)</span>), if for all non-zero vectors <span class="math inline">\(\bar{x} \in \mathbb{R}^n,\; \bar{x}^TA\bar{x} &lt; 0\)</span>.</p></li>
<li><p>Similarly, a symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is <strong>negative semidefinite</strong> (NSD), denoted by <span class="math inline">\(A \preceq 0\)</span> (or <span class="math inline">\(A \le 0\)</span>), if for all non-zero vectors <span class="math inline">\(\bar{x} \in \mathbb{R}^n,\; \bar{x}^TA\bar{x} \le 0\)</span>.</p></li>
<li><p>Finally, a symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is <strong>indefinite</strong>, if it is neither positive semidefinite or negative semidefinite. In other words, if there exists <span class="math inline">\(\bar{x}, \bar{x}&#39;, \in \mathbb{R}^{n}\)</span> such taht <span class="math inline">\(\bar{x}^TA\bar{x} &gt; 0\)</span> and <span class="math inline">\(\bar{x&#39;}^TA\bar{x}&#39; &gt; 0\)</span></p></li>
</ul>

<div class="rmdnote">
<p>Note that when talking about <span class="math inline">\(A\)</span> being PD, PSD, ND, NSD or indefinite, <span class="math inline">\(A\)</span> is always assumed to be <strong>symmetric</strong>.</p>
Also, if <span class="math inline">\(A\)</span> is positive definite, then <span class="math inline">\(−A\)</span> is negative definite and viceversa. Likewise, if <span class="math inline">\(A\)</span> is positive semidefinite then <span class="math inline">\(−A\)</span> is negative semidefinite and vice versa. If <span class="math inline">\(A\)</span> is indefinite, then so is <span class="math inline">\(−A\)</span>.
</div>

<p>From theorem <a href="eigenthings-and-quadratic-forms.html#thm:principal-axes">4.6</a>, we know that the sign of eigenvalues are closely related to classifications of symmetric matrices here. Take positive definite matrices for example, the following statements of <span class="math inline">\(A\)</span> are equivalent:</p>
<ul>
<li><p>For any <span class="math inline">\(\bar{x} \in \mathbb{R}^n, \; \bar{x}^TA\bar{x} &gt; 0\)</span></p></li>
<li><p>Let <span class="math inline">\(\lambda_i, \; i = 1, ..., n\)</span> be <span class="math inline">\(A\)</span>’s eigenvalues, <span class="math inline">\(\lambda_i &gt; 0\)</span></p></li>
<li><p>All leading determinants of <span class="math inline">\(A &gt; 0\)</span></p></li>
<li><p>All pivots are <span class="math inline">\(&gt; 0\)</span></p></li>
</ul>
<p>Classification of <span class="math inline">\(A \in \mathbb{S}^{n}\)</span> by its eigenvalue can be applied in general.</p>
<hr>

<div class="theorem">
<p><span id="thm:unnamed-chunk-13" class="theorem"><strong>Theorem 4.7  (Quadratic forms and eigenvalues)  </strong></span>Let <span class="math inline">\(A \in \mathbb{S}^{n}\)</span>. Then the quadratic form <span class="math inline">\(\bar{x}^TA\bar{x}\)</span> and <span class="math inline">\(A\)</span> is:</p>
<ul>
<li><p>positive definite if and only if the eigenvalues of <span class="math inline">\(A\)</span> are all positive</p></li>
<li><p>negative definite if and only if the eigenvalues of <span class="math inline">\(A\)</span> are all negative</p></li>
<li><p>indefinite if and only if <span class="math inline">\(A\)</span> has both positive and negative eigenvalues</p>
</div></li>
</ul>
<hr>

<div class="corollary">
<span id="cor:unnamed-chunk-14" class="corollary"><strong>Corollary 4.2  </strong></span>Given positive definite matrices <span class="math inline">\(A, B \in \mathbb{S}^n\)</span> and <span class="math inline">\(\alpha \in \mathbb{R}\)</span>, the following results <strong>remain to be positive definite</strong>.
</div>

<ul>
<li><p>Scalar multiplication of PD matrices <span class="math inline">\(\alpha A\)</span> are PD matrices</p></li>
<li><p>The sum of PD matrices <span class="math inline">\(A +B\)</span> are PD matrices</p></li>
<li><p>If a PD matrix is invertible, its inverse <span class="math inline">\(A^{-1}\)</span> is also PD.</p></li>
<li><p>Similar matrix of a PD matrix is PD.</p></li>
</ul>
<hr>

<div class="corollary">
<span id="cor:ata-pd" class="corollary"><strong>Corollary 4.3  </strong></span>Given any matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> are always positive semidefinite matrices
</div>

<p><strong>PROOF</strong></p>
<p>By definition, <span class="math inline">\(A^TA\)</span> is a positive semidefinite matrix if for any <span class="math inline">\(\bar{x} \in \mathbb{R}^n\)</span>, the quadratic form <span class="math inline">\(\bar{x}^T(A^TA)\bar{x} \ge 0\)</span>.</p>
<p><span class="math display">\[
\begin{split}
\bar{x}^T(A^TA)\bar{x} &amp;= (\bar{x}^TA^T)(A\bar{x}) \\
&amp;= (A\bar{x})^T(A\bar{x}) \\
&amp;= \|A\bar{x}\|^2
\end{split}
\]</span>
It turns out that the result is the square of the 2-norm of <span class="math inline">\(A\bar{x}\)</span> (nonnegative). This also tells <span class="math inline">\(A^TA\)</span> is positive definite when <span class="math inline">\(\bar{x} \not\subseteq \mathcal{N}(A)\)</span></p>
<p>Similarly, the quadratic form for <span class="math inline">\(AA^T\)</span> can be refactored in to the 2-norm of <span class="math inline">\(A^T\bar{x}\)</span>.</p>
<hr>
<hr>

<div class="corollary">
<span id="cor:ridge-invertible" class="corollary"><strong>Corollary 4.4  </strong></span><span class="math inline">\(A^TA +\lambda I\)</span> and <span class="math inline">\(AA^T + \lambda I\)</span> are always positive definite and invertible for <span class="math inline">\(\lambda &gt; 0\)</span>
</div>

<p><strong>PROOF</strong></p>
<p>From the previous corollary <a href="eigenthings-and-quadratic-forms.html#cor:ata-pd">4.3</a> we know that <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> are positive semidefinite, and that they have the same nonzero eigenvalues from corollary <a href="eigenthings-and-quadratic-forms.html#cor:same-nonzero">4.1</a>. According to Section <a href="eigenthings-and-quadratic-forms.html#additional-properties-of-eigenvalues-and-eigenvectors">4.1.1</a>, eigenvalues for <span class="math inline">\(P(A)\)</span> are <span class="math inline">\(P(\lambda)\)</span> for polynomial function <span class="math inline">\(P\)</span>. Therefore, <span class="math inline">\(A^TA +\lambda I\)</span> and <span class="math inline">\(AA^T + \lambda I\)</span> share a positive set of <span class="math inline">\(n\)</span> eigenvalues <span class="math inline">\(\lambda_1 + \lambda, ..., \lambda_r + \lambda, \lambda, ..., \lambda\)</span>, so they are PD and invertible.</p>
</div>
</div>
<div id="cholesky-factorization" class="section level2">
<h2><span class="header-section-number">4.5</span> Cholesky factorization</h2>

<div class="lemma">
<span id="lem:unnamed-chunk-15" class="lemma"><strong>Lemma 4.1  </strong></span>A symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is positive semidefinite if and only if it can be expressed in the gram matrix form <span class="math inline">\(B^TB\)</span> of some matrix <span class="math inline">\(B\)</span>.
</div>

<p><br></p>
<p>The previous corollary <a href="eigenthings-and-quadratic-forms.html#cor:ata-pd">4.3</a> shows that if <span class="math inline">\(A = B^TB\)</span> then it is positive definite. Conversely, if <span class="math inline">\(A\)</span> is PSD (or PD), we have <span class="math inline">\(A = Q \Lambda Q^T\)</span> where <span class="math inline">\(\Lambda\)</span>’s diagonal entries are all nonnegative. Then we can set <span class="math inline">\(\Lambda ^{1/2} = \Sigma\)</span> and <span class="math inline">\(B = (Q\Sigma)^T\)</span>. Then <span class="math inline">\(A = Q\Sigma^2Q^T = (Q\Sigma)(Q\Sigma)^T = B^TB\)</span>.</p>
<p>Note that we could also have stated this lemma using <span class="math inline">\(BB^T\)</span> instead of <span class="math inline">\(B^TB\)</span>, and the proof is similar.</p>
<p>This lemma is inspiring in that for every PD matrix <span class="math inline">\(A \in \mathbb{R}^n\)</span>, there exists factorization <span class="math inline">\(A = BB^T\)</span>.</p>
<p>In fact, this factorization is not unique. We can use an orthogonal matrix <span class="math inline">\(P\)</span> to create an additional orthogonal factorization <span class="math inline">\(A = BB^T = B(PP^T)B^T = (BP)(BP)^T\)</span>.</p>
<p>Since the initial <span class="math inline">\(B = P\Lambda^{1/2}\)</span> is full rank, with an appropriate choice of <span class="math inline">\(P\)</span>, we can turn the <span class="math inline">\(PB\)</span> into an <strong>lower triangular matrix</strong> <span class="math inline">\(L\)</span> such that <span class="math inline">\(A = LL^T\)</span>.</p>
<p>The uniqueness of this factorization with lower triangular matrix <span class="math inline">\(L\)</span> can be proved with induction. The decomposition of PD matrices into the product of an lower triangular matrix and its transpose is called the <strong>Cholesky factorization</strong>, in this factorization</p>
<p><span class="math display">\[
A = LL^T
\]</span></p>
<p>To make it more clear</p>
<p><span class="math display">\[
\begin{bmatrix}
a_{11} &amp; \cdots &amp; a_{1n} \\
\vdots &amp; \ddots &amp; \vdots \\
a_{n1} &amp; \cdots &amp; a_{nn}
\end{bmatrix}
= 
\begin{bmatrix}
l_{11} &amp; \cdots &amp; 0\\
\vdots &amp; \ddots &amp; \vdots \\
l_{n1} &amp; \cdots &amp; l_{nn}
\end{bmatrix}
\begin{bmatrix}
l_{11} &amp; \cdots &amp; l_{n1} \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; l_{nn}
\end{bmatrix}
\]</span></p>
<p>Since <span class="math inline">\(L\)</span> is lower triangular, we can solve <span class="math inline">\(L\)</span> from <span class="math inline">\(A = LL^T\)</span> with a system of equations that can be easily solved using back-substitution. For example, <span class="math inline">\(l_{11} = \sqrt{a_{11}}\)</span>, and <span class="math inline">\(a_{i1} / l_{11}\)</span>.</p>
<p>The Cholesky factorization is a special case of LU decomposition</p>
</div>
<div id="rayleigh-quotients" class="section level2">
<h2><span class="header-section-number">4.6</span> Rayleigh quotients</h2>
<p>Let <span class="math inline">\(A \in \mathbb{S}^n\)</span> and <span class="math inline">\(\bar{x} \in \mathbb{R}^n\)</span>, <strong>Rayleigh quotient</strong> is defined as</p>
<p><span class="math display">\[
R_{A}(\bar{x}) = \frac{\bar{x}^TA\bar{x}}{\bar{x}^T\bar{x}}
\]</span>
The Rayleigh quotient has some nice properties:</p>
<ul>
<li><p>scale invariance: for any vector <span class="math inline">\(\bar{x} \not= 0\)</span> and any scalar <span class="math inline">\(\alpha \not= 0\)</span>, <span class="math inline">\(R_{A}(\bar{x}) = R_{A}(\alpha\bar{x})\)</span></p></li>
<li><p>If <span class="math inline">\(\bar{x}\)</span> is a eigenvector of <span class="math inline">\(A\)</span> with eigenvalue <span class="math inline">\(\lambda\)</span>, then <span class="math inline">\(R_{A}(\bar{x}) = \lambda\)</span></p></li>
<li><p>The Rayleigh quotient is bounded by the largest and smallest eigenvalue of <span class="math inline">\(A\)</span>, i.e. </p></li>
</ul>
<p><span class="math display">\[
\lambda_{\text{min}}(A) \le R_{A}(\bar{x}) \le \lambda_{\text{max}}(A)
\]</span></p>
<p><strong>PROOF</strong></p>
<p>Since the Rayleigh quotient does not depend on the 2-norm of vector <span class="math inline">\(\bar{x}\)</span>, we may assume a unit vector <span class="math inline">\(\bar{x}^T\bar{x} = 1\)</span>, and Rayleigh quotient simplifies to the quadratic form <span class="math inline">\(\bar{x}^TA\bar{x}\)</span>.</p>
<p>Next, orthogonally diagonalize <span class="math inline">\(A\)</span> as <span class="math inline">\(Q \Lambda Q\)</span>, we know that when <span class="math inline">\(\bar{x} = Q \bar{y}\)</span>:</p>
<p><span class="math display">\[
\bar{x}^TA\bar{x} = \bar{y}^T \Lambda \bar{y} \tag{1}
\]</span>
Also</p>
<p><span class="math display">\[
1= \bar{x}^T\bar{x} = (Q\bar{y})^T Q\bar{y} = \bar{y}^TQ^TQ\bar{y} = \bar{y}^T\bar{y} \tag{2}
\]</span></p>
<p>Expand <span class="math inline">\(\bar{y}^T \Lambda \bar{y}\)</span> in (1) we get</p>
<p><span class="math display">\[
\bar{x}^TA\bar{x} = \lambda_1y_1^2 + \lambda_2y_2^2 + \cdots + \lambda_ny_n^2 \tag{3}
\]</span>
where <span class="math inline">\(\{\lambda_1, ..., \lambda_n\}\)</span> are diagonal entries of <span class="math inline">\(\Lambda\)</span> and eigenvalues of <span class="math inline">\(A\)</span>. Let us suppose that the set <span class="math inline">\(\{\lambda_1, \lambda_2, \cdots, \lambda_n\}\)</span> has already been ordered descendingly, so that <span class="math inline">\(\lambda_1 &gt; \lambda_2 &gt; \cdots &gt; \lambda_n\)</span>.</p>
<p>We can obtain the inequality from (3) and the order of eigenvalues:</p>
<p><span class="math display">\[
\begin{split}
\bar{x}^TA\bar{x} &amp;= \lambda_1y_1^2 + \lambda_2y_2^2 + \cdots + \lambda_ny_n^2 \\
&amp;\le \lambda_1y_1^2 + \underbrace{\lambda_1y_2^2 + \cdots + \lambda_1y_n^2}_{\lambda_1 \text{ is the greatest eigenvalue}} \\ 
&amp;\le \lambda_1(\bar{y}^T\bar{y}) \\
&amp;= \lambda_1
\end{split}
\]</span>
The equation reach equality if and only if <span class="math inline">\([y_1, y_2, \cdots, y_n] = [1, 0, \cdots, 0]\)</span>. Since <span class="math inline">\(\bar{x} = Q\bar{y}\)</span>, we have</p>
<p><span class="math display">\[
\bar{x} = 
\begin{bmatrix}
\bar{q}_1 &amp; \bar{q}_2 &amp; \cdots &amp; \bar{q}_n
\end{bmatrix}
\begin{bmatrix}
1 \\
0 \\
\vdots \\
0
\end{bmatrix}
= \bar{q}_1
\]</span>
Similarly, the minimum of the Rayleigh quotient will be <span class="math inline">\(\lambda_n\)</span>, with <span class="math inline">\(\bar{x} = \bar{q}_n\)</span>.</p>
<hr>
<p>From the optimization perspective, the bound of Rayleigh quotient amounts to a constrained optimization problem</p>
<p><span class="math display">\[
\begin{aligned}
\text{objective function} &amp;:  \bar{x}^TA\bar{x}\\
\text{subject to}&amp;: \bar{x}^T\bar{x} = 1 
\end{aligned}
\]</span>
The maximum and minimum of the objective function are <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_n\)</span>, with <span class="math inline">\(\bar{x}\)</span> being <span class="math inline">\(\bar{q}_1\)</span> and <span class="math inline">\(\bar{q}_n\)</span> respectively.</p>
<p>If we add more constraints, for example, that <span class="math inline">\(\bar{x}\)</span> should be orthogonal to <span class="math inline">\(\bar{q}_1\)</span>, then <span class="math inline">\(\bar{x}^TA\bar{x}\)</span> has maximum <span class="math inline">\(\lambda_2\)</span> attained at <span class="math inline">\(\bar{x} = \lambda_2\)</span></p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-16" class="theorem"><strong>Theorem 4.8  </strong></span>Let <span class="math inline">\(A \in \mathbb{S}^n\)</span> with orthogonal diagonalization <span class="math inline">\(A = Q\Lambda Q^T\)</span>, where the entries on the diagonal of <span class="math inline">\(\Lambda\)</span> are arranged so that <span class="math inline">\(\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_n\)</span>. Then for <span class="math inline">\(k = 2, ...\)</span>, the maximum of value of <span class="math inline">\(\bar{x}^T A \bar{x}\)</span> subject to constraints</p>
<span class="math display">\[
\bar{x}^T\bar{x} =  1, \;\; \bar{x}^T\bar{q}_1 = 0, \;\; \dots \;\;, \bar{x}^T\bar{q}_{k-1} = 0
\]</span>
is the eigenvalue <span class="math inline">\(\lambda_k\)</span>, and this maximum is attained at <span class="math inline">\(\bar{x} = \bar{q}_k\)</span>
</div>

<p><strong>PROOF</strong></p>
<p>From <span class="math inline">\(\bar{x} = P\bar{y}\)</span> we know that</p>
<p><span class="math display">\[
\bar{x} = y_1\bar{q}_1 + \cdots + + y_{k-1}\bar{q}_{k-1} + y_k\bar{q}_k + \cdots +  y_{n}\bar{q}_n 
\]</span>
Left multiply by <span class="math inline">\(\bar{q}_1^T\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\bar{q}_1^T\bar{x} &amp;= y_1\bar{q}_1^T\bar{q}_1 + \cdots + + y_{k-1}\bar{q}_1^T\bar{q}_{k-1} + y_k\bar{q}_1^T\bar{q}_k + \cdots +  y_{n}\bar{q}_1^T\bar{q}_n  \\
&amp;= y_1\bar{q}_1^T\bar{q}_1 \\
&amp;= y_1
\end{aligned} 
\]</span>
Since <span class="math inline">\(\bar{q}_1^T\bar{x} = \bar{x}^T\bar{q}_1 = 0\)</span>, we have <span class="math inline">\(y_1 = 0\)</span>. Similarly, <span class="math inline">\(y_2 = \cdots = y_{k-1} = 0\)</span>, and <span class="math inline">\(\bar{y}\)</span> becomes <span class="math inline">\([0 \;\; \cdots \;\; 0 \;\; y_{k} \;\; \cdots \;\; y_n]\)</span>. And the inequality now becomes:</p>
<p><span class="math display">\[
\begin{split}
\bar{x}^TA\bar{x} &amp;= \lambda_1y_1^2 + \lambda_2y_2^2 + \cdots + \lambda_ny_n^2 \\
&amp;= \lambda_ky_k^2 + \cdots + \lambda_ny_n^ 2 \\
&amp;\le \lambda_ky_k^2 + \cdots + \lambda_ky_n^2 \\ 
&amp;\le \lambda_k(\bar{y}^T\bar{y}) \\
&amp;= \lambda_k
\end{split}
\]</span></p>
<p>It’s easy to see that <span class="math inline">\(\bar{x}^TA\bar{x}\)</span> gets its maximum <span class="math inline">\(\lambda_k\)</span> when <span class="math inline">\(y_k = 0\)</span> and other weights being zero. So the solution <span class="math inline">\(\bar{x}\)</span> can be solved as</p>
<p><span class="math display">\[
\begin{split}
\bar{x} &amp;= 
\begin{bmatrix}
\bar{q}_1 &amp;  \cdots &amp; \bar{q}_k &amp;   \bar{q}_{k+1} &amp; \cdots &amp;\bar{q}_n
\end{bmatrix}
\begin{bmatrix}
0 \\
\vdots \\
\underbrace{1}_{k\text{th weight}} \\
0 \\
\vdots \\
0
\end{bmatrix} \\
&amp;= \bar{q}_k
\end{split}
\]</span></p>
</div>
<div id="svd" class="section level2">
<h2><span class="header-section-number">4.7</span> SVD</h2>
<div id="singular-values-of-m-x-n-matrix" class="section level3">
<h3><span class="header-section-number">4.7.1</span> Singular values of m x n matrix</h3>
<p>The singular value decomposition illustrates a way of decomposing <em>any</em> matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> into the form <span class="math inline">\(U \Sigma V^T\)</span>, where <span class="math inline">\(U = [\bar{u}_1 \;\; \cdots \;\; \bar{u}_n]\)</span> and <span class="math inline">\(V = [\bar{v}_1 \;\; \cdots \;\; \bar{v}_n]\)</span> are both orthogonal matrices, and <span class="math inline">\(\Sigma\)</span> a diagonal matrix with entries being the square root of the eigenvalues of <span class="math inline">\(A^TA\)</span> (perhaps plus some zeros).</p>
<p>Before proceeding to the theorem, let’s explore the motivating idea behind SVD. For (square) diagonalizable matrix <span class="math inline">\(A \in \mathbb{S}^{n}\)</span>, the absolute value of the eigenvalues measure the amounts that <span class="math inline">\(A\)</span> stretches or shrinks eigenvectors, consider the ratio between the length of <span class="math inline">\(\bar{x}\)</span> before and after left multiplied by <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
\frac{\|A\bar{x}\|}{\|\bar{x}\|} 
= \frac{\|\lambda\bar{x}\|}{\|\bar{x}\|}
= \frac{\lambda\|\bar{x}\|}{\|\bar{x}\|} = \lambda
\]</span>
If <span class="math inline">\(\lambda_1\)</span> is the greatest eigenvalue, then the corresponding eigenvector <span class="math inline">\(\bar{v}_1\)</span> identifies the direction in which <span class="math inline">\(A\)</span>’s stretching effect is greatest.</p>
<p>So, the question is, can we identify a similar ratio and direction for <em>rectangular</em> matrices <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, even though they does not have eigenvalues and eigenvectors?</p>
<p>The answer is yes. Note that maximize <span class="math inline">\(\frac{\|A\bar{x}\|}{\|\bar{x}\|}\)</span> (now <span class="math inline">\(\bar{x}\)</span> is any vector <span class="math inline">\(\in \mathbb{R}^n\)</span>) is equivalent to maximize <span class="math inline">\(\frac{\|A\bar{x}\|^2}{\|\bar{x}\|^2}\)</span></p>
<p><span class="math display">\[
\begin{split}
\frac{\|A\bar{x}\|^2}{\|\bar{x}\|^2} &amp;= \frac{(A\bar{x})^T(A\bar{x})}{\bar{x}^T\bar{x}} \\
&amp;= \frac{\bar{x}^T(A^TA)\bar{x}}{\bar{x}^T\bar{x}}
\end{split}
\]</span>
Since <span class="math inline">\(A^TA\)</span> is <strong>symmetric</strong>, this is the form of a Rayleigh quotients <a href="eigenthings-and-quadratic-forms.html#rayleigh-quotients">4.6</a>! We know that the largest possible value is of this quotient <span class="math inline">\(\lambda_1\)</span>, the greatest eigenvalue of <span class="math inline">\(A^TA\)</span>, with <span class="math inline">\(\bar{x} = \bar{v}_1\)</span>, among the <strong>orthonormal</strong> set <span class="math inline">\(\{\bar{v}_1, \cdots, \bar{v}_n\}\)</span>. Note that here <span class="math inline">\(V = [\bar{v}_1 \;\; \cdots \;\; \bar{v}_n]\)</span> is already a orthogonal matrix, previously denoted by <span class="math inline">\(Q\)</span>.</p>
<p>To sum up, the greatest possible stretching ratio of <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> on a vector <span class="math inline">\(\bar{x} \in \mathbb{R}^n\)</span> is <span class="math inline">\(\sqrt{\lambda_1}\)</span>. Generally, let <span class="math inline">\(\{\bar{v}_1, \cdots, \bar{v}_n\}\)</span> be a orthonormal basis for <span class="math inline">\(\mathbb{R}^n\)</span> consisting of eigenvectors of <span class="math inline">\(A^TA\)</span>, and <span class="math inline">\(\lambda_1, ..., \lambda_n\)</span> be the eigenvalues of <span class="math inline">\(A^TA\)</span>, for <span class="math inline">\(i = 1, \cdots, n\)</span></p>
<p><span class="math display">\[
\|A\bar{v}_i\| ^ 2 = \bar{v}_i^T(A^TA)\bar{v}_i = \lambda_i\bar{v}_i^T\bar{v}_i = \lambda_i
\]</span></p>
<p>From corollary <a href="eigenthings-and-quadratic-forms.html#cor:ata-pd">4.3</a>, we know that <span class="math inline">\(A^TA\)</span> are positive semidefinite matrices. Thus, <span class="math inline">\(\lambda_i \ge 0, \, i = 1, ..., n\)</span>, and we can find their square root <span class="math inline">\(\sigma_i = \sqrt{\lambda_i}\)</span>.</p>
<p><br></p>

<div class="definition">
<span id="def:unnamed-chunk-17" class="definition"><strong>Definition 4.7  (Singular values)  </strong></span>The <strong>singular values</strong> of <span class="math inline">\(A\)</span> are the square roots of the eigenvalues of <span class="math inline">\(A^TA\)</span>, denoted by <span class="math inline">\(\sigma_1, ..., \sigma_n\)</span>. That is, <span class="math inline">\(\sigma_i = \sqrt{\lambda_i}\)</span>, and they are often arranged in descending order so that <span class="math inline">\(\lambda_1 \ge \cdots \ge \lambda_n\)</span>. Geometrically, singular values of <span class="math inline">\(A\)</span> are the length of the vectors <span class="math inline">\(A\bar{v}_1, ..., A\bar{v}_n\)</span>, where <span class="math inline">\(\{\bar{v}_1, ..., \bar{v}_n\}\)</span> is the <em>orthonormal</em> basis of <span class="math inline">\(A^TA\)</span>’s eigenspace.
</div>

<p><br></p>

<div class="theorem">
<span id="thm:svd-rank" class="theorem"><strong>Theorem 4.9  </strong></span>Proceeding from previous definitons of singular values, and suppose <span class="math inline">\(A\)</span> has at least one nonzero singular values. Then <span class="math inline">\(\{A\bar{v}_1, ..., A\bar{v}_r\}\)</span> is an orthogonal basis for <span class="math inline">\(\mathcal{R}(A)\)</span>, and <span class="math inline">\(\text{rank} \;A = r\)</span>
</div>

<p><strong>PROOF</strong></p>
<p>First, let’s examine that <span class="math inline">\(\{A\bar{v}_1, ..., A\bar{v}_r\}\)</span> is a orthogonal basis: any pair of two distinct vectors <span class="math inline">\(A\bar{v}_i, A\bar{v}_j, \; i,j = 1, ..., r\)</span> are orthogonal to each other</p>
<p><span class="math display">\[
\begin{split}
(A\bar{v}_i)^T(A\bar{v}_j) &amp;=  \bar{v}_i^TA^TA\bar{v}_j \\
&amp;= \bar{v}_i^T(\lambda_j\bar{v}_j) \\
&amp;= 0
\end{split}
\]</span></p>
<p>Next, we show that any vector in <span class="math inline">\(\mathcal{R}(A)\)</span> is a linear a combination of <span class="math inline">\(\{A\bar{v}_1, ..., A\bar{v}_r\}\)</span>. Note that <span class="math inline">\(\{\bar{v}_1, ..., \bar{v}_n\}\)</span> is a orthonormal basis of <span class="math inline">\(A^TA\)</span>’s eigenspace <span class="math inline">\(\mathbb{R}^n\)</span>. Therefore, for any vector <span class="math inline">\(\bar{y} = A\bar{x}\)</span> in <span class="math inline">\(\mathcal{R}(A)\)</span> , there exists <span class="math inline">\(\bar{x} = c_1\bar{v}_1 + \cdots + c_n\bar{v}_n\)</span>, thus</p>
<p><span class="math display">\[
\begin{split}
\bar{y} &amp;= A\bar{x} = A(c_1\bar{v}_1 + \cdots + c_n\bar{v}_n) \\
&amp;= c_1 A \bar{v}_1 + \cdots + c_r A \bar{v}_r + c_{r+1} A \bar{v}_{r+1} + \cdots + c_n A \bar{v}_n 
\end{split}
\tag{1}
\]</span>
Since <span class="math inline">\(\lambda_{r+1} = \cdots = \lambda_{n} = 0\)</span>, <span class="math inline">\(A\bar{v}_{r+1}, ..., A\bar{v}_{n}\)</span> have length <span class="math inline">\(0\)</span>: they are zero vectors. And (1) is reduced to</p>
<p><span class="math display">\[
\bar{y} = c_1 A \bar{v}_1 + \cdots + c_r A\bar{v}_r
\]</span></p>
<p>Thus any <span class="math inline">\(\bar{y} \in \mathcal{R}(A)\)</span> is in Span<span class="math inline">\(\{A\bar{v}_1, ..., A\bar{v}_r\}\)</span>, and <span class="math inline">\(\{A\bar{v}_1, ..., A\bar{v}_r\}\)</span> is an orthogonal basis for <span class="math inline">\(\mathcal{R}(A)\)</span>. This also shows that the column rank of <span class="math inline">\(A\)</span> is equal to its number of nonzero singular values.</p>
</div>
<div id="svd-theorem" class="section level3">
<h3><span class="header-section-number">4.7.2</span> The singular value decomposition</h3>
<p>Let’s begin SVD by the <span class="math inline">\(m \times n\)</span> diagonal matrix <span class="math inline">\(\Sigma\)</span> of the form</p>
<p><span class="math display">\[
\Sigma = \begin{bmatrix}
\sigma_1 \\
&amp; \ddots &amp;  \\ 
&amp; &amp; \sigma_r \\
&amp; &amp; &amp; 0 \\
&amp; &amp; &amp; &amp; \ddots \\
&amp; &amp; &amp; &amp; &amp; 0 \\
\end{bmatrix}
\tag{1}
\]</span></p>
<p>There are <span class="math inline">\(r\)</span> nonzero entries on the diagonal, being <span class="math inline">\(A\)</span>’s nonzero singular values, and the left positions are filled by <span class="math inline">\(0\)</span> to form a <span class="math inline">\(m \times n\)</span> matrix. If <span class="math inline">\(r\)</span> equals <span class="math inline">\(m\)</span> or <span class="math inline">\(n\)</span> or both, some or all of the zero blocks do not appear.</p>

<div class="theorem">
<p><span id="thm:SVD" class="theorem"><strong>Theorem 4.10  (The Singular Value Decomposition)  </strong></span>Let <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> with rank <span class="math inline">\(r\)</span>. There exists an diagonal matrix <span class="math inline">\(\mathbb{\Sigma} \in \mathbb{R}^{m \times n}\)</span> as in (1) for which the first <span class="math inline">\(r\ \times r\)</span> block is a diagonal matrix with the first <span class="math inline">\(r\)</span> singular values of <span class="math inline">\(A\)</span> on its diagonal, and there exist <span class="math inline">\(U \in \mathbb{R}^{m \times m}\)</span> and <span class="math inline">\(V \in \mathbb{R}^{n \times n}\)</span> such that</p>
<span class="math display">\[
A = U \Sigma V^T
\]</span>
</div>

<p><strong>PROOF</strong></p>
<p>Since <span class="math inline">\(A\)</span> has <span class="math inline">\(r\)</span> nonzero singular values which measure the length of <span class="math inline">\(A\bar{v}_i, \; i = 1, ...n\)</span>, there exists orthogonal basis <span class="math inline">\(\{A\bar{v}_1, ..., A\bar{v}_r\}\)</span> for <span class="math inline">\(\mathcal{R}(A)\)</span>, we can further normalize the set to produce the <em>orthonormal</em> set <span class="math inline">\(\bar{u}_1, ..., \bar{u}_r\)</span>:</p>
<p><span class="math display">\[
\bar{u}_i = \frac{A\bar{v}_i}{\sigma_i}, \;\; i = 1, ..., r
\]</span>
Now we can extend <span class="math inline">\(\{\bar{u}_1, ..., \bar{u}_r\}\)</span> to an orthonormal basis <span class="math inline">\(\{\bar{u}_1, ..., \bar{u}_m\}\)</span> of <span class="math inline">\(\mathbb{R}^m\)</span>, and let</p>
<p><span class="math display">\[
U = [\bar{u}_1 \;\; \cdots \;\; \bar{u}_m], \quad V = [\bar{v}_1 \;\; \cdots \;\; \bar{v}_n]
\]</span></p>
<p>and <span class="math inline">\(\Sigma\)</span> be as be as in (1) above. Write out</p>
<p><span class="math display">\[
\begin{split}
U\Sigma &amp;= [\bar{u}_1 \;\; \cdots \;\; \bar{u}_r \;\; \cdots \;\; \bar{u}_m]_{m \times m}
\begin{bmatrix}
\sigma_1 \\
&amp; \ddots &amp;  \\ 
&amp; &amp; \sigma_r \\
&amp; &amp; &amp; 0 \\
&amp; &amp; &amp; &amp; \ddots \\
&amp; &amp; &amp; &amp; &amp; 0 \\
\end{bmatrix}_{m\times n} \\
&amp;= [\sigma_1\bar{u}_1 \;\; \cdots \;\;   \sigma_r\bar{u}_r \;\; \bar{0} \;\;  \cdots \;\;  \bar{0}] \\
&amp; = [A\bar{v}_1 \;\; \cdots \;\; A\bar{v}_r \;\; A\bar{v}_{r+1} \;\; \cdots \;\; A \bar{v}_n] \\
&amp;= A_{m \times n}V_{n \times n}
\end{split}
\]</span></p>
<p>And because <span class="math inline">\(V\)</span> is orthogonal</p>
<p><span class="math display">\[
A = U \Sigma V^{-1} =  U \Sigma V^{T}
\]</span>
<span class="math inline">\(\bar{u}_i\)</span> and <span class="math inline">\(\bar{v}_i\)</span> are called <em>left eigenvector</em> and right eigenvector of <span class="math inline">\(A\)</span> respectively.</p>
<p>It’s easy to verify that the spectral decomposition <a href="eigenthings-and-quadratic-forms.html#spectral-decomposition">4.3.1</a> is a special case of SVD when <span class="math inline">\(A \in \mathbb{R}^{n}, \;\; m = n\)</span>. In that case, <span class="math inline">\(\Sigma\)</span> is a square matrix and <span class="math inline">\(U\)</span> is equal to <span class="math inline">\(V\)</span>.</p>
<p>When <span class="math inline">\(\Sigma\)</span> contains rows or columns of zeros (i.e, <span class="math inline">\(r &lt; \min(m, n)\)</span>), we can write SVD in a more compact form. Divide <span class="math inline">\(U, \Sigma, V\)</span> into submatrices</p>
<p><span class="math display">\[
U = [U_r \;\; U_{m-r}], \quad \text{where } U_r = [\bar{u}_1 \;\; \cdots \;\; \bar{u}_r] \\
V = [V_r \;\; V_{m-r}], \quad \text{where } V_r = [\bar{v}_1 \;\; \cdots \;\; \bar{v}_r] \\
\Sigma = 
\begin{bmatrix}
D &amp; 0 \\
0 &amp; 0
\end{bmatrix}
\quad \text{where } D = 
\begin{bmatrix}
\lambda_1 \\ 
 &amp; \ddots \\
 &amp; &amp; \lambda_r
\end{bmatrix}
\]</span>
The partitioned matrix multiplication shows that</p>
<p><span class="math display">\[
A = [U_r \;\; U_{m-r}]
\begin{bmatrix}
D &amp; 0 \\
0 &amp; 0
\end{bmatrix}
\begin{bmatrix}
V_r^T \\
V_{n-r}^T
\end{bmatrix}
= U_rDV_{r}^T
\]</span>
This more compact form is called a reduced <strong>singular value decomposition</strong>.</p>
<p>Another way to write this is</p>
<p><span class="math display">\[
A = \sum_{i=1}^{t}{\sigma_i}\bar{u}_i\bar{v}_i
\]</span></p>
<hr>
<p>Right multiply the non-compact form <span class="math inline">\(A = U\Sigma V^T\)</span> by <span class="math inline">\(A^T\)</span> , we get the spectral decomposition of symmetric matrix <span class="math inline">\(AA^T\)</span>.</p>
<p><span class="math display">\[
AA^T = (U \Sigma V^T)(U \Sigma V^T)^T = U \Sigma \Sigma^T VV^TU^T = U (\Sigma\Sigma^T) U^T       \tag{1}
\]</span></p>
<p>Therefore, <span class="math inline">\([\bar{u}_1 \;\; \cdots \;\; \bar{u}_n]\)</span> are revealed as the orthonormal basis for <span class="math inline">\(AA^T\)</span>’s eigenspace, as <span class="math inline">\([\bar{v}_1 \;\; \cdots \;\; \bar{v}_n]\)</span> are for <span class="math inline">\(A^TA\)</span>.</p>
<p>Formula (1) echoes the fact that <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> have the same set of nonzero eigenvalues, because <span class="math inline">\(\Sigma\Sigma^T\)</span> produces nonzero set <span class="math inline">\(\lambda_1, ..., \lambda_r\)</span>.</p>
<p>In fact, if were to ask for a direction in which <span class="math inline">\(A^T\)</span> has its greatest stretching effect instead of <span class="math inline">\(A\)</span>, we would still result in the equivalent decomposition <span class="math inline">\(A^T = V\Sigma U^T\)</span>, with <span class="math inline">\(\bar{v}_i = \frac{A\bar{u}_i}{\sigma_i}\)</span>.</p>
<p>It’s also easy to test that <span class="math inline">\(\{A\bar{u}_1, ..., A\bar{u}_r\}\)</span> produces an orthogonal basis for <span class="math inline">\(\mathcal{R}(A^T)\)</span> or <span class="math inline">\(\mathcal{R}(A)\)</span>. The process is analogous to theorem <a href="eigenthings-and-quadratic-forms.html#thm:svd-rank">4.9</a> where <span class="math inline">\(\{A\bar{v}_1, ..., A\bar{v}_r\}\)</span> are shown to span <span class="math inline">\(\mathcal{R}(A)\)</span>.</p>
<p>For any vector <span class="math inline">\(\bar{y}\)</span> in <span class="math inline">\(\mathcal{R}(A)\)</span>, we have</p>
<p><span class="math display">\[
\begin{align*}
\bar{y} &amp;= A^T\bar{x} \\
&amp;= A^T(c_1\bar{u}_1 + \cdots + c_1\bar{u}_n) \\
&amp;= c_1A\bar{u}_1 + \cdots + c_rA\bar{u}_r + \bar{0} + \cdots + \bar{0} &amp;&amp; (\text{because }A\bar{u}_i = \sigma_i\bar{v}_i)\\
&amp;= c_1A\bar{u}_1 + \cdots + c_rA\bar{u}_r 
\end{align*}
\]</span></p>
<p>Thus, SVD can be thought of an connection between two spectral decomposition</p>
<p><span class="math display">\[
A^TA = V (\Sigma^T\Sigma)V^T  \\
AA^T = U (\Sigma\Sigma^T) U^T
\]</span></p>
<p>This shed light on the relationship between SVD and the fundamental theorem of linear algebra <a href="vector-spaces.html#thm:fundamental-theorem">2.1</a></p>
<table>
<thead>
<tr class="header">
<th align="center">Subspace</th>
<th align="center">Columns</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\mathcal{R}(A)\)</span></td>
<td align="center">the first <span class="math inline">\(r\)</span> columns of <span class="math inline">\(U\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\mathcal{R}(A^T)\)</span></td>
<td align="center">the first <span class="math inline">\(r\)</span> columns of <span class="math inline">\(V\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\mathcal{N}(A)\)</span></td>
<td align="center">the last <span class="math inline">\(n - r\)</span> columns of <span class="math inline">\(V\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\mathcal{N}(A^T)\)</span></td>
<td align="center">the last <span class="math inline">\(m - r\)</span> columns of <span class="math inline">\(U\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="matrix-norms" class="section level2">
<h2><span class="header-section-number">4.8</span> Matrix norms</h2>
<p>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be matrices conformable for the operations below, a matrix norm should at first satisfy 3 axioms of norm:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\|A\| \ge 0\)</span> for all <span class="math inline">\(x \in X\)</span>, with equality if and only if all elements of <span class="math inline">\(A\)</span> is zero (nonnegative)<br />
</li>
<li><span class="math inline">\(\|\alpha A\| = |\alpha|\,\|A\|\)</span> (homogeneous)<br />
</li>
<li><span class="math inline">\(\|A + B\| &lt; \|A\| + \|B\|\)</span> (triangular inequality)</li>
</ol>
<p>Additionally, in the case of square matrices (thus, m = n), some (but not all) matrix norms satisfy the following condition, which is related to the fact that matrices are more than just vectors</p>
<ol start="4" style="list-style-type: decimal">
<li><span class="math inline">\(\|AB\| &lt; \|A\|\,\|B\|\)</span> for <span class="math inline">\(A, B \in \mathbb{R}^{n \times n}\)</span></li>
</ol>
<p>A matrix norm that satisfies this additional property is called a <strong>submultiplicative</strong> norm.</p>
<p>There are 2 main categories of matrix norms.</p>
<ul>
<li>induced norms (defined in terms of vector norms)<br />
</li>
<li>entry-wise norms (treat <span class="math inline">\(A_{m \times n}\)</span> like a long vector with <span class="math inline">\(m \times n\)</span> elements)</li>
</ul>
<div id="induced-norms" class="section level3">
<h3><span class="header-section-number">4.8.1</span> Induced norms</h3>
<p>Induced norms define matrix norms in terms of vectors, also called <em>operator norm</em> since <span class="math inline">\(A\)</span> acts like an operator in this definition. Note that matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> maps a vector <span class="math inline">\(\bar{x} \not = 0\)</span> from <span class="math inline">\(\mathbb{R}^n\)</span> to <span class="math inline">\(\mathbb{R}^m\)</span>. In particular, if the p-norm is used for both <span class="math inline">\(\mathbb{R}^n\)</span> and <span class="math inline">\(\mathbb{R}^m\)</span>, then the induced norm is</p>
<p><span class="math display">\[
\|A\|_p = \max \frac{\|A\bar{x}\|_p}{\|\bar{x}\|_p}
\]</span>
The subscript <span class="math inline">\(p\)</span> can be misleading, because the appropriate name for this matrix norm may not be “p-norm”, but rather “induced norm when p-norm is used in both spaces”. The p-norm of a matrix meaning usually means entry-wise p-norms or the Scatten p-norms defined in subsequent sections.</p>
<p>In the special cases where <span class="math inline">\(p = 1, 2, ..., \infty\)</span>, <span class="math inline">\(\|A\|_p\)</span> is the maximum absolute column sums, largest singular value, and the maximum absolute row sums</p>
<p><span class="math display">\[
\begin{aligned}
\|A\|_1 &amp;= \max \sum_{i=1}^{m}{|A_{ij}|} \\
\|A\|_2 &amp;= \sigma_1 \\
\|A\|_{\infty} &amp;= \max \sum_{j=1}^{n}{|A_{ij}|}
\end{aligned}
\]</span>
For symmetric matrix A, we have</p>
<p><span class="math display">\[
\|A\|_1 = \|A\|_{\infty}
\]</span>
and</p>
<p><span class="math display">\[
\|A\|_2 = \lambda_1
\]</span>
The induced 2-norm are also called the <strong>spectral norm</strong>.</p>
<p>By definition, the following inequality holds for induced matrix norms</p>
<p><span class="math display">\[
\|A\bar{x}\|_P \le \|A\|_p\|\bar{x}\|_p
\]</span></p>
<hr>

<div class="proposition">
<p><span id="prp:unnamed-chunk-18" class="proposition"><strong>Proposition 4.2  </strong></span>Induced matrix norms satisfies the additional submultiplicative property in that</p>
<span class="math display">\[
\|AB\|_p \le \|A\|_p\|B\|_p
\]</span>
</div>

<p><strong>PROOF</strong></p>
<p>For any <span class="math inline">\(\bar{x} \in \mathbb{R}^n\)</span></p>
<p><span class="math display">\[
\|AB\bar{x}\|_p \le \|A\|_p\|B\bar{x}\|_p \le \|A\|_p\|B\|_p\|\bar{x}\|_p
\]</span>
si</p>
<p><span class="math display">\[
\|AB\|_p = \max \frac{\|A\bar{x}\|_p}{\|\bar{x}\|_p} \le \max \frac{\|A\|_p\|B\|_p\|\bar{x}\|_p}{\|\bar{x}\|_p} = \|A\|_p\|B\|_p
\]</span></p>
</div>
<div id="entry-wise-norm" class="section level3">
<h3><span class="header-section-number">4.8.2</span> Entry-wise norm</h3>
<p>Entry-wise norms treat an <span class="math inline">\(m \times n\)</span> matrix as a long vector of size <span class="math inline">\(m \times n\)</span>, denoted by <span class="math inline">\(\text{vec}(A)\)</span>. For example, using the p-norm for vectors, we get</p>
<p><span class="math display">\[
\|A\|_{p,p} = \|\text{vec}(A)\|_p = \Bigg(\sum_{j=1}^n\sum_{i=1}^{m}{|A_{ij}|}^p \Bigg)^{\frac{1}{p}}
\]</span></p>
<p>More generally, the p,q norm is defined by</p>
<p><span class="math display">\[
\|A\|_{p, q} = \Bigg (  \sum_{j=1}^n \Big (\sum_{i=1}^{n}{|A_{ij|}}^p \Big)^{\frac{q}{p}}    \Bigg)^{\frac{1}{q}}
\]</span>
Another important member norm of this norm family is the <strong>Frobenius norm</strong>, or the F-norm.</p>
<p><span class="math display">\[
\| A\|_F = \sqrt{\sum_{j=1}^n\sum_{j=1}^{m}{A_{ij}^2}} = \sqrt{\text{tr}(A^TA)} = \sqrt{\sum_{i=1}^{\min(m,n)}{\sigma_i^2}}  
\]</span>
where <span class="math inline">\(\sigma_i\)</span> is the nonzero singular value of <span class="math inline">\(A\)</span>.</p>

<div class="proposition">
<span id="prp:unnamed-chunk-19" class="proposition"><strong>Proposition 4.3  </strong></span>The F-norm is a submultiplicative norm.
</div>

<p><strong>PROOF</strong></p>
<p>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are of appropriate size such that</p>
<p><span class="math display">\[
A = 
\begin{bmatrix}
a_1^T \\
\vdots \\
a_m^T
\end{bmatrix}
, \;
B = 
\begin{bmatrix}
b_1 &amp; \cdots &amp; b_m
\end{bmatrix} 
\\
\quad \\
\begin{aligned}
\|AB\| &amp;= \sqrt{\sum_{i, j}{|a_i^Tb_j|^2}} \\
&amp; \le \sqrt{\sum_{i, j}{\| a_i \|^2 \| b_j\|^2}} \\
&amp;= \sqrt{\sum_{i}{\|a_i\|^2}} \sqrt{\sum_{j}{\|b_j\|^2}} \\
&amp;= \|A\| \|B\|
\end{aligned}
\]</span></p>
<p>The first inequality comes from the Cauchy-Schwarz inequality <span class="math inline">\(a \cdot b \le \|a\| \|b\|\)</span></p>
</div>
<div id="other-matrix-norms" class="section level3">
<h3><span class="header-section-number">4.8.3</span> Other matrix norms</h3>
<p>One can pick to one of two ways to generalize the F-norm. One is a generalization of the direct definition that F-norm is the 2-norm of <span class="math inline">\(||\text{vec}()||\)</span>. For <span class="math inline">\(p \ge 1\)</span>, it is the <strong>Frobenius-p</strong> norm:</p>
<p><span class="math display">\[
\| A \|_{F_p} = \Bigg (\sum_{i,j}{|A_{ij}|^p} \Bigg)^{\frac{1}{p}}
\]</span>
The Frobenius p-norm is the ordinary Frobenius norm.</p>
<p>Another generalization stems from the relationship between F-norm and singular values of <span class="math inline">\(A\)</span>. The <strong>Schatten p norm</strong> or <strong>nuclear p-norm</strong> is the p-norm of the vector composed of <span class="math inline">\(A\)</span>’s singular values</p>
<p><span class="math display">\[
\|A\|_{S_p} = \Big( \sum_{i = 1}^{\min(m, n)}{\sigma_i^p}\Big)^{\frac{1}{p}}
\]</span></p>
</div>
<div id="unitary-invariancy" class="section level3">
<h3><span class="header-section-number">4.8.4</span> Unitary invariancy</h3>

<div class="definition">
<p><span id="def:unitary-invariant" class="definition"><strong>Definition 4.8  (unitary invariant)  </strong></span>A matrix norm <span class="math inline">\(\|\cdot \|\)</span> is said to be unitary invariant if for all orthogonal matrices <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> of appropriate size</p>
<span class="math display">\[
\|A\| = \|UAV\|
\]</span>
</div>

<p>(unitary matrices refers to orthogonal matrices with complex-valued entries, but I focus on real matrices here.)</p>
<p>Essentially, if a norm depends only on the singular values of a matrix, it is unitary invariant. Since for such norms:</p>
<p><span class="math display">\[
\|A\| = \|\Sigma\|
\]</span>
Multiply two orthogonal matrices <span class="math inline">\(U\)</span> and <span class="math inline">\(V^T\)</span> on each side,</p>
<p><span class="math display">\[
\begin{aligned}
\|UA V^T\| &amp; = \| U \Sigma V^T\| \\
\|UAV^T\| &amp;= \|A\|
\end{aligned}
\]</span></p>

<div class="todo">
Does this work for all orthogonal matrices <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span>?
</div>

<p>The spectral norm (induced p-norm), F-norm and Schattan norm are all unitary invariant.</p>
</div>
</div>
<div id="low-rank-approximation" class="section level2">
<h2><span class="header-section-number">4.9</span> Low rank approximation</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="orthogonality.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="solutions-of-linear-system-ax-b.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": "https://github.com/enixam/math-foundations/edit/master/eigen-quadratic.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
