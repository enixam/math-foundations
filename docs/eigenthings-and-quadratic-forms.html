<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Eigenthings and Quadratic Forms | Mathematical Notes for Machine Learning</title>
  <meta name="description" content="Chapter 4 Eigenthings and Quadratic Forms | Mathematical Notes for Machine Learning" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Eigenthings and Quadratic Forms | Mathematical Notes for Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="enixam/math-foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Eigenthings and Quadratic Forms | Mathematical Notes for Machine Learning" />
  
  
  

<meta name="author" content="Qiushi Yan" />


<meta name="date" content="2020-09-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="orthogonality.html"/>
<link rel="next" href="singular-value-decomposition.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Math Notes for Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Linear Algebra</b></span></li>
<li class="chapter" data-level="1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html"><i class="fa fa-check"></i><b>1</b> Basic Matrix Algebra</a><ul>
<li class="chapter" data-level="1.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.1</b> Matrix Multiplication</a><ul>
<li class="chapter" data-level="1.1.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#geometric-transformations"><i class="fa fa-check"></i><b>1.1.1</b> Geometric Transformations</a></li>
<li class="chapter" data-level="1.1.2" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#selector-matrix"><i class="fa fa-check"></i><b>1.1.2</b> Selector matrix</a></li>
<li class="chapter" data-level="1.1.3" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#discrete-convolution"><i class="fa fa-check"></i><b>1.1.3</b> Discrete Convolution</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#elemetary-matrix-and-row-operations"><i class="fa fa-check"></i><b>1.2</b> Elemetary matrix and row operations</a></li>
<li class="chapter" data-level="1.3" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#lu-factorization"><i class="fa fa-check"></i><b>1.3</b> LU Factorization</a></li>
<li class="chapter" data-level="1.4" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#determinants"><i class="fa fa-check"></i><b>1.4</b> Determinants</a><ul>
<li class="chapter" data-level="1.4.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#cofactor-expansion"><i class="fa fa-check"></i><b>1.4.1</b> Cofactor Expansion</a></li>
<li class="chapter" data-level="1.4.2" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#geometric-interpretation-of-determinant"><i class="fa fa-check"></i><b>1.4.2</b> Geometric Interpretation of Determinant</a></li>
<li class="chapter" data-level="1.4.3" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#properties-of-determinant"><i class="fa fa-check"></i><b>1.4.3</b> Properties of Determinant</a></li>
<li class="chapter" data-level="1.4.4" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#cramers-rule"><i class="fa fa-check"></i><b>1.4.4</b> Cramerâ€™s Rule</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#trace"><i class="fa fa-check"></i><b>1.5</b> Trace</a></li>
<li class="chapter" data-level="1.6" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#matrix-inversion"><i class="fa fa-check"></i><b>1.6</b> Matrix Inversion</a><ul>
<li class="chapter" data-level="1.6.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#the-matrix-inversion-lemma"><i class="fa fa-check"></i><b>1.6.1</b> The Matrix Inversion Lemma</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#matrix-multiplication-as-linear-transformation"><i class="fa fa-check"></i><b>1.7</b> Matrix Multiplication as Linear Transformation</a></li>
<li class="chapter" data-level="1.8" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#complexity-of-matrix-computation"><i class="fa fa-check"></i><b>1.8</b> Complexity of Matrix Computation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="vector-spaces.html"><a href="vector-spaces.html"><i class="fa fa-check"></i><b>2</b> Vector Spaces</a><ul>
<li class="chapter" data-level="2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#vector-space"><i class="fa fa-check"></i><b>2.1</b> Vector Space</a><ul>
<li class="chapter" data-level="2.1.1" data-path="vector-spaces.html"><a href="vector-spaces.html#euclidean-space"><i class="fa fa-check"></i><b>2.1.1</b> Euclidean Space</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="vector-spaces.html"><a href="vector-spaces.html#metric-spaces-normed-spaces-inner-product-spaces"><i class="fa fa-check"></i><b>2.2</b> Metric Spaces, Normed Spaces, Inner Product Spaces</a><ul>
<li class="chapter" data-level="2.2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#metric-and-norm"><i class="fa fa-check"></i><b>2.2.1</b> Metric and Norm</a></li>
<li class="chapter" data-level="2.2.2" data-path="vector-spaces.html"><a href="vector-spaces.html#inner-produc-outer-product-cross-product"><i class="fa fa-check"></i><b>2.2.2</b> Inner Produc, Outer Product, Cross Product</a></li>
<li class="chapter" data-level="2.2.3" data-path="vector-spaces.html"><a href="vector-spaces.html#restricted-definition-of-inner-products-in-rn"><i class="fa fa-check"></i><b>2.2.3</b> Restricted Definition of Inner Products in <span class="math inline">\(R^n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="vector-spaces.html"><a href="vector-spaces.html#subspaces"><i class="fa fa-check"></i><b>2.3</b> Subspaces</a></li>
<li class="chapter" data-level="2.4" data-path="vector-spaces.html"><a href="vector-spaces.html#fundamental-theorem"><i class="fa fa-check"></i><b>2.4</b> Fundamental Theorem of Linear Algebra</a></li>
<li class="chapter" data-level="2.5" data-path="vector-spaces.html"><a href="vector-spaces.html#rank"><i class="fa fa-check"></i><b>2.5</b> Rank</a><ul>
<li class="chapter" data-level="2.5.1" data-path="vector-spaces.html"><a href="vector-spaces.html#effect-of-operations-on-matrix-rank"><i class="fa fa-check"></i><b>2.5.1</b> Effect of Operations on Matrix Rank</a></li>
<li class="chapter" data-level="2.5.2" data-path="vector-spaces.html"><a href="vector-spaces.html#gram-matrix"><i class="fa fa-check"></i><b>2.5.2</b> Gram Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="vector-spaces.html"><a href="vector-spaces.html#bases-and-coordinate-systems"><i class="fa fa-check"></i><b>2.6</b> Bases and Coordinate Systems</a><ul>
<li class="chapter" data-level="2.6.1" data-path="vector-spaces.html"><a href="vector-spaces.html#change-of-basis"><i class="fa fa-check"></i><b>2.6.1</b> Change of Basis</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="vector-spaces.html"><a href="vector-spaces.html#complexity-of-vector-computations"><i class="fa fa-check"></i><b>2.7</b> Complexity of Vector Computations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="orthogonality.html"><a href="orthogonality.html"><i class="fa fa-check"></i><b>3</b> Orthogonality</a><ul>
<li class="chapter" data-level="3.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition"><i class="fa fa-check"></i><b>3.1</b> Orthogonal Decomposition</a><ul>
<li class="chapter" data-level="3.1.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-complements"><i class="fa fa-check"></i><b>3.1.1</b> Orthogonal Complements</a></li>
<li class="chapter" data-level="3.1.2" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-sets-and-orthogonal-basis"><i class="fa fa-check"></i><b>3.1.2</b> Orthogonal Sets and Orthogonal Basis</a></li>
<li class="chapter" data-level="3.1.3" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition-1"><i class="fa fa-check"></i><b>3.1.3</b> Orthogonal Decomposition</a></li>
<li class="chapter" data-level="3.1.4" data-path="orthogonality.html"><a href="orthogonality.html#best-approximation"><i class="fa fa-check"></i><b>3.1.4</b> Best Approximation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="orthogonality.html"><a href="orthogonality.html#projection-and-idempotent-matrices"><i class="fa fa-check"></i><b>3.2</b> Projection and idempotent matrices</a></li>
<li class="chapter" data-level="3.3" data-path="orthogonality.html"><a href="orthogonality.html#gram-schmidt-process"><i class="fa fa-check"></i><b>3.3</b> Gram-Schmidt Process</a></li>
<li class="chapter" data-level="3.4" data-path="orthogonality.html"><a href="orthogonality.html#qr-factorizaiton"><i class="fa fa-check"></i><b>3.4</b> QR Factorizaiton</a></li>
<li class="chapter" data-level="3.5" data-path="orthogonality.html"><a href="orthogonality.html#orthonormal-sets-and-orthogonal-matrices"><i class="fa fa-check"></i><b>3.5</b> Orthonormal Sets and Orthogonal Matrices</a><ul>
<li class="chapter" data-level="3.5.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-matrices"><i class="fa fa-check"></i><b>3.5.1</b> Orthogonal Matrices</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="orthogonality.html"><a href="orthogonality.html#lesat-squares-problems"><i class="fa fa-check"></i><b>3.6</b> Lesat Squares Problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html"><i class="fa fa-check"></i><b>4</b> Eigenthings and Quadratic Forms</a><ul>
<li class="chapter" data-level="4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>4.1</b> Eigenvectors and Eigenvalues</a><ul>
<li class="chapter" data-level="4.1.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#additional-properties-of-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>4.1.1</b> Additional Properties of Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="4.1.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#left-eigenvectors-and-right-eigenvectors"><i class="fa fa-check"></i><b>4.1.2</b> Left Eigenvectors and Right Eigenvectors</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#diagnolization-and-similar-matrices"><i class="fa fa-check"></i><b>4.2</b> Diagnolization and Similar Matrices</a><ul>
<li class="chapter" data-level="4.2.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#similarity"><i class="fa fa-check"></i><b>4.2.1</b> Similarity</a></li>
<li class="chapter" data-level="4.2.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#jordan-matrix"><i class="fa fa-check"></i><b>4.2.2</b> Jordan Matrix</a></li>
<li class="chapter" data-level="4.2.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#simultaneous-diagonalization"><i class="fa fa-check"></i><b>4.2.3</b> Simultaneous Diagonalization</a></li>
<li class="chapter" data-level="4.2.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>4.2.4</b> Cayley-Hamilton Theorem</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#symmetric-matrices"><i class="fa fa-check"></i><b>4.3</b> Symmetric Matrices</a><ul>
<li class="chapter" data-level="4.3.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#spectral-decomposition"><i class="fa fa-check"></i><b>4.3.1</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="4.3.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#a-orthogonality"><i class="fa fa-check"></i><b>4.3.2</b> A-Orthogonality</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#quadratic-forms"><i class="fa fa-check"></i><b>4.4</b> Quadratic Forms</a><ul>
<li class="chapter" data-level="4.4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#change-of-variable"><i class="fa fa-check"></i><b>4.4.1</b> Change of Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#classification-of-quadratic-forms"><i class="fa fa-check"></i><b>4.4.2</b> Classification of Quadratic Forms</a></li>
<li class="chapter" data-level="4.4.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#gershgorin-discs-and-diagonal-dominance"><i class="fa fa-check"></i><b>4.4.3</b> Gershgorin Discs and Diagonal Dominance</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#cholesky-factorization"><i class="fa fa-check"></i><b>4.5</b> Cholesky Factorization</a></li>
<li class="chapter" data-level="4.6" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#rayleigh-quotients"><i class="fa fa-check"></i><b>4.6</b> Rayleigh Quotients</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html"><i class="fa fa-check"></i><b>5</b> Singular Value Decomposition</a><ul>
<li class="chapter" data-level="5.1" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#singular-values"><i class="fa fa-check"></i><b>5.1</b> Singular Values</a></li>
<li class="chapter" data-level="5.2" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#svd-theorem"><i class="fa fa-check"></i><b>5.2</b> SVD</a></li>
<li class="chapter" data-level="5.3" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#matrix-norms"><i class="fa fa-check"></i><b>5.3</b> Matrix Norms</a><ul>
<li class="chapter" data-level="5.3.1" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#induced-norms"><i class="fa fa-check"></i><b>5.3.1</b> Induced Norms</a></li>
<li class="chapter" data-level="5.3.2" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#entry-wise-norm"><i class="fa fa-check"></i><b>5.3.2</b> Entry-wise Norm</a></li>
<li class="chapter" data-level="5.3.3" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#other-matrix-norms"><i class="fa fa-check"></i><b>5.3.3</b> Other Matrix Norms</a></li>
<li class="chapter" data-level="5.3.4" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#unitary-invariant-norms"><i class="fa fa-check"></i><b>5.3.4</b> Unitary Invariant Norms</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#low-rank-approximation"><i class="fa fa-check"></i><b>5.4</b> Low Rank Approximation</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-system.html"><a href="linear-system.html"><i class="fa fa-check"></i><b>6</b> Solutions of Linear System Ax = b</a><ul>
<li class="chapter" data-level="6.1" data-path="linear-system.html"><a href="linear-system.html#generalized-inverse"><i class="fa fa-check"></i><b>6.1</b> Generalized Inverse</a></li>
<li class="chapter" data-level="6.2" data-path="linear-system.html"><a href="linear-system.html#ill-conditioned-matrices"><i class="fa fa-check"></i><b>6.2</b> Ill-Conditioned Matrices</a><ul>
<li class="chapter" data-level="6.2.1" data-path="linear-system.html"><a href="linear-system.html#condition-number"><i class="fa fa-check"></i><b>6.2.1</b> Condition Number</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Multivariate Calculus</b></span></li>
<li class="chapter" data-level="7" data-path="partial-derivatives.html"><a href="partial-derivatives.html"><i class="fa fa-check"></i><b>7</b> Partial Derivatives</a><ul>
<li class="chapter" data-level="7.1" data-path="partial-derivatives.html"><a href="partial-derivatives.html#limit-and-continuity"><i class="fa fa-check"></i><b>7.1</b> Limit and Continuity</a></li>
<li class="chapter" data-level="7.2" data-path="partial-derivatives.html"><a href="partial-derivatives.html#partial-derivative"><i class="fa fa-check"></i><b>7.2</b> Partial Derivative</a><ul>
<li class="chapter" data-level="7.2.1" data-path="partial-derivatives.html"><a href="partial-derivatives.html#gradient-and-directional-derivative"><i class="fa fa-check"></i><b>7.2.1</b> Gradient and Directional Derivative</a></li>
<li class="chapter" data-level="7.2.2" data-path="partial-derivatives.html"><a href="partial-derivatives.html#linearization-of-two-variable-functions"><i class="fa fa-check"></i><b>7.2.2</b> Linearization of Two-variable Functions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="partial-derivatives.html"><a href="partial-derivatives.html#differentials"><i class="fa fa-check"></i><b>7.3</b> Differentials</a><ul>
<li class="chapter" data-level="7.3.1" data-path="partial-derivatives.html"><a href="partial-derivatives.html#continuity-partial-derivatives-and-differentiability"><i class="fa fa-check"></i><b>7.3.1</b> Continuity, Partial Derivatives and Differentiability</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="partial-derivatives.html"><a href="partial-derivatives.html#divergence-curl-and-laplacian"><i class="fa fa-check"></i><b>7.4</b> Divergence, Curl, and Laplacian</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="matrix-calculus.html"><a href="matrix-calculus.html"><i class="fa fa-check"></i><b>8</b> Matrix Calculus</a><ul>
<li class="chapter" data-level="8.1" data-path="matrix-calculus.html"><a href="matrix-calculus.html#the-chain-rule"><i class="fa fa-check"></i><b>8.1</b> The Chain Rule</a></li>
<li class="chapter" data-level="8.2" data-path="matrix-calculus.html"><a href="matrix-calculus.html#useful-identities-in-matirx-calculus"><i class="fa fa-check"></i><b>8.2</b> Useful Identities in Matirx Calculus</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="taylor-series.html"><a href="taylor-series.html"><i class="fa fa-check"></i><b>9</b> Taylor Series</a><ul>
<li class="chapter" data-level="9.1" data-path="taylor-series.html"><a href="taylor-series.html#convergence-of-taylor-series"><i class="fa fa-check"></i><b>9.1</b> Convergence of Taylor Series</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="multiple-integral.html"><a href="multiple-integral.html"><i class="fa fa-check"></i><b>10</b> Multiple Integral</a></li>
<li class="part"><span><b>III Probability Theory</b></span></li>
<li class="chapter" data-level="11" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html"><i class="fa fa-check"></i><b>11</b> Basics of Probability Theory</a><ul>
<li class="chapter" data-level="11.1" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#probabilty-space"><i class="fa fa-check"></i><b>11.1</b> Probabilty Space</a></li>
<li class="chapter" data-level="11.2" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#counting"><i class="fa fa-check"></i><b>11.2</b> Counting</a></li>
<li class="chapter" data-level="11.3" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#conditional-probability"><i class="fa fa-check"></i><b>11.3</b> Conditional Probability</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html"><i class="fa fa-check"></i><b>12</b> Random variables and moments</a><ul>
<li class="chapter" data-level="12.1" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#properties-of-expectation-and-variance"><i class="fa fa-check"></i><b>12.1</b> Properties of Expectation and Variance</a><ul>
<li class="chapter" data-level="12.1.1" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#random-vectors"><i class="fa fa-check"></i><b>12.1.1</b> Random vectors</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#other-summaries"><i class="fa fa-check"></i><b>12.2</b> Other Summaries</a></li>
<li class="chapter" data-level="12.3" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#moment-generating-functions"><i class="fa fa-check"></i><b>12.3</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="univariate-distributions.html"><a href="univariate-distributions.html"><i class="fa fa-check"></i><b>13</b> Univariate Distributions</a><ul>
<li class="chapter" data-level="13.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#uniform-distribution"><i class="fa fa-check"></i><b>13.1</b> Uniform Distribution</a></li>
<li class="chapter" data-level="13.2" data-path="univariate-distributions.html"><a href="univariate-distributions.html#normal-distribution"><i class="fa fa-check"></i><b>13.2</b> Normal Distribution</a><ul>
<li class="chapter" data-level="13.2.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#log-normal-distribution"><i class="fa fa-check"></i><b>13.2.1</b> Log Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="univariate-distributions.html"><a href="univariate-distributions.html#binomial-distribution-and-beta-distribution"><i class="fa fa-check"></i><b>13.3</b> Binomial Distribution and Beta Distribution</a></li>
<li class="chapter" data-level="13.4" data-path="univariate-distributions.html"><a href="univariate-distributions.html#poisson-distribution"><i class="fa fa-check"></i><b>13.4</b> Poisson Distribution</a><ul>
<li class="chapter" data-level="13.4.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#poisson-process"><i class="fa fa-check"></i><b>13.4.1</b> Poisson Process</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="univariate-distributions.html"><a href="univariate-distributions.html#exponential-distribution-and-gamma-distribution"><i class="fa fa-check"></i><b>13.5</b> Exponential Distribution and Gamma Distribution</a><ul>
<li class="chapter" data-level="13.5.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#properties"><i class="fa fa-check"></i><b>13.5.1</b> Properties</a></li>
<li class="chapter" data-level="13.5.2" data-path="univariate-distributions.html"><a href="univariate-distributions.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>13.5.2</b> Inverse Gamma Distribution</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="univariate-distributions.html"><a href="univariate-distributions.html#beta-distribution"><i class="fa fa-check"></i><b>13.6</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>14</b> Multivariate Distributions</a><ul>
<li class="chapter" data-level="14.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>14.1</b> Multivariate Normal Distribution</a><ul>
<li class="chapter" data-level="14.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#chi-square-distribution"><i class="fa fa-check"></i><b>14.1.1</b> Chi-square Distribution</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#dirichlet-distributon"><i class="fa fa-check"></i><b>14.2</b> Dirichlet Distributon</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="markov-chain.html"><a href="markov-chain.html"><i class="fa fa-check"></i><b>15</b> Markov Chain</a></li>
<li class="part"><span><b>IV Learning Theory</b></span></li>
<li class="chapter" data-level="16" data-path="the-learning-problem.html"><a href="the-learning-problem.html"><i class="fa fa-check"></i><b>16</b> The Learning Problem</a></li>
<li class="part"><span><b>V Optimization</b></span></li>
<li class="chapter" data-level="17" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html"><i class="fa fa-check"></i><b>17</b> Basics of Optimization</a><ul>
<li class="chapter" data-level="17.1" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#univariate-optimization"><i class="fa fa-check"></i><b>17.1</b> Univariate Optimization</a></li>
<li class="chapter" data-level="17.2" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#multivariate-optimization"><i class="fa fa-check"></i><b>17.2</b> Multivariate Optimization</a></li>
<li class="chapter" data-level="17.3" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#convex-functions"><i class="fa fa-check"></i><b>17.3</b> Convex Functions</a></li>
<li class="chapter" data-level="17.4" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#method-of-lagrange-multiplier"><i class="fa fa-check"></i><b>17.4</b> Method of Lagrange Multiplier</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="gradient-descent.html"><a href="gradient-descent.html"><i class="fa fa-check"></i><b>18</b> Gradient Descent</a></li>
<li class="part"><span><b>VI Applications</b></span></li>
<li class="chapter" data-level="19" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>19</b> Linear Models</a><ul>
<li class="chapter" data-level="19.1" data-path="linear-models.html"><a href="linear-models.html#ordinary-least-squares"><i class="fa fa-check"></i><b>19.1</b> Ordinary Least Squares</a><ul>
<li class="chapter" data-level="19.1.1" data-path="linear-models.html"><a href="linear-models.html#least-square-estimation"><i class="fa fa-check"></i><b>19.1.1</b> Least Square Estimation</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="linear-models.html"><a href="linear-models.html#weighted-least-squares"><i class="fa fa-check"></i><b>19.2</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="19.3" data-path="linear-models.html"><a href="linear-models.html#partial-least-squares"><i class="fa fa-check"></i><b>19.3</b> Partial Least Squares</a></li>
<li class="chapter" data-level="19.4" data-path="linear-models.html"><a href="linear-models.html#regularized-regression"><i class="fa fa-check"></i><b>19.4</b> Regularized Regression</a><ul>
<li class="chapter" data-level="19.4.1" data-path="linear-models.html"><a href="linear-models.html#ridge-regression"><i class="fa fa-check"></i><b>19.4.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="19.4.2" data-path="linear-models.html"><a href="linear-models.html#lasso"><i class="fa fa-check"></i><b>19.4.2</b> Lasso</a></li>
<li class="chapter" data-level="19.4.3" data-path="linear-models.html"><a href="linear-models.html#elastic-net"><i class="fa fa-check"></i><b>19.4.3</b> Elastic Net</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>20</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="21" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>21</b> Text Mining</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li>
  <a href="https://github.com/rstudio/bookdown" target="blank">Written with bookdown</a>
</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Notes for Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="eigenthings-and-quadratic-forms" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Eigenthings and Quadratic Forms</h1>
<div id="eigenvectors-and-eigenvalues" class="section level2">
<h2><span class="header-section-number">4.1</span> Eigenvectors and Eigenvalues</h2>

<div class="definition">
<p><span id="def:eigen" class="definition"><strong>Definition 4.1  (Eigenvectors and eigenvalues)  </strong></span>An <strong>eigenvector</strong> of an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is a <em>nonzero</em> vector <span class="math inline">\(\bar{x}\)</span> such that <span class="math inline">\(A\bar{x} = \lambda\bar{x}\)</span>.</p>
<span class="math inline">\(\lambda\)</span> is the <strong>eigenvalue</strong> of <span class="math inline">\(A\)</span> if there is a nontrivial solution <span class="math inline">\(\bar{x}\)</span> of <span class="math inline">\(A\bar{x} = \lambda \bar{x}\)</span>; such an <span class="math inline">\(\bar{x}\)</span> is called an <em>eigenvector corresponding to <span class="math inline">\(\lambda\)</span></em>
</div>

<p>To find eigenvalues and corresponding eigenvectors of <span class="math inline">\(A\)</span>, we look at the equation</p>
<p><span class="math display">\[
(A - \lambda I)\bar{x}= 0
\]</span></p>
<p>Since eigenvector <span class="math inline">\(\bar{x}\)</span> must be nonzero, <span class="math inline">\((A - \lambda I)\)</span> is a singular matrix</p>
<p><span class="math display" id="eq:characteristic-equation">\[\begin{equation}
\tag{4.1}
\det (A - \lambda I) = 0
\end{equation}\]</span></p>
<p>Eq <a href="eigenthings-and-quadratic-forms.html#eq:characteristic-equation">(4.1)</a> is called the <strong>characteristic equation</strong> of matrix <span class="math inline">\(A\)</span>. This is a scalar equation containing information about eigenvalues and eigenvectors of a square matrix <span class="math inline">\(A\)</span>.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-1" class="theorem"><strong>Theorem 3.1  </strong></span>Eigenvalues of a trangular matrix are its diagonal entries.
</div>

<div class="proof">
Proof
</div>
<p>Consider the <span class="math inline">\(3 \times 3\)</span> case. If <span class="math inline">\(A\)</span> is upper triangular, then <span class="math inline">\(A - \lambda I\)</span> has the form</p>
<p><span class="math display">\[
\begin{bmatrix}
a_{11} - \lambda &amp; a_{12} &amp; a_{13} \\
0 &amp; a_{22} - \lambda &amp; a_{23}  \\
0 &amp; 0 &amp; a_{33} - \lambda
\end{bmatrix}
\]</span>
So the roots of characteristic are <span class="math inline">\(a_{11}, a_{22}, a_{33}\)</span> respectively.</p>
<p>There are some useful results about how eigenvalues change after various manipulations.</p>
<ol style="list-style-type: decimal">
<li><p>For any <span class="math inline">\(k, b \in \mathbb{R}\)</span>, <span class="math inline">\(\bar{x}\)</span> is an eigenvector of <span class="math inline">\(kA + bI\)</span> with eigenvalue <span class="math inline">\(k\lambda + b\)</span></p></li>
<li><p>If <span class="math inline">\(A\)</span> is invertible, then <span class="math inline">\(\bar{x}\)</span> is an eigenvector of <span class="math inline">\(A^{-1}\)</span> with eigenvalue <span class="math inline">\(1/\lambda\)</span></p></li>
<li><p><span class="math inline">\(A^{k}\bar{x} = \lambda^{k}\bar{x}\)</span></p></li>
</ol>
<div class="proof">
Proof
</div>
<p>For (1)
<span class="math display">\[
(kA + bI)\bar{x} = kA\bar{x} + bI\bar{x} = k \lambda\bar{x} + b\bar{x} = (k\lambda + b)\bar{x} 
\]</span></p>
<p>For(2)</p>
<p><span class="math display">\[
\bar{x} = A^{-1}A\bar{x} =  A^{-1}\lambda \bar{x} = \lambda A^{-1}\bar{x}
\]</span></p>
<p>The next theorem is important in terms of diagonalization and spectral decomposition</p>

<div class="theorem">
<span id="thm:distinct-eigenvalue" class="theorem"><strong>Theorem 4.1  </strong></span>For distinct eigenvalues <span class="math inline">\(\lambda_1, \cdots, \lambda_r\)</span> of an <span class="math inline">\(n \times n\)</span> matrix A, their corresponding eigenvectors <span class="math inline">\(\bar{v_1}, ..., \bar{v_r}\)</span> are linearly independent.
</div>

<div class="proof">
Proof
</div>
<p>Suppose for r distinct eigenvalue <span class="math inline">\(\lambda_1, \cdots, \lambda_r\)</span>, the set <span class="math inline">\(\{\bar{v_1}, ..., \bar{v_r}\}\)</span> is not linearly independent, and <span class="math inline">\(p\)</span> is the least index such that <span class="math inline">\(\bar{v}_{p+1}\)</span> is a linear combination of the preceding vectors. Then there exists scalars <span class="math inline">\(c_1, \cdots, c_p\)</span> such that</p>
<p><span class="math display">\[
c_1\bar{v}_1 + \cdots + c_p\bar{v}_p = \bar{v}_{p+1} \tag{1}
\]</span>
Left multiply by <span class="math inline">\(A\)</span>, and note we have <span class="math inline">\(A\bar{v}_i = \lambda_i\bar{v}_i\)</span> for <span class="math inline">\(i = 1, ..., n\)</span></p>
<p><span class="math display">\[
c_1\lambda_1\bar{v}_1 + \cdots + c_p\lambda_p\bar{v}_p = \lambda_{p+1}\bar{v}_{p+1} \tag{2}
\]</span>
Multiplying both sides of (2) by <span class="math inline">\(\lambda_{p+1}\)</span> and subtracting (2) from the result</p>
<p><span class="math display">\[
c_1(\lambda_1 - \lambda_{p+1})\bar{v}_1 +\cdots + c_p(\lambda_p - \lambda_{p+1})\bar{v}_p = 0 \tag{3}
\]</span>
Since <span class="math inline">\(\bar{v}_1, ..., \bar{v}_p\)</span> are linearly independent, weights in (3) must be all zero. Since <span class="math inline">\(\lambda_1, \cdots, \lambda_p\)</span> are distinct, hence <span class="math inline">\(c_i = 0, \, i = 1, ..., p\)</span>. But then (5) says that eigenvector <span class="math inline">\(\bar{v}_{p+1}\)</span> is zero vector, which contradicts definition <a href="eigenthings-and-quadratic-forms.html#def:eigen">4.1</a></p>
<hr>

<div class="corollary">
<span id="cor:same-nonzero" class="corollary"><strong>Corollary 4.1  </strong></span>Let <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> has the same set of <em>nonzero</em> eigenvalues.
</div>

<div class="proof">
Proof
</div>
<p>Let <span class="math inline">\(\lambda\)</span> be a nonzero eigenvalue of <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(\bar{x}\)</span> its eigenvector</p>
<p><span class="math display">\[
\begin{split}
(A^TA)\bar{x} &amp;= \lambda\bar{x} \\
\end{split}
\]</span>
Left multiply by <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
AA^T(A\bar{x}) = \lambda (A\bar{x})
\]</span>
We will have to verify that <span class="math inline">\(A\bar{x}\)</span> is no zero vector before concluding <span class="math inline">\(\lambda\)</span> is also an eigenvector of <span class="math inline">\(AA^T\)</span>. Suppose <span class="math inline">\(A\bar{x} = 0\)</span>, then <span class="math inline">\(A^TA\bar{x} =\lambda\bar{x} = 0\)</span>. Since <span class="math inline">\(\bar{x}\)</span> is a eigenvector which is nonzero, <span class="math inline">\(\lambda = 0\)</span>, which contradicts our former statement. Thus, any nonzero eigenvalue of <span class="math inline">\(A^TA\)</span> is also an eigenvalue of <span class="math inline">\(AA^T\)</span>.</p>
<p><span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> are known as Gram matrix and left Gram matrix in corollary <a href="vector-spaces.html#prp:gram-matrix">2.1</a></p>
<div id="additional-properties-of-eigenvalues-and-eigenvectors" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Additional Properties of Eigenvalues and Eigenvectors</h3>
<p>Let <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> with eigenvalues <span class="math inline">\(\lambda_1, ..., \lambda_n\)</span>. Here are some additional properties of this matrix and its eigenvalues:</p>
<ul>
<li>The trace of <span class="math inline">\(A\)</span> is the sum of all eigenvalues</li>
</ul>
<p><span class="math display">\[
\text{tr}(A) = \sum_{i=1}^{n}{\lambda_i}
\]</span></p>
<ul>
<li>The determinant of <span class="math inline">\(A\)</span> is the product of all its eigenvalues.</li>
</ul>
<p><span class="math display">\[
\det(A) = \prod_{i=1}^{n}{\lambda_i}
\]</span></p>
<ul>
<li><p>The eigenvalues of <span class="math inline">\(k\)</span>th power of <span class="math inline">\(A\)</span>, i.e.Â <span class="math inline">\(A^k\)</span>, is <span class="math inline">\(\lambda_1^k, ..., \lambda_n^k\)</span></p></li>
<li><p>If <span class="math inline">\(A\)</span> is invertible, then eigenvalues of <span class="math inline">\(A^{-1}\)</span> are <span class="math inline">\(\frac{1}{\lambda_1}, ..., \frac{1}{\lambda_n}\)</span></p></li>
<li><p>For a polynomial function <span class="math inline">\(P\)</span> the eigenvalues of <span class="math inline">\(P(A)\)</span> are <span class="math inline">\(P(\lambda_1), ..., P(\lambda_n)\)</span></p></li>
</ul>
</div>
<div id="left-eigenvectors-and-right-eigenvectors" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Left Eigenvectors and Right Eigenvectors</h3>
<p><span class="math display">\[
\bar{x}A = \lambda\bar{x}
\]</span></p>
</div>
</div>
<div id="diagnolization-and-similar-matrices" class="section level2">
<h2><span class="header-section-number">4.2</span> Diagnolization and Similar Matrices</h2>

<div class="definition">
<p><span id="def:diagonalization" class="definition"><strong>Definition 4.2  (Diagonalization thoerem)  </strong></span>An <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is diagnolizable <strong>if and only if</strong> A has <span class="math inline">\(n\)</span> independent linearly independent eigenvectors.</p>
<p>In such case, in <span class="math inline">\(A = P \Lambda P^{-1}\)</span>, the diagonal entries of <span class="math inline">\(D\)</span> are eigenvalues that correpond, respectively, to the eigenvectors of in <span class="math inline">\(P\)</span></p>
In other words, <span class="math inline">\(A\)</span> is diagnolizable if and only if there are enough eigenvectors in form a basis of <span class="math inline">\(R^n\)</span>, called an <strong>eigenvector basis</strong> of <span class="math inline">\(R^n\)</span>
</div>

<div class="proof">
Proof
</div>
<p><span class="math display">\[
\begin{split}
AP &amp;= A[\bar{v}_1 \cdots \bar{v}_n] \\
   &amp;= [A\bar{v}_1 \cdots A\bar{v}_n] \\ 
   &amp;= [\lambda_1\bar{v}_1 \cdots \lambda_n\bar{v}_n]
\end{split}
\]</span>
while on the other side of the equation:</p>
<p><span class="math display">\[
\begin{aligned}
DP &amp;= 
[\bar{v}_1 \cdots \bar{v}_n]
\begin{bmatrix}
\lambda_1 &amp; 0 &amp; \cdots &amp; 0\\
0  &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_n 
\end{bmatrix} 
 \\
&amp;= [\lambda_1\bar{v}_1 \cdots \lambda_n\bar{v}_n]
\end{aligned}
\]</span></p>
<p>So that</p>
<p><span class="math display">\[
\begin{aligned}
AP &amp;= PD \\
A &amp;= P \Lambda P^{-1}
\end{aligned}
\]</span>
Because <span class="math inline">\(P\)</span> contains <span class="math inline">\(n\)</span> independent columns so itâ€™s invertible.</p>
<p>According to theorem <a href="eigenthings-and-quadratic-forms.html#thm:distinct-eigenvalue">4.1</a>, an <span class="math inline">\(n \times n\)</span> matrix with <span class="math inline">\(n\)</span> distinct eigenvalues is diagonalizable. This is a sufficient condition.</p>
<p>For matrices whose eigenvalues are not distinct, there is still a change that it is diagonalizable. For any matrix <span class="math inline">\(A_{n\times n}\)</span>, as long as the sum of the dimensions of the eigenspaces equals <span class="math inline">\(n\)</span> then <span class="math inline">\(P\)</span> is invertible. This could happen in the following two scenarios</p>
<ol style="list-style-type: decimal">
<li><p>The characteristic polynomial factors completely into linear factors. This is the case when <span class="math inline">\(A\)</span> has n distinct eigenvalues.</p></li>
<li><p>The dimension of the eigenspace for each <span class="math inline">\(\lambda_k\)</span> equals the multiplicity of <span class="math inline">\(\lambda_k\)</span>. Thus <span class="math inline">\(A\)</span> with repeated eigenvalues can still be diagonalizable.<br />
Repeated eigenvalues create the possibility that a diagonalization might not exist. Particularly, if less than <span class="math inline">\(r_i\)</span> eigenvectors exist for an eigenvalue with multiplicity <span class="math inline">\(r_i\)</span>, a diagonalization does not exist. Such a matrix is said to be <strong>deflective</strong>.</p></li>
</ol>
<div id="similarity" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Similarity</h3>
<p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are both <span class="math inline">\(n \times n\)</span> matrices, then <span class="math inline">\(A\)</span> <strong>is similar to</strong> <span class="math inline">\(B\)</span> if there is an invertible matrix <span class="math inline">\(P\)</span> such that <span class="math inline">\(P^{-1}AP = B\)</span>, or equivalently if we write <span class="math inline">\(Q\)</span> for <span class="math inline">\(P^{-1}\)</span>, <span class="math inline">\(Q^{-1}BQ = A\)</span>. Changing <span class="math inline">\(A\)</span> into <span class="math inline">\(P^{-1}AP\)</span> is called a <strong>similarity transformation</strong>.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-2" class="theorem"><strong>Theorem 3.2  </strong></span>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are similar, they have the same eigenvalues.
</div>

<div class="proof">
Proof
</div>
<p>If <span class="math inline">\(B = P^{-1}AP\)</span>, then</p>
<p><span class="math display">\[
B - \lambda I = P^{-1}AP - \lambda P^{-1}P = P^{-1}(AP - \lambda P) =  P^{-1}(A - \lambda I) P
\]</span>
so that</p>
<p><span class="math display">\[
\det (B - \lambda I ) = \det(P) \cdot \det(A - \lambda I ) \cdot \det(P^{-1})
\]</span>
since <span class="math inline">\(\det(P) \cdot \det(P^{-1}) = \det (I) = 1\)</span>, we have</p>
<p><span class="math display">\[
\det (B - \lambda I)  = \det(A - \lambda I)
\]</span></p>
<p>As a result of their identical characteristic polynomial, <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span> have the same eigenvalues. We can also show that eigenvector of <span class="math inline">\(B\)</span> is <span class="math inline">\(P\bar{v}\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
A\bar{v} &amp;= \lambda\bar{v} \\
(P^{-1}BP)\bar{v} &amp;= \lambda\bar{v} \\
P(P^{-1}BP)\bar{v} &amp;= \lambda P\bar{v} \\
B(P\bar{v}) = \lambda P \bar{v}
\end{aligned}
\]</span></p>
<p>The similarity theorem leads to a interesting result.</p>

<div class="corollary">
<span id="cor:unnamed-chunk-3" class="corollary"><strong>Corollary 4.2  </strong></span>For <span class="math inline">\(A, B \in \mathbb{R}^{n \times n}\)</span>, <span class="math inline">\(AB\)</span> and <span class="math inline">\(BA\)</span> are similar matrices and therefore share the same set of eigenvalues.
</div>

<p>To prove this, we need to show that there exists a invertible matrix <span class="math inline">\(A\)</span> such that <span class="math inline">\(P^{-1}(AB)P = BA\)</span>. Take <span class="math inline">\(P = A\)</span> and the equation holds.</p>
<p>It is easy to show that similarity is <strong>transitive</strong>: if <span class="math inline">\(A\)</span> is similar to <span class="math inline">\(B\)</span>, <span class="math inline">\(B\)</span> is similar to <span class="math inline">\(C\)</span>, then <span class="math inline">\(A\)</span> is similar to <span class="math inline">\(C\)</span>. So similarity means a family of matrices with the same set of eigenvalues, the most special and simplest of which is the diagonal matrix (if this is an diagonalizable family). Some computer algorithms calculate eigenvalues of <span class="math inline">\(A\)</span> in this manner: with a sequential choices of <span class="math inline">\(P\)</span>, the off-diagonal elements of <span class="math inline">\(A\)</span> become smaller and smaller until <span class="math inline">\(A\)</span> becomes a triangular matrix or diagonal matrix, whose eigenvalues are simply diagonal entries and is the same as <span class="math inline">\(A\)</span>.</p>
<p>It is obvious that a diagonalizable matrix <span class="math inline">\(A\)</span> is similar to diagonal matrix <span class="math inline">\(D\)</span>, whose diagonal entries are <span class="math inline">\(A\)</span>â€™s eigenvalues <span class="math inline">\(\lambda_i\)</span>, and <span class="math inline">\(P = [\bar{v}_1 \;\; \cdots \;\; \bar{v}_n]^{-1}\)</span> where <span class="math inline">\(\bar{v}_i, \;i = 1,..., n\)</span> are eigenvectors corresponding to <span class="math inline">\(\lambda_i\)</span>.</p>
<p>But square matrix <span class="math inline">\(A\)</span> can still be similar to matrices other than <span class="math inline">\(D\)</span> with other choices of <span class="math inline">\(P\)</span>, and non-diagonal matrices can also have similar matrices of their own. In fact, <strong>every square matrix is similar to a matrix in Jordan matrix</strong> <a href="eigenthings-and-quadratic-forms.html#jordan-matrix">4.2.2</a>.</p>
<hr>
<p>Similarity is only a <em>sufficient</em> condition for identical eigenvalues. The matrices</p>
<p><span class="math display">\[
\begin{bmatrix}
2 &amp; 1 \\
0 &amp; 2 
\end{bmatrix}
\;\text{and}\;
\begin{bmatrix}
2 &amp; 0 \\
0 &amp; 2 
\end{bmatrix}
\]</span>
are not similar even though they have the same eigenvalues.</p>
</div>
<div id="jordan-matrix" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Jordan Matrix</h3>
<p>For non-diagonalizable square matrix <span class="math inline">\(A_{n \times n}\)</span>, the goal is to with similar transformation <span class="math inline">\(P^{-1}AP\)</span> construct a matrix that is as near to a diagonal matrix as possible.</p>

<div class="definition">
<span id="def:unnamed-chunk-4" class="definition"><strong>Definition 4.3  </strong></span>The <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(J_{\lambda, n}\)</span> with <span class="math inline">\(\lambda\)</span>s on the diagonal, <span class="math inline">\(1\)</span>s on the superdiagonal and <span class="math inline">\(0\)</span>s elsewhere is called a Jordan matrix. A Jordan matrix in Jordan normal form is a block matrix that has Jordan blocks down its block diagonal and is zero elsewhere
</div>

<p>An example of Jordan matrix, the appearance of <span class="math inline">\(\lambda_i\)</span> on the diagonal is equal to its multiplicity as <span class="math inline">\(A\)</span>â€™s eigenvalue.
<span class="math display">\[
\begin{bmatrix}
\lambda_1 &amp; 1  &amp; \\
&amp; \lambda_1 &amp; 1 &amp; \\
&amp; &amp; \lambda_1 &amp; \\ 
&amp; &amp; &amp; \lambda_2 &amp; 1 \\
&amp; &amp; &amp; &amp; \lambda_2  \\ 
&amp; &amp; &amp; &amp; &amp; \lambda_3 &amp; 1  \\ 
&amp; &amp; &amp; &amp; &amp; &amp; \ddots \\
&amp; &amp; &amp; &amp; &amp; &amp; &amp; \lambda_n &amp; 1 \\
&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \lambda_n
\end{bmatrix}
\]</span>
An illustration from <a href="https://en.wikipedia.org/wiki/Jordan_normal_form">wikipedia</a>, the circled area is the Jordan block.
<img src="images/jordan-blocks.png" width="40%" style="display: block; margin: auto;" /></p>
<p>Though the purpose of this section was not the computation details of Jordan matrices, it helps to give a concrete example. Consider <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
A = 
\begin{bmatrix}
5 &amp; 4 &amp; 2 &amp; 1 \\
0 &amp; 1 &amp; -1 &amp; -1 \\
-1 &amp; -1 &amp; 3 &amp; 0 \\
1 &amp; 1 &amp; -1 &amp; 2
\end{bmatrix}
\]</span></p>
<p>Including multiplicity, the eigenvalues of <span class="math inline">\(A\)</span> are <span class="math inline">\(\lambda = 1, 2, 4, 4\)</span>. And for <span class="math inline">\(\lambda = 4\)</span>, the eigenspace is 1 dimensional instead of 2, meaning <span class="math inline">\(A\)</span> is not diagonalizable. Nonetheless, <span class="math inline">\(A\)</span> is similar to the following Jordan matrix</p>
<p><span class="math display">\[
J = 
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 2 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 4 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 4
\end{bmatrix}
\]</span></p>
<p>To obtain <span class="math inline">\(P\)</span>, recall that <span class="math inline">\(P^{-1}AP = J\)</span>. Let <span class="math inline">\(P\)</span> have column vectors <span class="math inline">\(p_i, \; i = 1,...,4\)</span>, then:</p>
<p><span class="math display">\[
A[\bar{p}_1 \; \; \bar{p}_2 \;\; \bar{p}_3 \;\; \bar{p}_4] = [\bar{p}_1 \; \; \bar{p}_2 \;\; \bar{p}_3 \;\; \bar{p}_4]
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 2 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 4 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 4
\end{bmatrix}
= [\bar{p}_1 \;\; 2\bar{p}_2 \;\; 4\bar{p}_3 \;\; \bar{p}_3 + 4\bar{p}_4]
\]</span></p>
<p>We see that</p>
<p><span class="math display">\[
\begin{aligned}
(A - 1I)\bar{p}_1 &amp;= \bar{0} \\
(A - 2I)\bar{p}_2 &amp;= \bar{0} \\
(A - 4I)\bar{p}_3 &amp;= \bar{0} \\
(A - 1I)\bar{p}_4 &amp;= \bar{p}_3 
\end{aligned}
\]</span>
The solutions <span class="math inline">\(\bar{p}_i\)</span> are called <strong>generalized eigenvectors</strong> of <span class="math inline">\(A\)</span>.</p>
</div>
<div id="simultaneous-diagonalization" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Simultaneous Diagonalization</h3>
<p>A diagonlizable matrix family that share the <strong>same eigenvectors</strong> is called <em>simultaneously diagonalizable</em>. This notion is complimentary to a family of similar matrices that are diagonalizable, share eigenvalues but not eigenvectors.</p>

<div class="definition">
<p><span id="def:simultaneous-diagonalizable" class="definition"><strong>Definition 4.4  (Simultaneously diagonalizable)  </strong></span>Two diagonalizable matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are said to be <em>simultaneously diagonalizable</em> if a <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(P\)</span> exists, such that <span class="math inline">\(P^{-1}AP\)</span> and <span class="math inline">\(P^{-1}BP\)</span> are diagonal matrices. In other words</p>
<span class="math display">\[
A =  P\Lambda_1P^{-1} \\
B = P\Lambda_2P^{-1}
\]</span>
</div>

<p>The geometric interpretation of simultaneously diagonalizable matrices is that they perform scaling in the same set of directions.</p>
<hr>

<div class="theorem">
<span id="thm:unnamed-chunk-6" class="theorem"><strong>Theorem 1.8  </strong></span>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are diagonalizable matrices, they are simultaneously diagonalizable if and only if they commute, such that <span class="math inline">\(AB\)</span> = <span class="math inline">\(BA\)</span>.
</div>

<p>This theorem is useful in identifying diagonalizable matrices with the same eigenvectors.</p>
<p>For example, the matrices</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 0
\end{bmatrix}
\quad 
\text{and}
\quad
\begin{bmatrix}
1 &amp; 1 \\
0 &amp; 0
\end{bmatrix}
\]</span>
are not simultaneously diagonalizable because they do not commute.</p>
</div>
<div id="cayley-hamilton-theorem" class="section level3">
<h3><span class="header-section-number">4.2.4</span> Cayley-Hamilton Theorem</h3>
<p>For any square matrix <span class="math inline">\(A_{n \times n}\)</span>, the characteristic polynomial of <span class="math inline">\(\lambda\)</span> is defined as</p>
<p><span class="math display">\[
\det(A - \lambda I)
\]</span></p>
<p>We can obtain a polynomial of matrix <span class="math inline">\(A\)</span> by substituting <span class="math inline">\(A\)</span> for <span class="math inline">\(\lambda\)</span>, and <span class="math inline">\(kI\)</span> for constant terms. For example, the matrix form of the polynomial <span class="math inline">\(3\lambda^2 + 2\lambda + 2\)</span> is <span class="math inline">\(3A^2 + 2A + 2I\)</span>.</p>

<div class="theorem">
<p><span id="thm:cayley-hamilton" class="theorem"><strong>Theorem 4.2  (Cayley-Hamilton Theorem)  </strong></span></p>
Let <span class="math inline">\(f(\lambda)\)</span> be the polynomial function of the characteristic polynomial <span class="math inline">\(\det(A - \lambda I)\)</span>, where <span class="math inline">\(A\)</span> is a square matrix. Then <span class="math inline">\(f(A)\)</span> evaluates to a zero matrix.
</div>

<div class="proof">
Proof
</div>
<p>Though the Caley Hamilton theorem <a href="eigenthings-and-quadratic-forms.html#thm:cayley-hamilton">4.2</a> applies to any square matrix <span class="math inline">\(A\)</span>. Our proof only address the case for diagonalizable matrices.</p>
<p>When <span class="math inline">\(A\)</span> is diagonalizable, the polynomial of <span class="math inline">\(A\)</span> takes the form</p>
<p><span class="math display">\[
f(A) = Pf(\Lambda)P^{-1}
\]</span>
Since <span class="math inline">\(f(\lambda) = \det(A - \lambda I)\)</span>, and the diagonal entries of <span class="math inline">\(\Lambda\)</span> are the eigenvalues of <span class="math inline">\(A\)</span>. Evaluate <span class="math inline">\(f(\lambda)\)</span> at each diagonal entry of <span class="math inline">\(\Lambda\)</span> will be zero. Thus <span class="math inline">\(f(A)\)</span> is a zero matrix.</p>
<p>A direct result derived from the Cayley Hamilton theorem is that for every invertible matrix <span class="math inline">\(A\)</span>, its inverse <span class="math inline">\(A^{-1}\)</span> can be represented as a polynomial of <span class="math inline">\(A\)</span> with degree <span class="math inline">\(d - 1\)</span>.</p>
<p><r></p>

<div class="proposition">
<span id="prp:unnamed-chunk-7" class="proposition"><strong>Proposition 4.1  (Polynomial representation of matrix inverse)  </strong></span>The <em>inverse</em> of an invertible square matrix <span class="math inline">\(A\)</span> is a polynomial of <span class="math inline">\(A\)</span> with degree at most <span class="math inline">\(d -1\)</span>.
</div>

<p>Since the constant term in the characteristic polynomial is the product of eigenvalues, which is nonzero for non-singular matrices. If the product of all eigenvalues are <span class="math inline">\(k\)</span>, we can write the Cayley-Hamilton matrix polynomial <span class="math inline">\(f(A)\)</span> in the form <span class="math inline">\(f(A) = A \cdot g(A) + kI\)</span>, where <span class="math inline">\(A \cdot g(A)\)</span> is obtained by factoring out <span class="math inline">\(A\)</span> from the d-degree matrix polynomial, leaving <span class="math inline">\(g(A)\)</span> with degree of <span class="math inline">\(d - 1\)</span>. Since <span class="math inline">\(f(A)\)</span> evaluates to zero, we have</p>
<p><span class="math display">\[
A \underbrace{\Big( - g(A) / k\Big)}_{A^{-1}} = I
\]</span>
Therefore, <span class="math inline">\(A^{-1}\)</span> is shown to be a polynomial of <span class="math inline">\(A\)</span>.</p>
</div>
</div>
<div id="symmetric-matrices" class="section level2">
<h2><span class="header-section-number">4.3</span> Symmetric Matrices</h2>
<p>A <em>square</em> matrix <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> is <em>symmetric</em> if <span class="math inline">\(A = A^{T}\)</span>, and <em>anti-symmetric</em> if <span class="math inline">\(A = - A^{T}\)</span>.</p>
<p>It can be shown that for any <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>, <span class="math inline">\(A + A^T\)</span> is symmetric and <span class="math inline">\(A - A^T\)</span> anti-symmetric. So any square matrix <span class="math inline">\(A\)</span> can be wrote as a sum of a symmetric matrix and an anti-symmetric matrix</p>
<p><span class="math display">\[
A = \frac{1}{2}(A + A^T) + \frac{1}{2}(A - A^T)
\]</span></p>
<p>It is common to denote the set of all symmetric matrices of size <span class="math inline">\(n\)</span> as <span class="math inline">\(\mathbb{S}^n\)</span>, and <span class="math inline">\(A \in \mathbb{S}^n\)</span> means <span class="math inline">\(A\)</span> is a symmetric <span class="math inline">\(n \times n\)</span> matrix.</p>
<p>Symmetric matrices have some nice properties about diagonalization.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-8" class="theorem"><strong>Theorem 4.3  </strong></span>If <span class="math inline">\(A\)</span> is symmetric, eigenvectors from distinct eigenvalues are <strong>orthogonal</strong>.
</div>

<div class="proof">
Proof
</div>
<p>Let <span class="math inline">\(\bar{v}_1\)</span> and <span class="math inline">\(\bar{v}_2\)</span> be eigenvectors that correspond to distinct eigenvalues <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>. Compute</p>
<p><span class="math display">\[
\begin{split}
\lambda_1\bar{v}_1 \cdot \bar{v}_2 &amp;= (\lambda_1\bar{v}_1)^T\bar{v}_2 \\
&amp;= (\bar{v}_1^TA^T)\bar{v}_2 \\
&amp;= \bar{v}_1^T(A\bar{v}_2) \\
&amp;= \bar{v}_1^T(\lambda_2\bar{v}_2) \\
&amp;= \lambda_2\bar{v}_1 \cdot \bar{v}_2
\end{split}
\]</span>
because <span class="math inline">\(\lambda_1 \not = \lambda_2\)</span>, <span class="math inline">\(\bar{v}_1 \cdot \bar{v}_2 = 0\)</span>.</p>
<p>For symmetric matrices <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> without <span class="math inline">\(n\)</span> distinct eigenvalues, it turns out that the dimension of the eigenspace for each <span class="math inline">\(\lambda_k\)</span> always equals the multiplicity of <span class="math inline">\(\lambda_k\)</span>. For this reason, if <span class="math inline">\(A\)</span> is a symmetric matrix we can always construct a orthonormal set <span class="math inline">\(\{\bar{q}_1 \;\; \cdots \;\; \bar{q}_n\}\)</span> from <span class="math inline">\(\{\bar{v}_1 \;\; \cdots \;\; \bar{v}_n\}\)</span> such that</p>
<p><span class="math display">\[
Q^{T} = 
\begin{bmatrix}
\bar{q}_1^T \\
\vdots \\ 
\bar{q}_n^T
\end{bmatrix}
= Q^{-1}
\]</span>
Recall that matrix <span class="math inline">\(A\)</span> with <span class="math inline">\(n\)</span> linearly independent eigenvectors is diagonalizable and can be written as</p>
<p><span class="math display">\[
A = P \Lambda P^{-1}
\]</span>
where <span class="math inline">\(P = [\bar{v}_1 \;\; \cdots \;\; \bar{v}_n]\)</span> and <span class="math inline">\(\Lambda\)</span> is a diagonal matrix with eigenvalues on its diagonal entries.</p>
<p>With symmetric matrices, <span class="math inline">\(\{\bar{v}_1, \cdots, \bar{v}_n\}\)</span> must be linearly independent and can be transformed into a orthonormal basis <span class="math inline">\(\{\bar{q}_1, \cdots, \bar{q}_n\}\)</span>. With orthogonal matrix <span class="math inline">\(Q =[\bar{q}_1 \;\; \cdots \;\; \bar{q}_n]\)</span>, we have</p>
<p><span class="math display" id="eq:orthogonal-diagonalization">\[\begin{equation}
\tag{4.2}
A = Q \Lambda Q^{T}
\end{equation}\]</span></p>
<p>Such matrix <span class="math inline">\(A\)</span> is said to be <strong>orthogonally diagonalizable</strong>.</p>
<p>We have seen that for symmetric matrix <span class="math inline">\(A\)</span>, Eq <a href="eigenthings-and-quadratic-forms.html#eq:orthogonal-diagonalization">(4.2)</a> always holds. We can also also verify that if <span class="math inline">\(A\)</span> is orthogonally diagonalizable then it is a symmetric matrix</p>
<p><span class="math display">\[
A^T = (Q \Lambda Q^{T})^T = (Q^T)^T\Lambda^TQ^T= Q \Lambda Q^{T}  = A
\]</span></p>

<div class="theorem">
<span id="thm:unnamed-chunk-9" class="theorem"><strong>Theorem 4.4  </strong></span>An <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is orthogonally diagonalizable if an only if <span class="math inline">\(A\)</span> is a symmetric matrix.
</div>

<div id="spectral-decomposition" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Spectral Decomposition</h3>
<p>For orthogonally diagonalizable matrix <span class="math inline">\(A\)</span>, we have</p>
<p><span class="math display">\[
A = Q \Lambda Q^{T} = [\bar{q}_1 \;\; \cdots \;\; \bar{q}_n] 
\begin{bmatrix}
\lambda_1 &amp; &amp; \\
 &amp; \ddots \\
 &amp; &amp; \lambda_n
\end{bmatrix}
\begin{bmatrix}
\bar{q}_1^T \\
\vdots \\
\bar{q}_n
\end{bmatrix}
\]</span></p>
<p>It follows that</p>
<p><span class="math display" id="eq:spectral-decomposition">\[\begin{equation}
\tag{4.3}
A = \lambda_1\bar{q}_1\bar{q}_1^T + \cdots + \lambda_1\bar{q}_n\bar{q}_n^T
\end{equation}\]</span></p>
<p>Eq <a href="eigenthings-and-quadratic-forms.html#eq:spectral-decomposition">(4.3)</a> is called the <strong>spectral decomposition</strong>, breaking <span class="math inline">\(A\)</span> into pieces of rank 1 matrix. It got this name because he set of eigenvalues of a matrix <span class="math inline">\(A\)</span> is sometimes called its <em>spectrum</em>.</p>
</div>
<div id="a-orthogonality" class="section level3">
<h3><span class="header-section-number">4.3.2</span> A-Orthogonality</h3>

<div class="definition">
<span id="def:unnamed-chunk-10" class="definition"><strong>Definition 4.5  (A-Orthogonality)  </strong></span>Column vector <span class="math inline">\(\bar{v}_i\)</span> and <span class="math inline">\(\bar{v}_j\)</span> are said to be <em>A-orthogonal</em> if <span class="math inline">\(\bar{v}_i^TA\bar{v}_j = 0\)</span> for some <span class="math inline">\(n \times n\)</span> invertible matrix <span class="math inline">\(A\)</span>.
</div>

<p>Similarly, a set of column vectors <span class="math inline">\(\bar{v}_1, ..., \bar{v}_n\)</span> is A-orthogonal, if and only if <span class="math inline">\(\bar{v}_i^TA\bar{v}_i = 0\)</span> for each pair of vectors.</p>
</div>
</div>
<div id="quadratic-forms" class="section level2">
<h2><span class="header-section-number">4.4</span> Quadratic Forms</h2>

<div class="definition">
<span id="def:unnamed-chunk-11" class="definition"><strong>Definition 4.6  (Quadratic form)  </strong></span>A <strong>quadratic form</strong> on <span class="math inline">\(\mathbb{R}^n\)</span> is a function <span class="math inline">\(Q\)</span> defined on <span class="math inline">\(\mathbb{R}^n\)</span> whose value at a vector <span class="math inline">\(\bar{x}\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span> can be computed by an expression of the form <span class="math inline">\(Q(\bar{x}) = \bar{x}^TA\bar{x}\)</span>, where <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> is a <strong>symmetric</strong> matrix. <span class="math inline">\(A\)</span> is called the matrix of the quadraticc form.
</div>

<p>There exists a one-to-one mapping between symmetric matrix <span class="math inline">\(A\)</span> and the quadratic form. Consider the <span class="math inline">\(3 \times 3\)</span> case:</p>
<p><span class="math display">\[
\bar{x} =
\begin{bmatrix}
x_1 \\
x_3 \\
x_3 \\
\end{bmatrix}
, \;\; A = 
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33} 
\end{bmatrix} \\
\]</span></p>
<p><span class="math display">\[
\begin{split}
\bar{x}^TA\bar{x} &amp;= 
[x_1 \;\; x_2 \;\; x_3]
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33} 
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_3 \\
x_3 \\
\end{bmatrix} \\
&amp;= a_{11}x_1^2 + a_{22}x_2^2 + a_{33}x_3^2 + \\
&amp; \quad(a_{12} + a_{21})x_1x_2 + (a_{13} + a_{31})x_1x_3 + (a_{23} + a_{32})x_2x_3 
\end{split} 
\tag{1}
\]</span>
Since <span class="math inline">\(A\)</span> is symmetric, we have <span class="math inline">\(a_{ij} = a_{ji}\)</span>, thus</p>
<p><span class="math display">\[
\bar{x}^TA\bar{x}  = a_{11}x_1^2 + a_{22}x_2^2 + a_{33}x_3^2 + 2a_{12}x_1x_2 + 2a_{13}x_1x_3 + 2a_{23}x_2x_3 \tag{2} 
\]</span>
This verifies that <span class="math inline">\(\bar{x}^TA\bar{x}\)</span> when <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> is symmetric does result in a quadratic function of <span class="math inline">\(n\)</span> variables. Conversely, any quadratic function of <span class="math inline">\(n\)</span> variables, like shown in <span class="math inline">\((2)\)</span>, can be expressed in terms of <span class="math inline">\(\bar{x}^TA\bar{x}\)</span> with unique choice of symmetric matrix <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>.</p>
<div id="change-of-variable" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Change of Variable</h3>
<p>If <span class="math inline">\(\bar{x}\)</span> is a variable vector in <span class="math inline">\(\mathbb{R}^n\)</span>, then a <em>change of variable</em> is an equation of the form</p>
<p><span class="math display">\[
\begin{aligned}
\bar{x} &amp;= P\bar{y} \\
\text{or equivalently} \quad \bar{y} &amp;= P^{-1}\bar{x}
\end{aligned}
\]</span>
where <span class="math inline">\(P\)</span> is any invertible matrix <span class="math inline">\(\in \mathbb{R}^{n \times n}\)</span></p>

<div class="theorem">
<p><span id="thm:principal-axes" class="theorem"><strong>Theorem 4.5  (The Principal Axes Theorem)  </strong></span>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n \times n\)</span> symmetric matrix. Then there exists an orthogonal change of variable, <span class="math inline">\(\bar{x} = Q\bar{y}\)</span>, this transform the quadratic form <span class="math inline">\(\bar{x}^TA\bar{x}\)</span> into a quadratic form <span class="math inline">\(\bar{y}^T\Lambda\bar{y}\)</span> with no cross-product term.</p>
<p><span class="math inline">\(Q\)</span> is constructed with <span class="math inline">\(A\)</span>â€™s orthonormal eigenvectors <span class="math inline">\(\bar{q}_1, ..., \bar{q}_n\)</span>. According to theorem <a href="eigenthings-and-quadratic-forms.html#eq:orthogonal-diagonalization">(4.2)</a>:</p>
<span class="math display">\[
\bar{x}^TA\bar{x} = (Q\bar{y})^TA(Q\bar{y}) = \bar{y}^TQ^{T}AQ\bar{y} = \bar{y}^T \Lambda \bar{y}
\]</span>
</div>

<p>The principal axes theorem <a href="eigenthings-and-quadratic-forms.html#thm:principal-axes">4.5</a> shows that if <span class="math inline">\(A\)</span> is diagonalizable, quadratic form <span class="math inline">\(\bar{x}^TA\bar{x}\)</span> can be reexpressed into the form <span class="math inline">\(\lambda_1y_1^2 + \lambda_2y_2^2 + \cdots + \lambda_ny_n^2\)</span> with change of variables <span class="math inline">\(\bar{x} = Q\bar{y}\)</span>.</p>
</div>
<div id="classification-of-quadratic-forms" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Classification of Quadratic Forms</h3>
<ul>
<li><p>A symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is <strong>positive definite</strong> (PD) if for all non-zero vectors <span class="math inline">\(\bar{x} \in \mathbb{R}^n,\; \bar{x}^TA\bar{x} &gt; 0\)</span>. We can denote positive definite matrix <span class="math inline">\(A\)</span> as <span class="math inline">\(A \succ 0\)</span> (or <span class="math inline">\(A &gt; 0\)</span>). The set of all positive definite matrices is denoted as <span class="math inline">\(\mathbb{S}_{++}^n\)</span></p></li>
<li><p>A symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is <strong>positive semidefinite</strong> (PSD) if for all non-zero vectors <span class="math inline">\(\bar{x} \in \mathbb{R}^n,\; \bar{x}^TA\bar{x} \ge 0\)</span>. We can denote positive definite matrix <span class="math inline">\(A\)</span> as <span class="math inline">\(A \succeq 0\)</span> (or <span class="math inline">\(A \ge 0\)</span>). The set of all positive semidefinite matrices is denoted as <span class="math inline">\(\mathbb{S}_{+}^n\)</span></p></li>
<li><p>A symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is <strong>negative definite</strong> (ND), denoted by <span class="math inline">\(A \prec 0\)</span> (or <span class="math inline">\(A &lt; 0\)</span>), if for all non-zero vectors <span class="math inline">\(\bar{x} \in \mathbb{R}^n,\; \bar{x}^TA\bar{x} &lt; 0\)</span>.</p></li>
<li><p>Similarly, a symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is <strong>negative semidefinite</strong> (NSD), denoted by <span class="math inline">\(A \preceq 0\)</span> (or <span class="math inline">\(A \le 0\)</span>), if for all non-zero vectors <span class="math inline">\(\bar{x} \in \mathbb{R}^n,\; \bar{x}^TA\bar{x} \le 0\)</span>.</p></li>
<li><p>Finally, a symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is <strong>indefinite</strong>, if it is neither positive semidefinite or negative semidefinite. In other words, if there exists <span class="math inline">\(\bar{x}, \bar{x}&#39;, \in \mathbb{R}^{n}\)</span> such taht <span class="math inline">\(\bar{x}^TA\bar{x} &gt; 0\)</span> and <span class="math inline">\(\bar{x&#39;}^TA\bar{x}&#39; &gt; 0\)</span></p></li>
</ul>

<div class="rmdnote">
<p>Note that when talking about <span class="math inline">\(A\)</span> being PD, PSD, ND, NSD or indefinite, <span class="math inline">\(A\)</span> is always assumed to be <strong>symmetric</strong>.</p>
Also, if <span class="math inline">\(A\)</span> is positive definite, then <span class="math inline">\(âˆ’A\)</span> is negative definite and viceversa. Likewise, if <span class="math inline">\(A\)</span> is positive semidefinite then <span class="math inline">\(âˆ’A\)</span> is negative semidefinite and vice versa. If <span class="math inline">\(A\)</span> is indefinite, then so is <span class="math inline">\(âˆ’A\)</span>.
</div>

<p>From theorem <a href="eigenthings-and-quadratic-forms.html#thm:principal-axes">4.5</a>, we know that the sign of eigenvalues are closely related to classifications of symmetric matrices here. Take positive definite matrices for example, the following statements of <span class="math inline">\(A\)</span> are equivalent:</p>
<ul>
<li><p>For any <span class="math inline">\(\bar{x} \in \mathbb{R}^n, \; \bar{x}^TA\bar{x} &gt; 0\)</span></p></li>
<li><p>Let <span class="math inline">\(\lambda_i, \; i = 1, ..., n\)</span> be <span class="math inline">\(A\)</span>â€™s eigenvalues, <span class="math inline">\(\lambda_i &gt; 0\)</span></p></li>
<li><p>All pivots are <span class="math inline">\(&gt; 0\)</span></p></li>
<li><p>All leading determinants of <span class="math inline">\(A &gt; 0\)</span></p></li>
</ul>
<p>For the last criterion, leading determinant is the determinant of the top-left <span class="math inline">\(k \times k\)</span> lock of <span class="math inline">\(A\)</span> for all <span class="math inline">\(1 \le k \le n\)</span>. There is formal theorem on this</p>

<div class="theorem">
<span id="thm:sylvester-criterion" class="theorem"><strong>Theorem 4.6  (Sylvesterâ€™s Criterion)  </strong></span>A symmetric matrix <span class="math inline">\(A\)</span> is positive definite if and only if the determinant of the top-left <span class="math inline">\(k \times k\)</span> block of <span class="math inline">\(A\)</span> is strictly positive for all <span class="math inline">\(1 \le k \le n\)</span>
</div>

<p>Sylvesterâ€™s criterion is more complicated for positive semidefinite matrices, we have to check more matrices rather than just the top-left one.</p>
<p><br></p>
<p>Classification of <span class="math inline">\(A \in \mathbb{S}^{n}\)</span> by its eigenvalue can be applied in general.</p>
<hr>

<div class="theorem">
<p><span id="thm:unnamed-chunk-13" class="theorem"><strong>Theorem 4.7  (Quadratic forms and eigenvalues)  </strong></span>Let <span class="math inline">\(A \in \mathbb{S}^{n}\)</span>. Then the quadratic form <span class="math inline">\(\bar{x}^TA\bar{x}\)</span> and <span class="math inline">\(A\)</span> is:</p>
<ul>
<li><p>positive definite if and only if the eigenvalues of <span class="math inline">\(A\)</span> are all positive</p></li>
<li><p>negative definite if and only if the eigenvalues of <span class="math inline">\(A\)</span> are all negative</p></li>
<li><p>indefinite if and only if <span class="math inline">\(A\)</span> has both positive and negative eigenvalues</p>
</div></li>
</ul>
<hr>

<div class="corollary">
<span id="cor:unnamed-chunk-14" class="corollary"><strong>Corollary 4.3  </strong></span>Given positive definite matrices <span class="math inline">\(A, B \in \mathbb{S}^n\)</span> and <span class="math inline">\(\alpha \in \mathbb{R}\)</span>, the following results <strong>remain to be positive definite</strong>.
</div>

<ul>
<li><p>Scalar multiplication of PD matrices <span class="math inline">\(\alpha A\)</span> are PD matrices</p></li>
<li><p>The sum of PD matrices <span class="math inline">\(A +B\)</span> are PD matrices</p></li>
<li><p>If a PD matrix is invertible, its inverse <span class="math inline">\(A^{-1}\)</span> is also PD.</p></li>
<li><p>Similar matrix of a PD matrix is PD.</p></li>
</ul>
<hr>

<div class="corollary">
<span id="cor:ata-pd" class="corollary"><strong>Corollary 4.4  </strong></span>Given any matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> are always positive semidefinite matrices
</div>

<div class="proof">
Proof
</div>
<p>By definition, <span class="math inline">\(A^TA\)</span> is a positive semidefinite matrix if for any <span class="math inline">\(\bar{x} \in \mathbb{R}^n\)</span>, the quadratic form <span class="math inline">\(\bar{x}^T(A^TA)\bar{x} \ge 0\)</span>.</p>
<p><span class="math display">\[
\begin{split}
\bar{x}^T(A^TA)\bar{x} &amp;= (\bar{x}^TA^T)(A\bar{x}) \\
&amp;= (A\bar{x})^T(A\bar{x}) \\
&amp;= \|A\bar{x}\|^2
\end{split}
\]</span>
It turns out that the result is the square of the 2-norm of <span class="math inline">\(A\bar{x}\)</span> (nonnegative). This also tells <span class="math inline">\(A^TA\)</span> is positive definite when <span class="math inline">\(\bar{x} \not\subseteq \mathcal{N}(A)\)</span></p>
<p>Similarly, the quadratic form for <span class="math inline">\(AA^T\)</span> can be refactored in to the standard norm of <span class="math inline">\(A^T\bar{x}\)</span>.</p>
<hr>

<div class="corollary">
<span id="cor:ridge-invertible" class="corollary"><strong>Corollary 4.5  </strong></span><span class="math inline">\(A^TA +\lambda I\)</span> and <span class="math inline">\(AA^T + \lambda I\)</span> are always positive definite and invertible for <span class="math inline">\(\lambda &gt; 0\)</span>
</div>

<div class="proof">
Proof
</div>
<p>From the previous corollary <a href="eigenthings-and-quadratic-forms.html#cor:ata-pd">4.4</a> we know that <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> are positive semidefinite, and that they have the same nonzero eigenvalues from corollary <a href="eigenthings-and-quadratic-forms.html#cor:same-nonzero">4.1</a>. According to Section <a href="eigenthings-and-quadratic-forms.html#additional-properties-of-eigenvalues-and-eigenvectors">4.1.1</a>, eigenvalues for <span class="math inline">\(P(A)\)</span> are <span class="math inline">\(P(\lambda)\)</span> for polynomial function <span class="math inline">\(P\)</span>. Therefore, <span class="math inline">\(A^TA +\lambda I\)</span> and <span class="math inline">\(AA^T + \lambda I\)</span> share a positive set of <span class="math inline">\(n\)</span> eigenvalues <span class="math inline">\(\lambda_1 + \lambda, ..., \lambda_r + \lambda, \lambda, ..., \lambda\)</span>, so they are PD and invertible.</p>
</div>
<div id="gershgorin-discs-and-diagonal-dominance" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Gershgorin Discs and Diagonal Dominance</h3>
<p>This section introduces one more criterion for positive definite matrices. We first present the Gershgorin Disc Theorem, which is more convenient to be expressed in terms of complex matrices.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-15" class="theorem"><strong>Theorem 4.8  (Gershgorin Disc Theorem)  </strong></span>Let <span class="math inline">\(A \in \mathcal{M}_n(\mathbb{C})\)</span> and define the following objects</p>
<ul>
<li><p><span class="math inline">\(r_i = \sum_{j \not = i}|a_{ij}|\)</span> (the sum of the off-diagonal entries of the <span class="math inline">\(i\)</span>-th row of <span class="math inline">\(A\)</span>)</p></li>
<li><p><span class="math inline">\(D(a_{ii}, r_i)\)</span> is the closed disc in the complex plane centered at <span class="math inline">\(a_{ii}\)</span> with radius <span class="math inline">\(r_i\)</span>.</p></li>
</ul>
<p>Then every eigenvalue of <span class="math inline">\(A\)</span> is in at least one of the <span class="math inline">\(D(a_{ii}, r_i)\)</span> (called <em>Gershigorin discs</em>)</p>
</div>

<p>As an illustration taken from Wikipedia, the following figure presents 4 Gershgorin discs of the matrix</p>
<p><span class="math display">\[
\begin{bmatrix}
10 &amp; -1 &amp; 0 &amp; 1 \\
0.2 &amp; 8 &amp; 0.2 &amp; 0.2 \\
1 &amp; 1 &amp; 2 &amp; 1 \\
-1 &amp; -1 &amp; -1 &amp; -11
\end{bmatrix}
\]</span></p>
<p>In this case, all elements and eigenvalues are real. Therefore, Gershgorin discs are circles centered at the real number line.</p>
<p><img src="images/gershgorin-disc.png" width="640" style="display: block; margin: auto;" /></p>

<div class="proof">
Proof
</div>

<p>Let <span class="math inline">\(\lambda\)</span> be an eigenvalue of <span class="math inline">\(A\)</span> with associated eigenvector <span class="math inline">\(\bar{v}\)</span>. Suppose the largest entry of <span class="math inline">\(\bar{v}\)</span> is
<span class="math inline">\(v_i\)</span>, we scale <span class="math inline">\(\bar{v}\)</span> such that <span class="math inline">\(v_i = 1\)</span> and all other elements less than <span class="math inline">\(1\)</span>. Note <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(i\)</span> are arbitrary, for example, the associated eigenvector of the 3rd eigenvalue may have the largest entry at <span class="math inline">\(v_5\)</span>.</p>
<p>By the definition of matrix multiplication, we know <span class="math inline">\([A \bar{v}]_i = \sum_{j = 1}^{n}a_{ij}v_j = a_{ij}v_i + \sum_{j \not = i}a_{ij}v_j\)</span>. Also <span class="math inline">\(\lambda\)</span> is an eigenvalue and <span class="math inline">\(v_i = 1\)</span>, thus <span class="math inline">\([A\bar{v}]_i = \lambda v_ia\)</span>. Then</p>
<p><span class="math display">\[
a_{ij}\underbrace{v_i}_{1} + \sum_{j \not = i}a_{ij}v_j =  \lambda\underbrace{v_i}_{1}
\]</span></p>
<p><span class="math display">\[
\lambda - a_{ii} = \sum_{j \not = i}a_{ij}v_j
\]</span></p>
<p>Take absolute values on both sides and use the triangle equality</p>
<p><span class="math display">\[
|\lambda - a_{ii}| = |\sum_{j \not = i}a_{ij}v_j| \le \sum_{j \not = i} |a_{ij}||v_j| \qquad (v_j \le 1) 
\]</span></p>
<p><span class="math display">\[
|\lambda - a_{ii}| \le  \sum_{j \not = i} |a_{ij}| = r_i
\]</span>
In other words, for any eigenvalue <span class="math inline">\(\lambda\)</span>, the distance between <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(a_{ii}\)</span> is less than <span class="math inline">\(r_i\)</span>, where <span class="math inline">\(i\)</span> is the index at which <span class="math inline">\(\lambda\)</span>â€™s associated eigenvector take its largest value.</p>
<p>For diagonal entries, all <span class="math inline">\(r_i = 0\)</span>, thus their eigenvalues are exactly diagonal entries.</p>
<p>In cases where the off diagonal entries of <span class="math inline">\(A\)</span> are very small, then <span class="math inline">\(A\)</span>â€™s eigenvalue will be very close to diagonal entries. This leads to the notion of diagonal dominance.</p>

<div class="theorem">
<p><span id="thm:diagonal-dominance" class="theorem"><strong>Theorem 4.9  (Diagonal dominant matrix)  </strong></span>
Suppose that <span class="math inline">\(A \in \mathcal{M}_n{(\mathbb{C})}\)</span>, then <span class="math inline">\(A\)</span> is called</p>
<ol style="list-style-type: decimal">
<li><p><strong>diagonally dominant</strong> if <span class="math inline">\(|a_{ii}| \ge \sum_{j \not = i}|a_{ij}|\)</span> for all <span class="math inline">\(1 \le i \le n\)</span> and</p></li>
<li><p><strong>strictly diagonally dominant</strong> if <span class="math inline">\(|a_{ii}| &gt; \sum_{j \not = i}|a_{ij}|\)</span> for all <span class="math inline">\(1 \le i \le n\)</span></p></li>
</ol>
</div>

<p>A strictly diagonally dominant matrix is non-singular, because all of its Gershgorin discs do not touch zero. This result is known the <em>Levy-Desplanques theorem</em></p>

<div class="corollary">
<p><span id="cor:unnamed-chunk-18" class="corollary"><strong>Corollary 4.6  (non-negative diagonal entries and diagonal dominance implies PSD)  </strong></span></p>
<p>Suppose that <span class="math inline">\(A \in \mathcal{M}_n{(\mathbb{C})}\)</span> has <strong>non-negative</strong> diagonal entries</p>
<ul>
<li><p>If <span class="math inline">\(A\)</span> is diagonally dominant then it is positive semidefinite</p></li>
<li><p>If <span class="math inline">\(A\)</span> is strictly dominant then it is positive definite</p>
</div></li>
</ul>
<p>Note that this is a one-way theorem, unlike the criterion using eigenvalues, pivots and leading matrices. A PD matrix may not be diagonally dominant.</p>
</div>
</div>
<div id="cholesky-factorization" class="section level2">
<h2><span class="header-section-number">4.5</span> Cholesky Factorization</h2>

<div class="lemma">
<span id="lem:unnamed-chunk-19" class="lemma"><strong>Lemma 4.1  </strong></span>A symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is positive semidefinite if and only if it can be expressed in the gram matrix form <span class="math inline">\(B^TB\)</span> of some matrix <span class="math inline">\(B\)</span>.
</div>

<p><br></p>
<p>The previous corollary <a href="eigenthings-and-quadratic-forms.html#cor:ata-pd">4.4</a> shows that if <span class="math inline">\(A = B^TB\)</span> then it is positive definite. Conversely, if <span class="math inline">\(A\)</span> is PSD (or PD), we have <span class="math inline">\(A = Q \Lambda Q^T\)</span> where <span class="math inline">\(\Lambda\)</span>â€™s diagonal entries are all nonnegative. Then we can set <span class="math inline">\(\Lambda ^{1/2} = \Sigma\)</span> and <span class="math inline">\(B = (Q\Sigma)^T\)</span>. Then <span class="math inline">\(A = Q\Sigma^2Q^T = (Q\Sigma)(Q\Sigma)^T = B^TB\)</span>.</p>
<p>Note that we could also have stated this lemma using <span class="math inline">\(BB^T\)</span> instead of <span class="math inline">\(B^TB\)</span>, and the proof is similar.</p>
<p>This lemma is inspiring in that for every PD matrix <span class="math inline">\(A \in \mathbb{R}^n\)</span>, there exists factorization <span class="math inline">\(A = BB^T\)</span>.</p>
<p>In fact, this factorization is not unique. We can use an orthogonal matrix <span class="math inline">\(Q\)</span> to create an additional orthogonal factorization <span class="math inline">\(A = BB^T = B(QQ^T)B^T = (BQ)(BQ)^T\)</span>. And let <span class="math inline">\(BQ\)</span> be our new <span class="math inline">\(B\)</span>, we get another factorization.</p>
<p>Since the initial <span class="math inline">\(B = P\Lambda^{1/2}\)</span> is full rank, with an appropriate choice of <span class="math inline">\(Q\)</span>, we can turn the <span class="math inline">\(PB\)</span> into an <strong>lower triangular matrix</strong> <span class="math inline">\(L\)</span> such that <span class="math inline">\(A = LL^T\)</span>. This is essentailly a coordinate transformation in Section <a href="vector-spaces.html#change-of-basis">2.6.1</a></p>
<p>The uniqueness of this factorization with lower triangular matrix <span class="math inline">\(L\)</span> can be proved with induction. The decomposition of PD matrices into the product of an lower triangular matrix and its transpose is called the <strong>Cholesky factorization</strong>, in this factorization</p>
<p><span class="math display">\[
A = LL^T
\]</span></p>
<p>To make it more clear</p>
<p><span class="math display">\[
\begin{bmatrix}
a_{11} &amp; \cdots &amp; a_{1n} \\
\vdots &amp; \ddots &amp; \vdots \\
a_{n1} &amp; \cdots &amp; a_{nn}
\end{bmatrix}
= 
\begin{bmatrix}
l_{11} &amp; \cdots &amp; 0\\
\vdots &amp; \ddots &amp; \vdots \\
l_{n1} &amp; \cdots &amp; l_{nn}
\end{bmatrix}
\begin{bmatrix}
l_{11} &amp; \cdots &amp; l_{n1} \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; l_{nn}
\end{bmatrix}
\]</span></p>
<p>Since <span class="math inline">\(L\)</span> is lower triangular, we can solve <span class="math inline">\(L\)</span> from <span class="math inline">\(A = LL^T\)</span> with a system of equations that can be easily solved using back-substitution. For example, <span class="math inline">\(l_{11} = \sqrt{a_{11}}\)</span>, and <span class="math inline">\(a_{i1} / l_{11}\)</span>.</p>
<p>The Cholesky factorization is a special case of LU decomposition</p>
</div>
<div id="rayleigh-quotients" class="section level2">
<h2><span class="header-section-number">4.6</span> Rayleigh Quotients</h2>
<p>Let <span class="math inline">\(A \in \mathbb{S}^n\)</span> and <span class="math inline">\(\bar{x} \in \mathbb{R}^n\)</span>, <strong>Rayleigh quotient</strong> is defined as</p>
<p><span class="math display">\[
R_{A}(\bar{x}) = \frac{\bar{x}^TA\bar{x}}{\bar{x}^T\bar{x}}
\]</span>
The Rayleigh quotient has some nice properties:</p>
<ul>
<li><p>scale invariance: for any vector <span class="math inline">\(\bar{x} \not= 0\)</span> and any scalar <span class="math inline">\(\alpha \not= 0\)</span>, <span class="math inline">\(R_{A}(\bar{x}) = R_{A}(\alpha\bar{x})\)</span></p></li>
<li><p>If <span class="math inline">\(\bar{x}\)</span> is a eigenvector of <span class="math inline">\(A\)</span> with eigenvalue <span class="math inline">\(\lambda\)</span>, then <span class="math inline">\(R_{A}(\bar{x}) = \lambda\)</span></p></li>
<li><p>The Rayleigh quotient is bounded by the largest and smallest eigenvalue of <span class="math inline">\(A\)</span>, i.e.Â </p></li>
</ul>
<p><span class="math display">\[
\lambda_{\text{min}}(A) \le R_{A}(\bar{x}) \le \lambda_{\text{max}}(A)
\]</span></p>
<div class="proof">
Proof
</div>
<p>Since the Rayleigh quotient does not depend on the 2-norm of vector <span class="math inline">\(\bar{x}\)</span>, we may assume a unit vector <span class="math inline">\(\bar{x}^T\bar{x} = 1\)</span>, and Rayleigh quotient simplifies to the quadratic form <span class="math inline">\(\bar{x}^TA\bar{x}\)</span>.</p>
<p>Next, orthogonally diagonalize <span class="math inline">\(A\)</span> as <span class="math inline">\(Q \Lambda Q\)</span>, we know that when <span class="math inline">\(\bar{x} = Q \bar{y}\)</span>:</p>
<p><span class="math display">\[
\bar{x}^TA\bar{x} = \bar{y}^T \Lambda \bar{y} \tag{1}
\]</span>
Also</p>
<p><span class="math display">\[
1= \bar{x}^T\bar{x} = (Q\bar{y})^T Q\bar{y} = \bar{y}^TQ^TQ\bar{y} = \bar{y}^T\bar{y} \tag{2}
\]</span></p>
<p>Expand <span class="math inline">\(\bar{y}^T \Lambda \bar{y}\)</span> in (1) we get</p>
<p><span class="math display">\[
\bar{x}^TA\bar{x} = \lambda_1y_1^2 + \lambda_2y_2^2 + \cdots + \lambda_ny_n^2 \tag{3}
\]</span>
where <span class="math inline">\(\{\lambda_1, ..., \lambda_n\}\)</span> are diagonal entries of <span class="math inline">\(\Lambda\)</span> and eigenvalues of <span class="math inline">\(A\)</span>. Let us suppose that the set <span class="math inline">\(\{\lambda_1, \lambda_2, \cdots, \lambda_n\}\)</span> has already been ordered descendingly, so that <span class="math inline">\(\lambda_1 &gt; \lambda_2 &gt; \cdots &gt; \lambda_n\)</span>.</p>
<p>We can obtain the inequality from (3) and the order of eigenvalues:</p>
<p><span class="math display">\[
\begin{split}
\bar{x}^TA\bar{x} &amp;= \lambda_1y_1^2 + \lambda_2y_2^2 + \cdots + \lambda_ny_n^2 \\
&amp;\le \lambda_1y_1^2 + \underbrace{\lambda_1y_2^2 + \cdots + \lambda_1y_n^2}_{\lambda_1 \text{ is the greatest eigenvalue}} \\ 
&amp;\le \lambda_1(\bar{y}^T\bar{y}) \\
&amp;= \lambda_1
\end{split}
\]</span>
The equation reach equality if and only if <span class="math inline">\([y_1, y_2, \cdots, y_n] = [1, 0, \cdots, 0]\)</span>. Since <span class="math inline">\(\bar{x} = Q\bar{y}\)</span>, we have</p>
<p><span class="math display">\[
\bar{x} = 
\begin{bmatrix}
\bar{q}_1 &amp; \bar{q}_2 &amp; \cdots &amp; \bar{q}_n
\end{bmatrix}
\begin{bmatrix}
1 \\
0 \\
\vdots \\
0
\end{bmatrix}
= \bar{q}_1
\]</span>
Similarly, the minimum of the Rayleigh quotient will be <span class="math inline">\(\lambda_n\)</span>, with <span class="math inline">\(\bar{x} = \bar{q}_n\)</span>.</p>
<hr>
<p>From the optimization perspective, the bound of Rayleigh quotient amounts to a constrained optimization problem</p>
<p><span class="math display">\[
\begin{aligned}
\text{objective function} &amp;:  \bar{x}^TA\bar{x}\\
\text{subject to}&amp;: \bar{x}^T\bar{x} = 1 
\end{aligned}
\]</span>
The maximum and minimum of the objective function are <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_n\)</span>, with <span class="math inline">\(\bar{x}\)</span> being <span class="math inline">\(\bar{q}_1\)</span> and <span class="math inline">\(\bar{q}_n\)</span> respectively.</p>
<p>If we add more constraints, for example, that <span class="math inline">\(\bar{x}\)</span> should be orthogonal to <span class="math inline">\(\bar{q}_1\)</span>, then <span class="math inline">\(\bar{x}^TA\bar{x}\)</span> has maximum <span class="math inline">\(\lambda_2\)</span> attained at <span class="math inline">\(\bar{x} = \lambda_2\)</span></p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-20" class="theorem"><strong>Theorem 4.10  </strong></span>Let <span class="math inline">\(A \in \mathbb{S}^n\)</span> with orthogonal diagonalization <span class="math inline">\(A = Q\Lambda Q^T\)</span>, where the entries on the diagonal of <span class="math inline">\(\Lambda\)</span> are arranged so that <span class="math inline">\(\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_n\)</span>. Then for <span class="math inline">\(k = 2, ...\)</span>, the maximum of value of <span class="math inline">\(\bar{x}^T A \bar{x}\)</span> subject to constraints</p>
<span class="math display">\[
\bar{x}^T\bar{x} =  1, \;\; \bar{x}^T\bar{q}_1 = 0, \;\; \dots \;\;, \bar{x}^T\bar{q}_{k-1} = 0
\]</span>
is the eigenvalue <span class="math inline">\(\lambda_k\)</span>, and this maximum is attained at <span class="math inline">\(\bar{x} = \bar{q}_k\)</span>
</div>

<div class="proof">
Proof
</div>
<p>From <span class="math inline">\(\bar{x} = P\bar{y}\)</span> we know that</p>
<p><span class="math display">\[
\bar{x} = y_1\bar{q}_1 + \cdots + + y_{k-1}\bar{q}_{k-1} + y_k\bar{q}_k + \cdots +  y_{n}\bar{q}_n 
\]</span>
Left multiply by <span class="math inline">\(\bar{q}_1^T\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\bar{q}_1^T\bar{x} &amp;= y_1\bar{q}_1^T\bar{q}_1 + \cdots + + y_{k-1}\bar{q}_1^T\bar{q}_{k-1} + y_k\bar{q}_1^T\bar{q}_k + \cdots +  y_{n}\bar{q}_1^T\bar{q}_n  \\
&amp;= y_1\bar{q}_1^T\bar{q}_1 \\
&amp;= y_1
\end{aligned} 
\]</span>
Since <span class="math inline">\(\bar{q}_1^T\bar{x} = \bar{x}^T\bar{q}_1 = 0\)</span>, we have <span class="math inline">\(y_1 = 0\)</span>. Similarly, <span class="math inline">\(y_2 = \cdots = y_{k-1} = 0\)</span>, and <span class="math inline">\(\bar{y}\)</span> becomes <span class="math inline">\([0 \;\; \cdots \;\; 0 \;\; y_{k} \;\; \cdots \;\; y_n]\)</span>. And the inequality now becomes:</p>
<p><span class="math display">\[
\begin{split}
\bar{x}^TA\bar{x} &amp;= \lambda_1y_1^2 + \lambda_2y_2^2 + \cdots + \lambda_ny_n^2 \\
&amp;= \lambda_ky_k^2 + \cdots + \lambda_ny_n^ 2 \\
&amp;\le \lambda_ky_k^2 + \cdots + \lambda_ky_n^2 \\ 
&amp;\le \lambda_k(\bar{y}^T\bar{y}) \\
&amp;= \lambda_k
\end{split}
\]</span></p>
<p>Itâ€™s easy to see that <span class="math inline">\(\bar{x}^TA\bar{x}\)</span> gets its maximum <span class="math inline">\(\lambda_k\)</span> when <span class="math inline">\(y_k = 0\)</span> and other weights being zero. So the solution <span class="math inline">\(\bar{x}\)</span> can be solved as</p>
<p><span class="math display">\[
\begin{split}
\bar{x} &amp;= 
\begin{bmatrix}
\bar{q}_1 &amp;  \cdots &amp; \bar{q}_k &amp;   \bar{q}_{k+1} &amp; \cdots &amp;\bar{q}_n
\end{bmatrix}
\begin{bmatrix}
0 \\
\vdots \\
\underbrace{1}_{k\text{th weight}} \\
0 \\
\vdots \\
0
\end{bmatrix} \\
&amp;= \bar{q}_k
\end{split}
\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="orthogonality.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="singular-value-decomposition.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/enixam/math-foundations/edit/master/eigen-quadratic.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
