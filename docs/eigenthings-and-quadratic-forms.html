<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Eigenthings and quadratic forms | Mathematical Notes for Machine Learning</title>
  <meta name="description" content="Chapter 4 Eigenthings and quadratic forms | Mathematical Notes for Machine Learning" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Eigenthings and quadratic forms | Mathematical Notes for Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="enixam/math-foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Eigenthings and quadratic forms | Mathematical Notes for Machine Learning" />
  
  
  

<meta name="author" content="Qiushi Yan" />


<meta name="date" content="2020-07-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="orthogonality.html"/>
<link rel="next" href="matrix-calculus.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Math foundations in Machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Linear Algebra</b></span></li>
<li class="chapter" data-level="1" data-path="matrix-basics.html"><a href="matrix-basics.html"><i class="fa fa-check"></i><b>1</b> Matrix basics</a><ul>
<li class="chapter" data-level="1.1" data-path="matrix-basics.html"><a href="matrix-basics.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.1</b> Matrix multiplication</a></li>
<li class="chapter" data-level="1.2" data-path="matrix-basics.html"><a href="matrix-basics.html#elemetary-matrix-and-row-operations"><i class="fa fa-check"></i><b>1.2</b> Elemetary matrix and row operations</a></li>
<li class="chapter" data-level="1.3" data-path="matrix-basics.html"><a href="matrix-basics.html#lu-factorization"><i class="fa fa-check"></i><b>1.3</b> LU factorization</a></li>
<li class="chapter" data-level="1.4" data-path="matrix-basics.html"><a href="matrix-basics.html#determinants"><i class="fa fa-check"></i><b>1.4</b> Determinants</a><ul>
<li class="chapter" data-level="1.4.1" data-path="matrix-basics.html"><a href="matrix-basics.html#cofactor-expansion"><i class="fa fa-check"></i><b>1.4.1</b> Cofactor expansion</a></li>
<li class="chapter" data-level="1.4.2" data-path="matrix-basics.html"><a href="matrix-basics.html#geometric-interpretation-of-determinant"><i class="fa fa-check"></i><b>1.4.2</b> Geometric interpretation of determinant</a></li>
<li class="chapter" data-level="1.4.3" data-path="matrix-basics.html"><a href="matrix-basics.html#properties-of-determinant"><i class="fa fa-check"></i><b>1.4.3</b> Properties of determinant</a></li>
<li class="chapter" data-level="1.4.4" data-path="matrix-basics.html"><a href="matrix-basics.html#cramers-rule"><i class="fa fa-check"></i><b>1.4.4</b> Cramerâ€™s rule</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="matrix-basics.html"><a href="matrix-basics.html#trace"><i class="fa fa-check"></i><b>1.5</b> Trace</a></li>
<li class="chapter" data-level="1.6" data-path="matrix-basics.html"><a href="matrix-basics.html#inverse-of-a-matrix"><i class="fa fa-check"></i><b>1.6</b> Inverse of a matrix</a></li>
<li class="chapter" data-level="1.7" data-path="matrix-basics.html"><a href="matrix-basics.html#matrix-multiplication-as-linear-transformation"><i class="fa fa-check"></i><b>1.7</b> Matrix multiplication as linear transformation</a></li>
<li class="chapter" data-level="1.8" data-path="matrix-basics.html"><a href="matrix-basics.html#statistics-and-proabability"><i class="fa fa-check"></i><b>1.8</b> Statistics and proabability</a><ul>
<li class="chapter" data-level="1.8.1" data-path="matrix-basics.html"><a href="matrix-basics.html#sample-statistics"><i class="fa fa-check"></i><b>1.8.1</b> Sample statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="vector-spaces.html"><a href="vector-spaces.html"><i class="fa fa-check"></i><b>2</b> Vector spaces</a><ul>
<li class="chapter" data-level="2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#four-subspaces"><i class="fa fa-check"></i><b>2.1</b> Four subspaces</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="orthogonality.html"><a href="orthogonality.html"><i class="fa fa-check"></i><b>3</b> Orthogonality</a><ul>
<li class="chapter" data-level="3.1" data-path="orthogonality.html"><a href="orthogonality.html#metric-spaces-normed-spaces-inner-product-spaces"><i class="fa fa-check"></i><b>3.1</b> Metric spaces, normed spaces, inner product spaces</a></li>
<li class="chapter" data-level="3.2" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition"><i class="fa fa-check"></i><b>3.2</b> Orthogonal decomposition</a><ul>
<li class="chapter" data-level="3.2.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-complements"><i class="fa fa-check"></i><b>3.2.1</b> Orthogonal complements</a></li>
<li class="chapter" data-level="3.2.2" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-sets-and-orthogonal-basis"><i class="fa fa-check"></i><b>3.2.2</b> Orthogonal sets and orthogonal basis</a></li>
<li class="chapter" data-level="3.2.3" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition-1"><i class="fa fa-check"></i><b>3.2.3</b> Orthogonal decomposition</a></li>
<li class="chapter" data-level="3.2.4" data-path="orthogonality.html"><a href="orthogonality.html#best-approximation"><i class="fa fa-check"></i><b>3.2.4</b> Best approximation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="orthogonality.html"><a href="orthogonality.html#gram-schmidt-process"><i class="fa fa-check"></i><b>3.3</b> Gram-Schmidt process</a><ul>
<li class="chapter" data-level="3.3.1" data-path="orthogonality.html"><a href="orthogonality.html#qr-factorizaiton"><i class="fa fa-check"></i><b>3.3.1</b> QR factorizaiton</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="orthogonality.html"><a href="orthogonality.html#orthonormal-sets-and-orthogonal-matrices"><i class="fa fa-check"></i><b>3.4</b> Orthonormal sets and orthogonal matrices</a><ul>
<li class="chapter" data-level="3.4.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-matrices"><i class="fa fa-check"></i><b>3.4.1</b> Orthogonal matrices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html"><i class="fa fa-check"></i><b>4</b> Eigenthings and quadratic forms</a><ul>
<li class="chapter" data-level="4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>4.1</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#diagnolization-and-similar-matrices"><i class="fa fa-check"></i><b>4.2</b> Diagnolization and similar matrices</a><ul>
<li class="chapter" data-level="4.2.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#similarity"><i class="fa fa-check"></i><b>4.2.1</b> similarity</a></li>
<li class="chapter" data-level="4.2.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#jordan-matrix"><i class="fa fa-check"></i><b>4.2.2</b> Jordan matrix</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#symmetric-matrices"><i class="fa fa-check"></i><b>4.3</b> Symmetric matrices</a><ul>
<li class="chapter" data-level="4.3.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#spectral-decomposition"><i class="fa fa-check"></i><b>4.3.1</b> Spectral decomposition</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#quadratic-forms"><i class="fa fa-check"></i><b>4.4</b> Quadratic forms</a><ul>
<li class="chapter" data-level="4.4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#change-of-variabele"><i class="fa fa-check"></i><b>4.4.1</b> Change of variabele</a></li>
<li class="chapter" data-level="4.4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#classification-of-quadratic-forms"><i class="fa fa-check"></i><b>4.4.2</b> Classification of quadratic forms</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#rayleigh-quotients"><i class="fa fa-check"></i><b>4.5</b> Rayleigh quotients</a></li>
<li class="chapter" data-level="4.6" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#svd"><i class="fa fa-check"></i><b>4.6</b> SVD</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="matrix-calculus.html"><a href="matrix-calculus.html"><i class="fa fa-check"></i><b>5</b> Matrix calculus</a></li>
<li class="part"><span><b>II Calculus</b></span></li>
<li class="chapter" data-level="6" data-path="taylor-series-and-expansion.html"><a href="taylor-series-and-expansion.html"><i class="fa fa-check"></i><b>6</b> Taylor series and expansion</a></li>
<li class="part"><span><b>III Applications</b></span></li>
<li class="chapter" data-level="7" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>7</b> Linear models</a><ul>
<li class="chapter" data-level="7.1" data-path="linear-models.html"><a href="linear-models.html#least-square-estimation"><i class="fa fa-check"></i><b>7.1</b> Least square estimation</a></li>
<li class="chapter" data-level="7.2" data-path="linear-models.html"><a href="linear-models.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>7.2</b> Maximum likelihood estimation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html"><i class="fa fa-check"></i><b>8</b> Principle component analysis</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li>
  <a href="https://github.com/rstudio/bookdown" target="blank">Written with bookdown</a>
</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Notes for Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="eigenthings-and-quadratic-forms" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Eigenthings and quadratic forms</h1>
<div id="eigenvectors-and-eigenvalues" class="section level2">
<h2><span class="header-section-number">4.1</span> Eigenvectors and eigenvalues</h2>

<div class="definition">
<p><span id="def:eigen" class="definition"><strong>Definition 4.1  (Eigenvectors and eigenvalues)  </strong></span>An <strong>eigenvector</strong> of an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is a <em>nonzero</em> vector <span class="math inline">\(\boldsymbol{x}\)</span> such that <span class="math inline">\(A\boldsymbol{x} = \lambda\boldsymbol{x}\)</span>.</p>
<span class="math inline">\(\lambda\)</span> is the <strong>eigenvalue</strong> of <span class="math inline">\(A\)</span> if there is a nontrivial solution <span class="math inline">\(\boldsymbol{x}\)</span> of <span class="math inline">\(A\boldsymbol{x} = \lambda \boldsymbol{x}\)</span>; such an <span class="math inline">\(\boldsymbol{x}\)</span> is called an <em>eigenvector corresponding to <span class="math inline">\(\lambda\)</span></em>
</div>

<p>To find eigenvalues and corresponding eigenvectors of <span class="math inline">\(A\)</span>, we look at the equation</p>
<p><span class="math display">\[
(A - \lambda I)\boldsymbol{x}= 0
\]</span></p>
<p>Since eigenvector <span class="math inline">\(\boldsymbol{x}\)</span> must be nonzero, <span class="math inline">\((A - \lambda I)\)</span> is a singular matrix</p>
<p><span class="math display" id="eq:characteristic-equation">\[\begin{equation}
\tag{4.1}
\det (A - \lambda I) = 0
\end{equation}\]</span></p>
<p>Eq <a href="eigenthings-and-quadratic-forms.html#eq:characteristic-equation">(4.1)</a> is called the <strong>characteristic equation</strong> of matrix <span class="math inline">\(A\)</span>. This is a scalar equation containing information about eigenvalues and eigenvectors of a square matrix <span class="math inline">\(A\)</span>.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-1" class="theorem"><strong>Theorem 1.1  </strong></span>Eigenvalues of a trangular matrix are its diagonal entries.
</div>

<p><strong>PROOF</strong></p>
<p>Consider the <span class="math inline">\(3 \times 3\)</span> case. If <span class="math inline">\(A\)</span> is upper triangular, then <span class="math inline">\(A - \lambda I\)</span> has the form</p>
<p><span class="math display">\[
\begin{bmatrix}
a_{11} - \lambda &amp; a_{12} &amp; a_{13} \\
0 &amp; a_{22} - \lambda &amp; a_{23}  \\
0 &amp; 0 &amp; a_{33} - \lambda
\end{bmatrix}
\]</span>
So the roots of characteristic are <span class="math inline">\(a_{11}, a_{22}, a_{33}\)</span> respectively.</p>
<p>There are some useful results about how eigenvalues change after various manipulations.</p>
<ol style="list-style-type: decimal">
<li>For any <span class="math inline">\(k, b \in \mathbb{R}\)</span>, <span class="math inline">\(\boldsymbol{x}\)</span> is an eigenvector of <span class="math inline">\(kA + bI\)</span> with eigenvalue <span class="math inline">\(k\lambda + b\)</span></li>
</ol>
<p><strong>PROOF</strong>
<span class="math display">\[
(kA + bI)\boldsymbol{x} = kA\boldsymbol{x} + bI\boldsymbol{x} = k \lambda\boldsymbol{x} + b\boldsymbol{x} = (k\lambda + b)\boldsymbol{x} 
\]</span></p>
<p>2, If <span class="math inline">\(A\)</span> is invertible, then <span class="math inline">\(\boldsymbol{x}\)</span> is an eigenvector of <span class="math inline">\(A^{-1}\)</span> with eigenvalue <span class="math inline">\(1/\lambda\)</span></p>
<p><strong>PROOF</strong></p>
<p><span class="math display">\[
\boldsymbol{x} = A^{-1}A\boldsymbol{x} =  A^{-1}\lambda \boldsymbol{x} = \lambda A^{-1}\boldsymbol{x}
\]</span>
3. <span class="math inline">\(A^{k}\boldsymbol{x} = \lambda^{k}\boldsymbol{x}\)</span></p>
<p>The next theorem is important in terms of diagonalization and spectral decomposition</p>

<div class="theorem">
<span id="thm:distinct-eigenvalue" class="theorem"><strong>Theorem 4.1  </strong></span>For distinct eigenvalues <span class="math inline">\(\lambda_1, \cdots, \lambda_r\)</span> of an <span class="math inline">\(n \times n\)</span> matrix A, their corresponding eigenvectors <span class="math inline">\(\boldsymbol{v_1}, ..., \boldsymbol{v_r}\)</span> are linearly independent.
</div>

<p><strong>PROOF</strong></p>
<p>Suppose for r distinct eigenvalue <span class="math inline">\(\lambda_1, \cdots, \lambda_r\)</span>, the set <span class="math inline">\(\{\boldsymbol{v_1}, ..., \boldsymbol{v_r}\}\)</span> is not linearly independent, and <span class="math inline">\(p\)</span> is the least index such that <span class="math inline">\(\boldsymbol{v}_{p+1}\)</span> is a linear combination of the preceding vectors. Then there exists scalars <span class="math inline">\(c_1, \cdots, c_p\)</span> such that</p>
<p><span class="math display">\[
c_1\boldsymbol{v}_1 + \cdots + c_p\boldsymbol{v}_p = \boldsymbol{v}_{p+1} \tag{1}
\]</span>
Left multiply by <span class="math inline">\(A\)</span>, and note we have <span class="math inline">\(A\boldsymbol{v}_i = \lambda_i\boldsymbol{v}_i\)</span> for <span class="math inline">\(i = 1, ..., n\)</span></p>
<p><span class="math display">\[
c_1\lambda_1\boldsymbol{v}_1 + \cdots + c_p\lambda_p\boldsymbol{v}_p = \lambda_{p+1}\boldsymbol{v}_{p+1} \tag{2}
\]</span>
Multiplying both sides of (2) by <span class="math inline">\(\lambda_{p+1}\)</span> and subtracting (2) from the result</p>
<p><span class="math display">\[
c_1(\lambda_1 - \lambda_{p+1})\boldsymbol{v}_1 +\cdots + c_p(\lambda_p - \lambda_{p+1})\boldsymbol{v}_p = 0 \tag{3}
\]</span>
Since <span class="math inline">\(\boldsymbol{v}_1, ..., \boldsymbol{v}_p\)</span> are linearly independent, weights in (3) must be all zero. Since <span class="math inline">\(\lambda_1, \cdots, \lambda_p\)</span> are distinct, hence <span class="math inline">\(c_i = 0, \, i = 1, ..., p\)</span>. But then (5) says that eigenvector <span class="math inline">\(\boldsymbol{v}_{p+1}\)</span> is zero vector, which contradicts definition <a href="eigenthings-and-quadratic-forms.html#def:eigen">4.1</a></p>
</div>
<div id="diagnolization-and-similar-matrices" class="section level2">
<h2><span class="header-section-number">4.2</span> Diagnolization and similar matrices</h2>

<div class="definition">
<p><span id="def:diagonalization" class="definition"><strong>Definition 4.2  (Diagonalization thoerem)  </strong></span>An <span class="math inline">\(n \ times n\)</span> matrix <span class="math inline">\(A\)</span> is diagnolizable <strong>if and only if</strong> A has <span class="math inline">\(n\)</span> independent linearly independent eigenvectors.</p>
<p>In such case, in <span class="math inline">\(A = P \Lambda P^{-1}\)</span>, the diagonal entries of <span class="math inline">\(D\)</span> are eigenvalues that correpond, respectively, to the eigenvectors of in <span class="math inline">\(P\)</span></p>
In other words, <span class="math inline">\(A\)</span> is diagnolizable if and only if there are enough eigenvectors in form a basis of <span class="math inline">\(R^n\)</span>, called an <strong>eigenvector basis</strong> of <span class="math inline">\(R^n\)</span>
</div>

<p><strong>Proof</strong></p>
<p><span class="math display">\[
\begin{split}
AP &amp;= A[\boldsymbol{v}_1 \cdots \boldsymbol{v}_n] \\
   &amp;= [A\boldsymbol{v}_1 \cdots A\boldsymbol{v}_n] \\ 
   &amp;= [\lambda_1\boldsymbol{v}_1 \cdots \lambda_n\boldsymbol{v}_n]
\end{split}
\]</span>
while on the other side of the equation:</p>
<p><span class="math display">\[
\begin{aligned}
DP &amp;= 
[\boldsymbol{v}_1 \cdots \boldsymbol{v}_n]
\begin{bmatrix}
\lambda_1 &amp; 0 &amp; \cdots &amp; 0\\
0  &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_n 
\end{bmatrix} 
 \\
&amp;= [\lambda_1\boldsymbol{v}_1 \cdots \lambda_n\boldsymbol{v}_n]
\end{aligned}
\]</span></p>
<p>So that</p>
<p><span class="math display">\[
\begin{aligned}
AP &amp;= PD \\
A &amp;= P \Lambda P^{-1}
\end{aligned}
\]</span>
Because <span class="math inline">\(P\)</span> contains <span class="math inline">\(n\)</span> independent columns so itâ€™s invertible.</p>
<p>According to theorem <a href="eigenthings-and-quadratic-forms.html#thm:distinct-eigenvalue">4.1</a>, an <span class="math inline">\(n \times n\)</span> matrix with <span class="math inline">\(n\)</span> distinct eigenvalues is diagonalizable. This is a sufficient condition.</p>
<p>For matrices whose eigenvalues are not distinct, there is still a change that it is diagonalizable. For any matrix <span class="math inline">\(A_{n\times n}\)</span>, as long as the sum of the dimensions of the eigenspaces equals <span class="math inline">\(n\)</span> then <span class="math inline">\(P\)</span> is invertible. This could happen in the following two scenarios</p>
<ol style="list-style-type: decimal">
<li><p>The characteristic polynomial factors completely into linear factors. This is the case when <span class="math inline">\(A\)</span> has n distinct eigenvalues.</p></li>
<li><p>The dimension of the eigenspace for each <span class="math inline">\(\lambda_k\)</span> equals the multiplicity of <span class="math inline">\(\lambda_k\)</span>. Thus <span class="math inline">\(A\)</span> with repeated eigenvalues can still be diagonalizable.</p></li>
</ol>
<div id="similarity" class="section level3">
<h3><span class="header-section-number">4.2.1</span> similarity</h3>
<p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are both <span class="math inline">\(n \times n\)</span> matrices, then <span class="math inline">\(A\)</span> <strong>is similar to</strong> <span class="math inline">\(N\)</span> if there is an invertible matrix <span class="math inline">\(P\)</span> such that <span class="math inline">\(P^{-1}AP = B\)</span>, or equivalently if we write <span class="math inline">\(Q\)</span> for <span class="math inline">\(P^{-1}\)</span>, <span class="math inline">\(Q^{-1}BQ = A\)</span>. Changing <span class="math inline">\(A\)</span> into <span class="math inline">\(P^{-1}AP\)</span> is called a <strong>similarity transformation</strong>.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-2" class="theorem"><strong>Theorem 3.1  </strong></span>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are similar, they have the same eigenvalues.
</div>

<p><strong>PROOF</strong><br />
If <span class="math inline">\(B = P^{-1}AP\)</span>, then</p>
<p><span class="math display">\[
B - \lambda I = P^{-1}AP - \lambda P^{-1}P = P^{-1}(AP - \lambda P) =  P^{-1}(A - \lambda I) P
\]</span>
so that</p>
<p><span class="math display">\[
\det (B - \lambda I ) = \det(P) \cdot \det(A - \lambda I ) \cdot \det(P^{-1})
\]</span>
since <span class="math inline">\(\det(P) \cdot \det(P^{-1}) = \det (I) = 1\)</span>, we have</p>
<p><span class="math display">\[
\det (B - \lambda I)  = \det(A - \lambda I)
\]</span></p>
<p>As a result of their identical characteristic polynomial, <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span> have the same eigenvalues. We can also show that eigenvector of <span class="math inline">\(B\)</span> is <span class="math inline">\(P\boldsymbol{v}\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
A\boldsymbol{v} &amp;= \lambda\boldsymbol{v} \\
(P^{-1}BP)\boldsymbol{v} &amp;= \lambda\boldsymbol{v} \\
P(P^{-1}BP)\boldsymbol{v} &amp;= \lambda P\boldsymbol{v} \\
B(P\boldsymbol{v}) = \lambda P \boldsymbol{v}
\end{aligned}
\]</span></p>
<p>The similarity theorem leads to a interesting proposition.</p>

<div class="proposition">
<span id="prp:unnamed-chunk-3" class="proposition"><strong>Proposition 4.1  </strong></span>For <span class="math inline">\(A, B \in \mathbb{R}^{n \times n}\)</span>, <span class="math inline">\(AB\)</span> and <span class="math inline">\(BA\)</span> are similar matrices and therefore share the same set of eigenvalues.
</div>

<p>To prove this, we need to show that there exists a invertible matrix <span class="math inline">\(A\)</span> such that <span class="math inline">\(P^{-1}(AB)P = BA\)</span>. Take <span class="math inline">\(P = A\)</span> and the equation holds.</p>
<p>It is easy to show that similarity is <strong>transitive</strong>: if <span class="math inline">\(A\)</span> is similar to <span class="math inline">\(B\)</span>, <span class="math inline">\(B\)</span> is similar to <span class="math inline">\(C\)</span>, then <span class="math inline">\(A\)</span> is similar to <span class="math inline">\(C\)</span>. So similarity means a family of matrices with the same set of eigenvalues, the most special and simplest of which is the diagonal matrix (if this is an diagonalizable family). Some computer algorithms calculate eigenvalues of <span class="math inline">\(A\)</span> in this manner: with a sequential choices of <span class="math inline">\(P\)</span>, the off-diagonal elements of <span class="math inline">\(A\)</span> become smaller and smaller until <span class="math inline">\(A\)</span> becomes a triangular matrix or diagonal matrix, whose eigenvalues are simply diagonal entries and is the same as <span class="math inline">\(A\)</span>.</p>
<p>It is obvious that a diagonalizable matrix <span class="math inline">\(A\)</span> is similar to diagonal matrix <span class="math inline">\(D\)</span>, whose diagonal entries are <span class="math inline">\(A\)</span>â€™s eigenvalues <span class="math inline">\(\lambda_i\)</span>, and <span class="math inline">\(P = [\boldsymbol{v}_1 \;\; \cdots \;\; \boldsymbol{v}_n]^{-1}\)</span> where <span class="math inline">\(\boldsymbol{v}_i, \;i = 1,..., n\)</span> are eigenvectors corresponding to <span class="math inline">\(\lambda_i\)</span>.</p>
<p>But square matrix <span class="math inline">\(A\)</span> can still be similar to matrices other than <span class="math inline">\(D\)</span> with other choices of <span class="math inline">\(P\)</span>, and non-diagonal matrices can also have similar matrices of their own. In fact, <strong>every square matrix is similar to a matrix in Jordan matrix</strong> <a href="eigenthings-and-quadratic-forms.html#jordan-matrix">4.2.2</a>.</p>
<hr>
<p>Similarity is only a <em>sufficient</em> condition for identical eigenvalues. The matrices</p>
<p><span class="math display">\[
\begin{bmatrix}
2 &amp; 1 \\
0 &amp; 2 
\end{bmatrix}
\;\text{and}\;
\begin{bmatrix}
2 &amp; 0 \\
0 &amp; 2 
\end{bmatrix}
\]</span>
are not similar even though they have the same eigenvalues.</p>
</div>
<div id="jordan-matrix" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Jordan matrix</h3>
<p>For non-diagonalizable matrices <span class="math inline">\(A_{n \times n}\)</span>, the goal is to with similar transformation <span class="math inline">\(P^{-1}AP\)</span> construct a matrix that is as nearest to a diagonal matrix as possible.</p>

<div class="definition">
<span id="def:unnamed-chunk-4" class="definition"><strong>Definition 4.3  </strong></span>The <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(J_{\lambda, n}\)</span> with <span class="math inline">\(\lambda\)</span>s on the diagonal, <span class="math inline">\(1\)</span>s on the superdiagonal and <span class="math inline">\(0\)</span>s elsewhere is called a Jordan matrix. A Jordan matrix in Jordan normal form is a block matrix that has Jordan blocks down its block diagonal and is zero elsewhere
</div>

<p>An example of Jordan matrix, the appearance of <span class="math inline">\(\lambda_i\)</span> on the diagonal is equal to its multiplicity as <span class="math inline">\(A\)</span>â€™s eigenvalue.
<span class="math display">\[
\begin{bmatrix}
\lambda_1 &amp; 1  &amp; \\
&amp; \lambda_1 &amp; 1 &amp; \\
&amp; &amp; \lambda_1 &amp; \\ 
&amp; &amp; &amp; \lambda_2 &amp; 1 \\
&amp; &amp; &amp; &amp; \lambda_2  \\ 
&amp; &amp; &amp; &amp; &amp; \lambda_3 &amp; 1  \\ 
&amp; &amp; &amp; &amp; &amp; &amp; \ddots \\
&amp; &amp; &amp; &amp; &amp; &amp; &amp; \lambda_n &amp; 1 \\
&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \lambda_n
\end{bmatrix}
\]</span>
An illustration from <a href="https://en.wikipedia.org/wiki/Jordan_normal_form">wikipedia</a>, the circled area is the Jordan blcok.
<img src="images/jordan-blocks.png" width="40%" style="display: block; margin: auto;" /></p>
<p>Though the purpose of this section was not the computation details of Jordan matrices, it helps to give a concrete example. Consider <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
A = 
\begin{bmatrix}
5 &amp; 4 &amp; 2 &amp; 1 \\
0 &amp; 1 &amp; -1 &amp; -1 \\
-1 &amp; -1 &amp; 3 &amp; 0 \\
1 &amp; 1 &amp; -1 &amp; 2
\end{bmatrix}
\]</span></p>
<p>Including multiplicity, the eigenvalues of <span class="math inline">\(A\)</span> are <span class="math inline">\(\lambda = 1, 2, 4, 4\)</span>. And for <span class="math inline">\(\lambda = 4\)</span>, the eigenspace is 1 dimensional instead of 2, meaning <span class="math inline">\(A\)</span> is not diagonalizable. Nonetheless, <span class="math inline">\(A\)</span> is similar to the following Jordan matrix</p>
<p><span class="math display">\[
J = 
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 2 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 4 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 4
\end{bmatrix}
\]</span></p>
<p>To obtain <span class="math inline">\(P\)</span>, recall that <span class="math inline">\(P^{-1}AP = J\)</span>. Let <span class="math inline">\(P\)</span> have column vectors <span class="math inline">\(p_i, \; i = 1,...,4\)</span>, then:</p>
<p><span class="math display">\[
A[\boldsymbol{p}_1 \; \; \boldsymbol{p}_2 \;\; \boldsymbol{p}_3 \;\; \boldsymbol{p}_4] = [\boldsymbol{p}_1 \; \; \boldsymbol{p}_2 \;\; \boldsymbol{p}_3 \;\; \boldsymbol{p}_4]
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 2 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 4 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 4
\end{bmatrix}
= [\boldsymbol{p}_1 \;\; 2\boldsymbol{p}_2 \;\; 4\boldsymbol{p}_3 \;\; \boldsymbol{p}_3 + 4\boldsymbol{p}_4]
\]</span></p>
<p>We see that</p>
<p><span class="math display">\[
\begin{aligned}
(A - 1I)\boldsymbol{p}_1 &amp;= \boldsymbol{0} \\
(A - 2I)\boldsymbol{p}_2 &amp;= \boldsymbol{0} \\
(A - 4I)\boldsymbol{p}_3 &amp;= \boldsymbol{0} \\
(A - 1I)\boldsymbol{p}_4 &amp;= \boldsymbol{p}_3 
\end{aligned}
\]</span>
The solutions <span class="math inline">\(\boldsymbol{p}_i\)</span> are called <strong>generalized eigenvectors</strong> of <span class="math inline">\(A\)</span>.</p>
</div>
</div>
<div id="symmetric-matrices" class="section level2">
<h2><span class="header-section-number">4.3</span> Symmetric matrices</h2>
<p>A <em>square</em> matrix <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> is <em>symmetric</em> if <span class="math inline">\(A = A^{T}\)</span>, and <em>anti-symmetric</em> if <span class="math inline">\(A = - A^{T}\)</span>.</p>
<p>It can be shown that for any <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>, <span class="math inline">\(A + A^T\)</span> is symmetric and <span class="math inline">\(A - A^T\)</span> anti-symmetric. So any square matrix <span class="math inline">\(A\)</span> can be wrote as a sum of a symmetric matrix and an anti-symmetric matrix</p>
<p><span class="math display">\[
A = \frac{1}{2}(A + A^T) + \frac{1}{2}(A - A^T)
\]</span></p>
<p>It is common to denote the set of all symmetric matrices of size <span class="math inline">\(n\)</span> as <span class="math inline">\(\mathbb{S}^n\)</span>, and <span class="math inline">\(A \in \mathbb{S}^n\)</span> means <span class="math inline">\(A\)</span> is a symmetric <span class="math inline">\(n \times n\)</span> matrix.</p>
<p>Symmetric matrices have some nice properties about diagonalization.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-6" class="theorem"><strong>Theorem 1.7  </strong></span>If <span class="math inline">\(A\)</span> is symmetric, eigenvectors from distinct eigenvalues are <strong>orthogonal</strong>.
</div>

<p><strong>PROOF</strong></p>
<p>Let <span class="math inline">\(\boldsymbol{v}_1\)</span> and <span class="math inline">\(\boldsymbol{v}_2\)</span> be eigenvectors that correspond to distinct eigenvalues <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>. Compute</p>
<p><span class="math display">\[
\begin{split}
\lambda_1\boldsymbol{v}_1 \cdot \boldsymbol{v}_2 &amp;= (\lambda_1\boldsymbol{v}_1)^T\boldsymbol{v}_2 \\
&amp;= (\boldsymbol{v}_1^TA^T)\boldsymbol{v}_2 \\
&amp;= \boldsymbol{v}_1^T(A\boldsymbol{v}_2) \\
&amp;= \boldsymbol{v}_1^T(\lambda_2\boldsymbol{v}_2) \\
&amp;= \lambda_2\boldsymbol{v}_1 \cdot \boldsymbol{v}_2
\end{split}
\]</span>
because <span class="math inline">\(\lambda_1 \not = \lambda_2\)</span>, <span class="math inline">\(\boldsymbol{v}_1 \cdot \boldsymbol{v}_2 = 0\)</span>.</p>
<p>For symmetric matrices <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> without <span class="math inline">\(n\)</span> distinct eigenvalues, it turns out that the dimension of the eigenspace for each <span class="math inline">\(\lambda_k\)</span> always equals the multiplicity of <span class="math inline">\(\lambda_k\)</span>. For this reason, if <span class="math inline">\(A\)</span> is a symmetric matrix we can always construct a orthonormal set <span class="math inline">\(\{\boldsymbol{q}_1 \;\; \cdots \;\; \boldsymbol{q}_n\}\)</span> from <span class="math inline">\(\{\boldsymbol{v}_1 \;\; \cdots \;\; \boldsymbol{v}_n\}\)</span> such that</p>
<p><span class="math display">\[
P^{T} = 
\begin{bmatrix}
\boldsymbol{q}_1^T \\
\vdots \\ 
\boldsymbol{q}_n^T
\end{bmatrix}
= P^{-1}
\]</span>
Recall that matrix <span class="math inline">\(A\)</span> with <span class="math inline">\(n\)</span> linearly independent eigenvectors is diagonalizable and can be written as</p>
<p><span class="math display">\[
A = P \Lambda P^{-1}
\]</span>
where <span class="math inline">\(P = [\boldsymbol{v}_1 \;\; \cdots \;\; \boldsymbol{v}_n]\)</span> and <span class="math inline">\(D\)</span> is a diagonal matrix with eigenvalues on its diagonal entries.</p>
<p>With symmetric matrices, after proper transformation we have <span class="math inline">\(P^T = P^{-1}\)</span>, so that</p>
<p><span class="math display" id="eq:orthogonal-diagonalization">\[\begin{equation}
\tag{4.2}
A = Q \Lambda Q^{T}
\end{equation}\]</span></p>
<p>Such matrix <span class="math inline">\(A\)</span> is said to be <strong>orthogonally diagonalizable</strong>.</p>
<p>We have seen that for symmetric matrix <span class="math inline">\(A\)</span>, Eq <a href="eigenthings-and-quadratic-forms.html#eq:orthogonal-diagonalization">(4.2)</a> always holds. We can also also verify that if <span class="math inline">\(A\)</span> is orthogonally diagonalizable then it is a symmetric matrix</p>
<p><span class="math display">\[
A^T = (Q \Lambda Q^{T})^T = PD^TP^T = Q \Lambda Q^{T}  = A
\]</span></p>

<div class="theorem">
<span id="thm:unnamed-chunk-7" class="theorem"><strong>Theorem 4.2  </strong></span>An <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is orthogonally diagonalizable if an only if <span class="math inline">\(A\)</span> is a symmetric matrix.
</div>

<div id="spectral-decomposition" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Spectral decomposition</h3>
<p>For orthogonally diagonalizable matrix <span class="math inline">\(A\)</span>, we have</p>
<p><span class="math display">\[
A = Q \Lambda Q^{T} = [\boldsymbol{q}_1 \;\; \cdots \;\; \boldsymbol{q}_n] 
\begin{bmatrix}
\lambda_1 &amp; &amp; \\
 &amp; \ddots \\
 &amp; &amp; \lambda_n
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{q}_1^T \\
\vdots \\
\boldsymbol{q}_n
\end{bmatrix}
\]</span></p>
<p>It follows that</p>
<p><span class="math display" id="eq:spectral-decomposition">\[\begin{equation}
\tag{4.3}
A = \lambda_1\boldsymbol{q}_1\boldsymbol{q}_1^T + \cdots + \lambda_1\boldsymbol{q}_n\boldsymbol{q}_n^T
\end{equation}\]</span></p>
<p>Eq <a href="eigenthings-and-quadratic-forms.html#eq:spectral-decomposition">(4.3)</a> is called the <strong>spectral decomposition</strong>, breaking <span class="math inline">\(A\)</span> into pieces of rank 1 matrix. It got this name because he set of eigenvalues of a matrix <span class="math inline">\(A\)</span> is sometimes called its <em>spectrum</em>.</p>
</div>
</div>
<div id="quadratic-forms" class="section level2">
<h2><span class="header-section-number">4.4</span> Quadratic forms</h2>

<div class="definition">
<span id="def:unnamed-chunk-8" class="definition"><strong>Definition 1.3  (Quadratic form)  </strong></span>A <strong>quadratic form</strong> on <span class="math inline">\(\mathbb{R}^n\)</span> is a function <span class="math inline">\(Q\)</span> defined on <span class="math inline">\(\mathbb{R}^n\)</span> whose value at a vector <span class="math inline">\(\boldsymbol{x}\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span> can be computed by an expression of the form <span class="math inline">\(Q(\boldsymbol{x}) = \boldsymbol{x}^TA\boldsymbol{x}\)</span>, where <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> is a <strong>symmetric</strong> matrix. <span class="math inline">\(A\)</span> is called the matrix of the quadraticc form.
</div>

<p>There exists a one-to-one mapping between symmetric matrix <span class="math inline">\(A\)</span> and the quadratic form. Consider the <span class="math inline">\(3 \times 3\)</span> case:</p>
<p><span class="math display">\[
\boldsymbol{x} =
\begin{bmatrix}
x_1 \\
x_3 \\
x_3 \\
\end{bmatrix}
, \;\; A = 
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33} 
\end{bmatrix} \\
\]</span></p>
<p><span class="math display">\[
\begin{split}
\boldsymbol{x}^TA\boldsymbol{x} &amp;= 
[x_1 \;\; x_2 \;\; x_3]
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33} 
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_3 \\
x_3 \\
\end{bmatrix} \\
&amp;= a_{11}x_1^2 + a_{22}x_2^2 + a_{33}x_3^2 + \\
&amp; \quad(a_{12} + a_{21})x_1x_2 + (a_{13} + a_{31})x_1x_3 + (a_{23} + a_{32})x_2x_3 
\end{split} 
\tag{1}
\]</span>
Since <span class="math inline">\(A\)</span> is symmetric, we have <span class="math inline">\(a_{ij} = a_{ji}\)</span>, thus</p>
<p><span class="math display">\[
\boldsymbol{x}^TA\boldsymbol{x}  = a_{11}x_1^2 + a_{22}x_2^2 + a_{33}x_3^2 + 2a_{12}x_1x_2 + 2a_{13}x_1x_3 + 2a_{23}x_2x_3 \tag{2} 
\]</span>
This verifies that <span class="math inline">\(\boldsymbol{x}^TA\boldsymbol{x}\)</span> when <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> is symmetric does result in a quadratic function of <span class="math inline">\(n\)</span> variables. Conversely, any quadratic function of <span class="math inline">\(n\)</span> variables, like shown in <span class="math inline">\((2)\)</span>, can be expressed in terms of <span class="math inline">\(\boldsymbol{x}^TA\boldsymbol{x}\)</span> with unique choice of symmetric matrix <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>.</p>
<div id="change-of-variabele" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Change of variabele</h3>
<p>If <span class="math inline">\(\boldsymbol{x}\)</span> is a variable vector in <span class="math inline">\(\mathbb{R}^n\)</span>, then a <em>change of variable</em> is an equation of the form</p>
<span class="math display">\[
\begin{aligned}
\boldsymbol{x} &amp;= P\boldsymbol{y} \\
\text{or equivalently} \quad \boldsymbol{y} &amp;= P^{-1}\boldsymbol{x}
\end{aligned}
\]</span>

<div class="theorem">
<p><span id="thm:unnamed-chunk-9" class="theorem"><strong>Theorem 4.3  (The Principal Axes Theorem)  </strong></span>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n \times n\)</span> symmetric matrix. Then there is an orthogonal change of variable, <span class="math inline">\(\boldsymbol{x} = P\boldsymbol{y}\)</span>, this transform the quadratic form <span class="math inline">\(\boldsymbol{x}^TA\boldsymbol{x}\)</span> into a quadratic form <span class="math inline">\(\boldsymbol{y}^TD\boldsymbol{y}\)</span> with no cross-product term.</p>
<p><span class="math inline">\(P\)</span> is constructed with <span class="math inline">\(A\)</span>â€™s orthonormal eigenvectors <span class="math inline">\(\boldsymbol{q}_1, ..., \boldsymbol{q}_n\)</span>. According to theorem <a href="eigenthings-and-quadratic-forms.html#eq:orthogonal-diagonalization">(4.2)</a>:</p>
<span class="math display">\[
\boldsymbol{x}^TA\boldsymbol{x} = (P\boldsymbol{y})^TA(P\boldsymbol{y}) = \boldsymbol{y}^TP^{T}AP\boldsymbol{y} = \boldsymbol{y}^TD\boldsymbol{y}
\]</span>
</div>

<p>The principle axes theorem <a href="#thm:the-principal-axes-theorem"><strong>??</strong></a> shows that if <span class="math inline">\(A\)</span> is diagonalizable, quadratic form <span class="math inline">\(\boldsymbol{x}^TA\boldsymbol{x}\)</span> can be reexpressed into the form <span class="math inline">\(\lambda_1y_1^2 + \lambda_2y_2^2 + \cdots + \lambda_ny_n^2\)</span> with change of variables <span class="math inline">\(\boldsymbol{x} = P\boldsymbol{y}\)</span>.</p>
</div>
<div id="classification-of-quadratic-forms" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Classification of quadratic forms</h3>
<ul>
<li><p>A symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is <strong>positive definite</strong> (PD) if for all non-zero vectors <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^n,\; \boldsymbol{x}^TA\boldsymbol{x} &gt; 0\)</span>. We can denote positive definite matrix <span class="math inline">\(A\)</span> as <span class="math inline">\(A \succ 0\)</span> (or <span class="math inline">\(A &gt; 0\)</span>). The set of all positive definite matrices is denoted as <span class="math inline">\(\mathbb{S}_{++}^n\)</span></p></li>
<li><p>A symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is <strong>positive semidefinite</strong> (PSD) if for all non-zero vectors <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^n,\; \boldsymbol{x}^TA\boldsymbol{x} \ge 0\)</span>. We can denote positive definite matrix <span class="math inline">\(A\)</span> as <span class="math inline">\(A \succeq 0\)</span> (or <span class="math inline">\(A \ge 0\)</span>). The set of all positive semidefinite matrices is denoted as <span class="math inline">\(\mathbb{S}_{+}^n\)</span></p></li>
<li><p>A symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is <strong>negative definite</strong> (ND), denoted by <span class="math inline">\(A \prec 0\)</span> (or <span class="math inline">\(A &lt; 0\)</span>), if for all non-zero vectors <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^n,\; \boldsymbol{x}^TA\boldsymbol{x} &lt; 0\)</span>.</p></li>
<li><p>Similarly, a symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is <strong>negative semidefinite</strong> (NSD), denoted by <span class="math inline">\(A \preceq 0\)</span> (or <span class="math inline">\(A \le 0\)</span>), if for all non-zero vectors <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^n,\; \boldsymbol{x}^TA\boldsymbol{x} \le 0\)</span>.</p></li>
<li><p>Finally, a symmetric matrix <span class="math inline">\(A \in \mathbb{S}^n\)</span> is <strong>indefinite</strong>, if it is neither positive semidefinite or negative semidefinite. In other words, if there exists <span class="math inline">\(\boldsymbol{x}, \boldsymbol{x}&#39;, \in \mathbb{R}^{n}\)</span> such taht <span class="math inline">\(\boldsymbol{x}^TA\boldsymbol{x} &gt; 0\)</span> and <span class="math inline">\(\boldsymbol{x&#39;}^TA\boldsymbol{x}&#39; &gt; 0\)</span></p></li>
</ul>

<div class="rmdnote">
<p>Note that when talking about <span class="math inline">\(A\)</span> being PD, PSD, ND, NSD or indefinite, <span class="math inline">\(A\)</span> is always assumed to be <strong>symmetric</strong>.</p>
Also, if <span class="math inline">\(A\)</span> is positive definite, then <span class="math inline">\(âˆ’A\)</span> is negative definite and viceversa. Likewise, if <span class="math inline">\(A\)</span> is positive semidefinite then <span class="math inline">\(âˆ’A\)</span> is negative semidefinite and vice versa. If <span class="math inline">\(A\)</span> is indefinite, then so is <span class="math inline">\(âˆ’A\)</span>.
</div>

<p>From theorem <a href="#thm:the-principal-axes-theorem"><strong>??</strong></a>, we know that the sign of eigenvalues are closely related to classifications of symmetric matrices here. Take positive definite matrices for example, the following statements of <span class="math inline">\(A\)</span> are equivalent:</p>
<ul>
<li><p>For any <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^n, \; \boldsymbol{x}^TA\boldsymbol{x} &gt; 0\)</span></p></li>
<li><p>Let <span class="math inline">\(\lambda_i, \; i = 1, ..., n\)</span> be <span class="math inline">\(A\)</span>â€™s eigenvalues, <span class="math inline">\(\lambda_i &gt; 0\)</span></p></li>
<li><p>All leading determinants of <span class="math inline">\(A &gt; 0\)</span></p></li>
<li><p>All pivots are <span class="math inline">\(&gt; 0\)</span></p></li>
</ul>
<p>Classification of <span class="math inline">\(A \in \mathbb{S}^{n}\)</span> by its eigenvalue can be applied in general.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-11" class="theorem"><strong>Theorem 4.4  (Quadratic forms and eigenvalues)  </strong></span>Let <span class="math display">\[A \in \mathbb{S}^{n}\]</span>. Then the quadratic form <span class="math inline">\(\boldsymbol{x}^TA\boldsymbol{x}\)</span> and <span class="math inline">\(A\)</span> is:</p>
<ul>
<li><p>positive definite if and only if the eigenvalues of <span class="math inline">\(A\)</span> are all positive</p></li>
<li><p>negative definite if and only if the eigenvalues of <span class="math inline">\(A\)</span> are all negative</p></li>
<li><p>indefinite if and only if <span class="math inline">\(A\)</span> has both positive and negative eigenvalues</p>
</div></li>
</ul>
</div>
</div>
<div id="rayleigh-quotients" class="section level2">
<h2><span class="header-section-number">4.5</span> Rayleigh quotients</h2>
<p>Let <span class="math inline">\(A \in \mathbb{S}^n\)</span> and <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^n\)</span>, <strong>Rayleigh quotient</strong> is defined as</p>
<p><span class="math display">\[
R_{A}(\boldsymbol{x}) = \frac{\boldsymbol{x}^TA\boldsymbol{x}}{\boldsymbol{x}^T\boldsymbol{x}}
\]</span>
The Rayleigh quotient has some nice properties:</p>
<ul>
<li><p>scale invariance: for any vector <span class="math inline">\(\boldsymbol{x} \not= 0\)</span> and any scalar <span class="math inline">\(\alpha \not= 0\)</span>, <span class="math inline">\(R_{A}(\boldsymbol{x}) = R_{A}(\alpha\boldsymbol{x})\)</span></p></li>
<li><p>If <span class="math inline">\(\boldsymbol{x}\)</span> is a eigenvector of <span class="math inline">\(A\)</span> with eigenvalue <span class="math inline">\(\lambda\)</span>, then <span class="math inline">\(R_{A}(\boldsymbol{x}) = \lambda\)</span></p></li>
<li><p>The Rayleigh quotient is bounded by the largest and smallest eigenvalue of <span class="math inline">\(A\)</span>, i.e.Â </p></li>
</ul>
<p><span class="math display">\[
\lambda_{\text{min}}(A) \le R_{A}(\boldsymbol{x}) \le \lambda_{\text{max}}(A)
\]</span></p>
<p><strong>PROOF</strong></p>
<p>Since the Rayleigh quotient does not depend on the 2-norm of vector <span class="math inline">\(\boldsymbol{x}\)</span>, we may assume a unit vector <span class="math inline">\(\boldsymbol{x}^T\boldsymbol{x} = 1\)</span>.</p>
<p>Next, orthogonally diagonalize <span class="math inline">\(A\)</span> as <span class="math inline">\(Q \Lambda Q\)</span></p>
</div>
<div id="svd" class="section level2">
<h2><span class="header-section-number">4.6</span> SVD</h2>
<p><span class="math display">\[
\begin{split}
U\Sigma &amp;= [\boldsymbol{q}_1 \;\; \cdots \;\; \boldsymbol{q}_n]
\begin{bmatrix}
\sigma_1 \\
&amp; \ddots &amp;  \\ 
&amp; &amp; \sigma_r \\
&amp; &amp; &amp; 0 \\
&amp; &amp; &amp; &amp; \ddots \\
&amp; &amp; &amp; &amp; &amp; 0 \\
\end{bmatrix} \\
&amp;= [\sigma_1\boldsymbol{q}_1 \;\; \cdots \;\; \sigma_n\boldsymbol{q}_n] \\
&amp; = [A\boldsymbol{v}_1 \;\; \cdots \;\; A\boldsymbol{v}_n] \\
&amp;= AV
\end{split}
\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="orthogonality.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="matrix-calculus.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": "https://github.com/enixam/math-foundations/edit/master/eigen-quadratic.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
