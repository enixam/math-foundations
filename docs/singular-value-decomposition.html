<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Singular Value Decomposition | Mathematical Notes for Machine Learning</title>
  <meta name="description" content="Chapter 5 Singular Value Decomposition | Mathematical Notes for Machine Learning" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Singular Value Decomposition | Mathematical Notes for Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="enixam/math-foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Singular Value Decomposition | Mathematical Notes for Machine Learning" />
  
  
  

<meta name="author" content="Qiushi Yan" />


<meta name="date" content="2020-10-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="eigenthings-and-quadratic-forms.html"/>
<link rel="next" href="linear-system.html"/>
<script src="libs/header-attrs-2.4/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Math Notes for Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Linear Algebra</b></span></li>
<li class="chapter" data-level="1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html"><i class="fa fa-check"></i><b>1</b> Basic Matrix Algebra</a>
<ul>
<li class="chapter" data-level="1.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.1</b> Matrix Multiplication</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#geometric-transformations"><i class="fa fa-check"></i><b>1.1.1</b> Geometric Transformations</a></li>
<li class="chapter" data-level="1.1.2" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#matrix-multiplication-as-linear-transformation"><i class="fa fa-check"></i><b>1.1.2</b> Matrix Multiplication as Linear Transformation</a></li>
<li class="chapter" data-level="1.1.3" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#selector-matrix"><i class="fa fa-check"></i><b>1.1.3</b> Selector Matrix</a></li>
<li class="chapter" data-level="1.1.4" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#discrete-convolution"><i class="fa fa-check"></i><b>1.1.4</b> Discrete Convolution</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#elemetary-matrix-and-row-operations"><i class="fa fa-check"></i><b>1.2</b> Elemetary matrix and row operations</a></li>
<li class="chapter" data-level="1.3" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#lu-factorization"><i class="fa fa-check"></i><b>1.3</b> LU Factorization</a></li>
<li class="chapter" data-level="1.4" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#determinants"><i class="fa fa-check"></i><b>1.4</b> Determinants</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#cofactor-expansion"><i class="fa fa-check"></i><b>1.4.1</b> Cofactor Expansion</a></li>
<li class="chapter" data-level="1.4.2" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#geometric-interpretation-of-determinant"><i class="fa fa-check"></i><b>1.4.2</b> Geometric Interpretation of Determinant</a></li>
<li class="chapter" data-level="1.4.3" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#properties-of-determinant"><i class="fa fa-check"></i><b>1.4.3</b> Properties of Determinant</a></li>
<li class="chapter" data-level="1.4.4" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#cramers-rule"><i class="fa fa-check"></i><b>1.4.4</b> Cramer’s Rule</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#trace"><i class="fa fa-check"></i><b>1.5</b> Trace</a></li>
<li class="chapter" data-level="1.6" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#matrix-inversion"><i class="fa fa-check"></i><b>1.6</b> Matrix Inversion</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#the-matrix-inversion-lemma"><i class="fa fa-check"></i><b>1.6.1</b> The Matrix Inversion Lemma</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#complexity-of-matrix-computation"><i class="fa fa-check"></i><b>1.7</b> Complexity of Matrix Computation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="vector-spaces.html"><a href="vector-spaces.html"><i class="fa fa-check"></i><b>2</b> Vector Spaces</a>
<ul>
<li class="chapter" data-level="2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#vector-space"><i class="fa fa-check"></i><b>2.1</b> Vector Space</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="vector-spaces.html"><a href="vector-spaces.html#euclidean-space"><i class="fa fa-check"></i><b>2.1.1</b> Euclidean Space</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="vector-spaces.html"><a href="vector-spaces.html#metric-spaces-normed-spaces-inner-product-spaces"><i class="fa fa-check"></i><b>2.2</b> Metric Spaces, Normed Spaces, Inner Product Spaces</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#metric-and-norm"><i class="fa fa-check"></i><b>2.2.1</b> Metric and Norm</a></li>
<li class="chapter" data-level="2.2.2" data-path="vector-spaces.html"><a href="vector-spaces.html#inner-produc-outer-product-cross-product"><i class="fa fa-check"></i><b>2.2.2</b> Inner Produc, Outer Product, Cross Product</a></li>
<li class="chapter" data-level="2.2.3" data-path="vector-spaces.html"><a href="vector-spaces.html#restricted-definition-of-inner-products-in-rn"><i class="fa fa-check"></i><b>2.2.3</b> Restricted Definition of Inner Products in <span class="math inline">\(R^n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="vector-spaces.html"><a href="vector-spaces.html#subspaces"><i class="fa fa-check"></i><b>2.3</b> Subspaces</a></li>
<li class="chapter" data-level="2.4" data-path="vector-spaces.html"><a href="vector-spaces.html#fundamental-theorem"><i class="fa fa-check"></i><b>2.4</b> Fundamental Theorem of Linear Algebra</a></li>
<li class="chapter" data-level="2.5" data-path="vector-spaces.html"><a href="vector-spaces.html#rank"><i class="fa fa-check"></i><b>2.5</b> Rank</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="vector-spaces.html"><a href="vector-spaces.html#effect-of-operations-on-matrix-rank"><i class="fa fa-check"></i><b>2.5.1</b> Effect of Operations on Matrix Rank</a></li>
<li class="chapter" data-level="2.5.2" data-path="vector-spaces.html"><a href="vector-spaces.html#gram-matrix"><i class="fa fa-check"></i><b>2.5.2</b> Gram Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="vector-spaces.html"><a href="vector-spaces.html#bases-and-coordinate-systems"><i class="fa fa-check"></i><b>2.6</b> Bases and Coordinate Systems</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="vector-spaces.html"><a href="vector-spaces.html#change-of-basis"><i class="fa fa-check"></i><b>2.6.1</b> Change of Basis</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="vector-spaces.html"><a href="vector-spaces.html#complexity-of-vector-computations"><i class="fa fa-check"></i><b>2.7</b> Complexity of Vector Computations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="orthogonality.html"><a href="orthogonality.html"><i class="fa fa-check"></i><b>3</b> Orthogonality</a>
<ul>
<li class="chapter" data-level="3.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition"><i class="fa fa-check"></i><b>3.1</b> Orthogonal Decomposition</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-complements"><i class="fa fa-check"></i><b>3.1.1</b> Orthogonal Complements</a></li>
<li class="chapter" data-level="3.1.2" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-sets-and-orthogonal-basis"><i class="fa fa-check"></i><b>3.1.2</b> Orthogonal Sets and Orthogonal Basis</a></li>
<li class="chapter" data-level="3.1.3" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition-1"><i class="fa fa-check"></i><b>3.1.3</b> Orthogonal Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="orthogonality.html"><a href="orthogonality.html#orthonormal-sets-and-orthogonal-matrices"><i class="fa fa-check"></i><b>3.2</b> Orthonormal Sets and Orthogonal Matrices</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-matrices"><i class="fa fa-check"></i><b>3.2.1</b> Orthogonal Matrices</a></li>
<li class="chapter" data-level="3.2.2" data-path="orthogonality.html"><a href="orthogonality.html#best-approximation"><i class="fa fa-check"></i><b>3.2.2</b> Best Approximation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="orthogonality.html"><a href="orthogonality.html#projection-and-idempotent-matrices"><i class="fa fa-check"></i><b>3.3</b> Projection and idempotent matrices</a></li>
<li class="chapter" data-level="3.4" data-path="orthogonality.html"><a href="orthogonality.html#gram-schmidt-process"><i class="fa fa-check"></i><b>3.4</b> Gram-Schmidt Process</a></li>
<li class="chapter" data-level="3.5" data-path="orthogonality.html"><a href="orthogonality.html#qr-factorization"><i class="fa fa-check"></i><b>3.5</b> QR Factorization</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="orthogonality.html"><a href="orthogonality.html#householder-qr-factorization"><i class="fa fa-check"></i><b>3.5.1</b> Householder QR factorization</a></li>
<li class="chapter" data-level="3.5.2" data-path="orthogonality.html"><a href="orthogonality.html#applications-of-qr-factorization"><i class="fa fa-check"></i><b>3.5.2</b> Applications of QR factorization</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="orthogonality.html"><a href="orthogonality.html#complexity"><i class="fa fa-check"></i><b>3.6</b> Complexity</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html"><i class="fa fa-check"></i><b>4</b> Eigenthings and Quadratic Forms</a>
<ul>
<li class="chapter" data-level="4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>4.1</b> Eigenvectors and Eigenvalues</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#additional-properties-of-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>4.1.1</b> Additional Properties of Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="4.1.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#left-eigenvectors-and-right-eigenvectors"><i class="fa fa-check"></i><b>4.1.2</b> Left Eigenvectors and Right Eigenvectors</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#qr-method"><i class="fa fa-check"></i><b>4.2</b> QR Method</a></li>
<li class="chapter" data-level="4.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#diagnolization-and-similar-matrices"><i class="fa fa-check"></i><b>4.3</b> Diagnolization and Similar Matrices</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#similarity"><i class="fa fa-check"></i><b>4.3.1</b> Similarity</a></li>
<li class="chapter" data-level="4.3.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#jordan-matrix"><i class="fa fa-check"></i><b>4.3.2</b> Jordan Matrix</a></li>
<li class="chapter" data-level="4.3.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#simultaneous-diagonalization"><i class="fa fa-check"></i><b>4.3.3</b> Simultaneous Diagonalization</a></li>
<li class="chapter" data-level="4.3.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>4.3.4</b> Cayley-Hamilton Theorem</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#symmetric-matrices"><i class="fa fa-check"></i><b>4.4</b> Symmetric Matrices</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#spectral-decomposition"><i class="fa fa-check"></i><b>4.4.1</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="4.4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#a-orthogonality"><i class="fa fa-check"></i><b>4.4.2</b> A-Orthogonality</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#quadratic-forms"><i class="fa fa-check"></i><b>4.5</b> Quadratic Forms</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#change-of-variable"><i class="fa fa-check"></i><b>4.5.1</b> Change of Variable</a></li>
<li class="chapter" data-level="4.5.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#classification-of-quadratic-forms"><i class="fa fa-check"></i><b>4.5.2</b> Classification of Quadratic Forms</a></li>
<li class="chapter" data-level="4.5.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#gershgorin-discs-and-diagonal-dominance"><i class="fa fa-check"></i><b>4.5.3</b> Gershgorin Discs and Diagonal Dominance</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#cholesky-factorization"><i class="fa fa-check"></i><b>4.6</b> Cholesky Factorization</a></li>
<li class="chapter" data-level="4.7" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#rayleigh-quotients"><i class="fa fa-check"></i><b>4.7</b> Rayleigh Quotients</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html"><i class="fa fa-check"></i><b>5</b> Singular Value Decomposition</a>
<ul>
<li class="chapter" data-level="5.1" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#singular-values"><i class="fa fa-check"></i><b>5.1</b> Singular Values</a></li>
<li class="chapter" data-level="5.2" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#svd-theorem"><i class="fa fa-check"></i><b>5.2</b> SVD</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#geometric-interpretation-of-svd"><i class="fa fa-check"></i><b>5.2.1</b> Geometric Interpretation of SVD</a></li>
<li class="chapter" data-level="5.2.2" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#uniqueness-of-svd"><i class="fa fa-check"></i><b>5.2.2</b> Uniqueness of SVD</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#matrix-norms"><i class="fa fa-check"></i><b>5.3</b> Matrix Norms</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#induced-norms"><i class="fa fa-check"></i><b>5.3.1</b> Induced Norms</a></li>
<li class="chapter" data-level="5.3.2" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#entry-wise-norm"><i class="fa fa-check"></i><b>5.3.2</b> Entry-wise Norm</a></li>
<li class="chapter" data-level="5.3.3" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#other-matrix-norms"><i class="fa fa-check"></i><b>5.3.3</b> Other Matrix Norms</a></li>
<li class="chapter" data-level="5.3.4" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#unitary-invariant-norms"><i class="fa fa-check"></i><b>5.3.4</b> Unitary Invariant Norms</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#low-rank-approximation"><i class="fa fa-check"></i><b>5.4</b> Low Rank Approximation</a></li>
<li class="chapter" data-level="5.5" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#pseudoinverse"><i class="fa fa-check"></i><b>5.5</b> Pseudoinverse</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#pseudoinverse-of-full-rank-matrix"><i class="fa fa-check"></i><b>5.5.1</b> Pseudoinverse of full rank matrix</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-system.html"><a href="linear-system.html"><i class="fa fa-check"></i><b>6</b> Solutions of Linear System Ax = b</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-system.html"><a href="linear-system.html#when-a-has-independent-columns-or-rows"><i class="fa fa-check"></i><b>6.1</b> When A Has Independent Columns or Rows</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="linear-system.html"><a href="linear-system.html#when-a-is-square"><i class="fa fa-check"></i><b>6.1.1</b> When A is square</a></li>
<li class="chapter" data-level="6.1.2" data-path="linear-system.html"><a href="linear-system.html#when-m-n-lesat-squares"><i class="fa fa-check"></i><b>6.1.2</b> When m &gt; n: Lesat Squares</a></li>
<li class="chapter" data-level="6.1.3" data-path="linear-system.html"><a href="linear-system.html#when-m-n-least-norms"><i class="fa fa-check"></i><b>6.1.3</b> When m &lt; n: Least Norms</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="linear-system.html"><a href="linear-system.html#otherwise"><i class="fa fa-check"></i><b>6.2</b> Otherwise</a></li>
<li class="chapter" data-level="6.3" data-path="linear-system.html"><a href="linear-system.html#ill-conditioned-matrices"><i class="fa fa-check"></i><b>6.3</b> Ill-Conditioned Matrices</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-system.html"><a href="linear-system.html#condition-number"><i class="fa fa-check"></i><b>6.3.1</b> Condition Number</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="special-matrices.html"><a href="special-matrices.html"><i class="fa fa-check"></i><b>7</b> Special Matrices</a></li>
<li class="part"><span><b>II Multivariate Calculus</b></span></li>
<li class="chapter" data-level="8" data-path="partial-derivatives.html"><a href="partial-derivatives.html"><i class="fa fa-check"></i><b>8</b> Partial Derivatives</a>
<ul>
<li class="chapter" data-level="8.1" data-path="partial-derivatives.html"><a href="partial-derivatives.html#limit-and-continuity"><i class="fa fa-check"></i><b>8.1</b> Limit and Continuity</a></li>
<li class="chapter" data-level="8.2" data-path="partial-derivatives.html"><a href="partial-derivatives.html#partial-derivative"><i class="fa fa-check"></i><b>8.2</b> Partial Derivative</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="partial-derivatives.html"><a href="partial-derivatives.html#gradient-and-directional-derivative"><i class="fa fa-check"></i><b>8.2.1</b> Gradient and Directional Derivative</a></li>
<li class="chapter" data-level="8.2.2" data-path="partial-derivatives.html"><a href="partial-derivatives.html#linearization-of-two-variable-functions"><i class="fa fa-check"></i><b>8.2.2</b> Linearization of Two-variable Functions</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="partial-derivatives.html"><a href="partial-derivatives.html#differentials"><i class="fa fa-check"></i><b>8.3</b> Differentials</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="partial-derivatives.html"><a href="partial-derivatives.html#continuity-partial-derivatives-and-differentiability"><i class="fa fa-check"></i><b>8.3.1</b> Continuity, Partial Derivatives and Differentiability</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="partial-derivatives.html"><a href="partial-derivatives.html#divergence-curl-and-laplacian"><i class="fa fa-check"></i><b>8.4</b> Divergence, Curl, and Laplacian</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="matrix-calculus.html"><a href="matrix-calculus.html"><i class="fa fa-check"></i><b>9</b> Matrix Calculus</a>
<ul>
<li class="chapter" data-level="9.1" data-path="matrix-calculus.html"><a href="matrix-calculus.html#the-chain-rule"><i class="fa fa-check"></i><b>9.1</b> The Chain Rule</a></li>
<li class="chapter" data-level="9.2" data-path="matrix-calculus.html"><a href="matrix-calculus.html#useful-identities-in-matirx-calculus"><i class="fa fa-check"></i><b>9.2</b> Useful Identities in Matirx Calculus</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="taylor-series.html"><a href="taylor-series.html"><i class="fa fa-check"></i><b>10</b> Taylor Series</a>
<ul>
<li class="chapter" data-level="10.1" data-path="taylor-series.html"><a href="taylor-series.html#convergence-of-taylor-series"><i class="fa fa-check"></i><b>10.1</b> Convergence of Taylor Series</a></li>
<li class="chapter" data-level="10.2" data-path="taylor-series.html"><a href="taylor-series.html#taylor-approximation-of-multivariate-functions"><i class="fa fa-check"></i><b>10.2</b> Taylor Approximation of Multivariate Functions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="multiple-integral.html"><a href="multiple-integral.html"><i class="fa fa-check"></i><b>11</b> Multiple Integral</a></li>
<li class="part"><span><b>III Probability Theory</b></span></li>
<li class="chapter" data-level="12" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html"><i class="fa fa-check"></i><b>12</b> Basics of Probability Theory</a>
<ul>
<li class="chapter" data-level="12.1" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#probabilty-space"><i class="fa fa-check"></i><b>12.1</b> Probabilty Space</a></li>
<li class="chapter" data-level="12.2" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#counting"><i class="fa fa-check"></i><b>12.2</b> Counting</a></li>
<li class="chapter" data-level="12.3" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#conditional-probability"><i class="fa fa-check"></i><b>12.3</b> Conditional Probability</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html"><i class="fa fa-check"></i><b>13</b> Random variables and moments</a>
<ul>
<li class="chapter" data-level="13.1" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#properties-of-expectation-and-variance"><i class="fa fa-check"></i><b>13.1</b> Properties of Expectation and Variance</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#stains-lemma"><i class="fa fa-check"></i><b>13.1.1</b> Stain’s Lemma</a></li>
<li class="chapter" data-level="13.1.2" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#random-vectors"><i class="fa fa-check"></i><b>13.1.2</b> Random Vectors</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#other-summaries"><i class="fa fa-check"></i><b>13.2</b> Other Summaries</a></li>
<li class="chapter" data-level="13.3" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#moment-generating-functions"><i class="fa fa-check"></i><b>13.3</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="inequalities.html"><a href="inequalities.html"><i class="fa fa-check"></i><b>14</b> Inequalities</a></li>
<li class="chapter" data-level="15" data-path="univariate-distributions.html"><a href="univariate-distributions.html"><i class="fa fa-check"></i><b>15</b> Univariate Distributions</a>
<ul>
<li class="chapter" data-level="15.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#uniform-distribution"><i class="fa fa-check"></i><b>15.1</b> Uniform Distribution</a></li>
<li class="chapter" data-level="15.2" data-path="univariate-distributions.html"><a href="univariate-distributions.html#normal-distribution"><i class="fa fa-check"></i><b>15.2</b> Normal Distribution</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#log-normal-distribution"><i class="fa fa-check"></i><b>15.2.1</b> Log Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="univariate-distributions.html"><a href="univariate-distributions.html#binomial-distribution-and-beta-distribution"><i class="fa fa-check"></i><b>15.3</b> Binomial Distribution and Beta Distribution</a></li>
<li class="chapter" data-level="15.4" data-path="univariate-distributions.html"><a href="univariate-distributions.html#poisson-distribution"><i class="fa fa-check"></i><b>15.4</b> Poisson Distribution</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#poisson-process"><i class="fa fa-check"></i><b>15.4.1</b> Poisson Process</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="univariate-distributions.html"><a href="univariate-distributions.html#exponential-distribution-and-gamma-distribution"><i class="fa fa-check"></i><b>15.5</b> Exponential Distribution and Gamma Distribution</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#properties"><i class="fa fa-check"></i><b>15.5.1</b> Properties</a></li>
<li class="chapter" data-level="15.5.2" data-path="univariate-distributions.html"><a href="univariate-distributions.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>15.5.2</b> Inverse Gamma Distribution</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="univariate-distributions.html"><a href="univariate-distributions.html#beta-distribution"><i class="fa fa-check"></i><b>15.6</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>16</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="16.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>16.1</b> Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#chi-square-distribution"><i class="fa fa-check"></i><b>16.1.1</b> Chi-square Distribution</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#dirichlet-distributon"><i class="fa fa-check"></i><b>16.2</b> Dirichlet Distributon</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="markov-chain.html"><a href="markov-chain.html"><i class="fa fa-check"></i><b>17</b> Markov Chain</a></li>
<li class="part"><span><b>IV Learning Theory</b></span></li>
<li class="chapter" data-level="18" data-path="the-learning-problem-framework.html"><a href="the-learning-problem-framework.html"><i class="fa fa-check"></i><b>18</b> The Learning Problem Framework</a>
<ul>
<li class="chapter" data-level="18.1" data-path="the-learning-problem-framework.html"><a href="the-learning-problem-framework.html#the-pac-learning-framework"><i class="fa fa-check"></i><b>18.1</b> The PAC Learning Framework</a></li>
</ul></li>
<li class="part"><span><b>V Optimization</b></span></li>
<li class="chapter" data-level="19" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html"><i class="fa fa-check"></i><b>19</b> Basics of Optimization</a>
<ul>
<li class="chapter" data-level="19.1" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#univariate-optimization"><i class="fa fa-check"></i><b>19.1</b> Univariate Optimization</a></li>
<li class="chapter" data-level="19.2" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#multivariate-optimization"><i class="fa fa-check"></i><b>19.2</b> Multivariate Optimization</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#method-of-lagrange-multiplier"><i class="fa fa-check"></i><b>19.2.1</b> ## Method of Lagrange Multiplier</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#convexity"><i class="fa fa-check"></i><b>19.3</b> Convexity</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#properties-of-convex-functions"><i class="fa fa-check"></i><b>19.3.1</b> Properties of Convex Functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="gradient-descent.html"><a href="gradient-descent.html"><i class="fa fa-check"></i><b>20</b> Gradient Descent</a></li>
<li class="part"><span><b>VI Applications</b></span></li>
<li class="chapter" data-level="21" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>21</b> Linear Models</a>
<ul>
<li class="chapter" data-level="21.1" data-path="linear-models.html"><a href="linear-models.html#ordinary-least-squares"><i class="fa fa-check"></i><b>21.1</b> Ordinary Least Squares</a>
<ul>
<li class="chapter" data-level="21.1.1" data-path="linear-models.html"><a href="linear-models.html#least-square-estimation"><i class="fa fa-check"></i><b>21.1.1</b> Least Square Estimation</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="linear-models.html"><a href="linear-models.html#weighted-least-squares"><i class="fa fa-check"></i><b>21.2</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="21.3" data-path="linear-models.html"><a href="linear-models.html#partial-least-squares"><i class="fa fa-check"></i><b>21.3</b> Partial Least Squares</a></li>
<li class="chapter" data-level="21.4" data-path="linear-models.html"><a href="linear-models.html#regularized-regression"><i class="fa fa-check"></i><b>21.4</b> Regularized Regression</a>
<ul>
<li class="chapter" data-level="21.4.1" data-path="linear-models.html"><a href="linear-models.html#ridge-regression"><i class="fa fa-check"></i><b>21.4.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="21.4.2" data-path="linear-models.html"><a href="linear-models.html#lasso"><i class="fa fa-check"></i><b>21.4.2</b> Lasso</a></li>
<li class="chapter" data-level="21.4.3" data-path="linear-models.html"><a href="linear-models.html#elastic-net"><i class="fa fa-check"></i><b>21.4.3</b> Elastic Net</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>22</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="22.0.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#pca-with-normalization"><i class="fa fa-check"></i><b>22.0.1</b> PCA with Normalization</a></li>
<li class="chapter" data-level="22.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#correspondence-analysis"><i class="fa fa-check"></i><b>22.1</b> Correspondence Analysis</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>23</b> Text Mining</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li>
  <a href="https://github.com/rstudio/bookdown" target="blank">Written with bookdown</a>
</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Notes for Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="singular-value-decomposition" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Singular Value Decomposition</h1>
<div id="singular-values" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Singular Values</h2>
<p>The singular value decomposition illustrates a way of decomposing <em>any</em> matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> into the form <span class="math inline">\(U \Sigma V^T\)</span>, where <span class="math inline">\(U = [\bar{u}_1 \;\; \cdots \;\; \bar{u}_n]\)</span> and <span class="math inline">\(V = [\bar{v}_1 \;\; \cdots \;\; \bar{v}_n]\)</span> are both orthogonal matrices, and <span class="math inline">\(\Sigma\)</span> a diagonal matrix with entries being the square root of the eigenvalues of <span class="math inline">\(A^TA\)</span> (perhaps plus some zeros).</p>
<p>Before proceeding to the theorem, let’s explore the motivating idea behind SVD. For (square) diagonalizable matrix <span class="math inline">\(A \in \mathbb{S}^{n}\)</span>, the absolute value of the eigenvalues measure the amounts that <span class="math inline">\(A\)</span> stretches or shrinks eigenvectors, consider the ratio between the length of <span class="math inline">\(\bar{x}\)</span> before and after left multiplied by <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
\frac{\|A\bar{x}\|}{\|\bar{x}\|} 
= \frac{\|\lambda\bar{x}\|}{\|\bar{x}\|}
= \frac{\lambda\|\bar{x}\|}{\|\bar{x}\|} = \lambda
\]</span>
If <span class="math inline">\(\lambda_1\)</span> is the greatest eigenvalue, then the corresponding eigenvector <span class="math inline">\(\bar{v}_1\)</span> identifies the direction in which <span class="math inline">\(A\)</span>’s stretching effect is greatest.</p>
<p>So, the question is, can we identify a similar ratio and direction for <em>rectangular</em> matrices <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, even though they does not have eigenvalues and eigenvectors?</p>
<p>The answer is yes. Note that maximize <span class="math inline">\(\frac{\|A\bar{x}\|}{\|\bar{x}\|}\)</span> (now <span class="math inline">\(\bar{x}\)</span> is any vector <span class="math inline">\(\in \mathbb{R}^n\)</span>) is equivalent to maximize <span class="math inline">\(\frac{\|A\bar{x}\|^2}{\|\bar{x}\|^2}\)</span></p>
<p><span class="math display">\[
\begin{split}
\frac{\|A\bar{x}\|^2}{\|\bar{x}\|^2} &amp;= \frac{(A\bar{x})^T(A\bar{x})}{\bar{x}^T\bar{x}} \\
&amp;= \frac{\bar{x}^T(A^TA)\bar{x}}{\bar{x}^T\bar{x}}
\end{split}
\]</span>
Since <span class="math inline">\(A^TA\)</span> is <strong>symmetric</strong>, this is the form of a Rayleigh quotients <a href="eigenthings-and-quadratic-forms.html#rayleigh-quotients">4.7</a>! We know that the largest possible value is of this quotient <span class="math inline">\(\lambda_1\)</span>, the greatest eigenvalue of <span class="math inline">\(A^TA\)</span>, with <span class="math inline">\(\bar{x} = \bar{v}_1\)</span>, among the <strong>orthonormal</strong> set <span class="math inline">\(\{\bar{v}_1, \cdots, \bar{v}_n\}\)</span>. Note that here <span class="math inline">\(V = [\bar{v}_1 \;\; \cdots \;\; \bar{v}_n]\)</span> is already a orthogonal matrix, previously denoted by <span class="math inline">\(Q\)</span>.</p>
<p>To sum up, the greatest possible stretching ratio of <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> on a vector <span class="math inline">\(\bar{x} \in \mathbb{R}^n\)</span> is <span class="math inline">\(\sqrt{\lambda_1}\)</span>. Generally, let <span class="math inline">\(\{\bar{v}_1, \cdots, \bar{v}_n\}\)</span> be a orthonormal basis for <span class="math inline">\(\mathbb{R}^n\)</span> consisting of eigenvectors of <span class="math inline">\(A^TA\)</span>, and <span class="math inline">\(\lambda_1, ..., \lambda_n\)</span> be the eigenvalues of <span class="math inline">\(A^TA\)</span>, for <span class="math inline">\(i = 1, \cdots, n\)</span></p>
<p><span class="math display">\[
\|A\bar{v}_i\| ^ 2 = \bar{v}_i^T(A^TA)\bar{v}_i = \lambda_i\bar{v}_i^T\bar{v}_i = \lambda_i
\]</span></p>
<p>From corollary <a href="eigenthings-and-quadratic-forms.html#cor:ata-pd">4.4</a>, we know that <span class="math inline">\(A^TA\)</span> are positive semidefinite matrices. Thus, <span class="math inline">\(\lambda_i \ge 0, \, i = 1, ..., n\)</span>, and we can find their square root <span class="math inline">\(\sigma_i = \sqrt{\lambda_i}\)</span>.</p>
<p><br></p>

<div class="definition">
<span id="def:singular-value" class="definition"><strong>Definition 5.1  (Singular values)  </strong></span>The singular values of <span class="math inline">\(A\)</span> are the square roots of the eigenvalues of <span class="math inline">\(A^TA\)</span>, denoted by <span class="math inline">\(\sigma_1, ..., \sigma_n\)</span>. That is, <span class="math inline">\(\sigma_i = \sqrt{\lambda_i}\)</span>, and they are often arranged in descending order so that <span class="math inline">\(\lambda_1 \ge \cdots \ge \lambda_n\)</span>. Geometrically, singular values of <span class="math inline">\(A\)</span> are the length of the vectors <span class="math inline">\(A\bar{v}_1, ..., A\bar{v}_n\)</span>, where <span class="math inline">\(\{\bar{v}_1, ..., \bar{v}_n\}\)</span> is the <em>orthonormal</em> basis of <span class="math inline">\(A^TA\)</span>’s eigenspace.
</div>
<p><br></p>

<div class="theorem">
<span id="thm:svd-rank" class="theorem"><strong>Theorem 5.1  </strong></span>Proceeding from previous definitons of singular values, and suppose <span class="math inline">\(A\)</span> has at least one nonzero singular values. Then <span class="math inline">\(\{A\bar{v}_1, ..., A\bar{v}_r\}\)</span> is an orthogonal basis for <span class="math inline">\(\mathcal{R}(A)\)</span>, and <span class="math inline">\(\text{rank} \;A = r\)</span>
</div>
<div class="pr">
<p>First, let’s examine that <span class="math inline">\(\{A\bar{v}_1, ..., A\bar{v}_r\}\)</span> is a orthogonal basis: any pair of two distinct vectors <span class="math inline">\(A\bar{v}_i, A\bar{v}_j, \; i,j = 1, ..., r\)</span> are orthogonal to each other</p>
<p><span class="math display">\[
\begin{split}
(A\bar{v}_i)^T(A\bar{v}_j) &amp;=  \bar{v}_i^TA^TA\bar{v}_j \\
&amp;= \bar{v}_i^T(\lambda_j\bar{v}_j) \\
&amp;= 0
\end{split}
\]</span></p>
<p>Next, we show that any vector in <span class="math inline">\(\mathcal{R}(A)\)</span> is a linear a combination of <span class="math inline">\(\{A\bar{v}_1, ..., A\bar{v}_r\}\)</span>. Note that <span class="math inline">\(\{\bar{v}_1, ..., \bar{v}_n\}\)</span> is a orthonormal basis of <span class="math inline">\(A^TA\)</span>’s eigenspace <span class="math inline">\(\mathbb{R}^n\)</span>. Therefore, for any vector <span class="math inline">\(\bar{y} = A\bar{x}\)</span> in <span class="math inline">\(\mathcal{R}(A)\)</span> , there exists <span class="math inline">\(\bar{x} = c_1\bar{v}_1 + \cdots + c_n\bar{v}_n\)</span>, thus</p>
<p><span class="math display">\[
\begin{split}
\bar{y} &amp;= A\bar{x} = A(c_1\bar{v}_1 + \cdots + c_n\bar{v}_n) \\
&amp;= c_1 A \bar{v}_1 + \cdots + c_r A \bar{v}_r + c_{r+1} A \bar{v}_{r+1} + \cdots + c_n A \bar{v}_n 
\end{split}
\tag{1}
\]</span>
Since <span class="math inline">\(\lambda_{r+1} = \cdots = \lambda_{n} = 0\)</span>, <span class="math inline">\(A\bar{v}_{r+1}, ..., A\bar{v}_{n}\)</span> have length <span class="math inline">\(0\)</span>: they are zero vectors. And (1) is reduced to</p>
<p><span class="math display">\[
\bar{y} = c_1 A \bar{v}_1 + \cdots + c_r A\bar{v}_r
\]</span></p>
<p>Thus any <span class="math inline">\(\bar{y} \in \mathcal{R}(A)\)</span> is in Span<span class="math inline">\(\{A\bar{v}_1, ..., A\bar{v}_r\}\)</span>, and <span class="math inline">\(\{A\bar{v}_1, ..., A\bar{v}_r\}\)</span> is an orthogonal basis for <span class="math inline">\(\mathcal{R}(A)\)</span>. This also shows that the column rank of <span class="math inline">\(A\)</span> is equal to its number of nonzero singular values.</p>
</div>
</div>
<div id="svd-theorem" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> SVD</h2>
<p>Let’s begin SVD by the <span class="math inline">\(m \times n\)</span> diagonal matrix <span class="math inline">\(\Sigma\)</span> of the form</p>
<p><span class="math display">\[
\Sigma = \begin{bmatrix}
\sigma_1 \\
&amp; \ddots &amp;  \\ 
&amp; &amp; \sigma_r \\
&amp; &amp; &amp; 0 \\
&amp; &amp; &amp; &amp; \ddots \\
&amp; &amp; &amp; &amp; &amp; 0 \\
\end{bmatrix}
\tag{1}
\]</span></p>
<p>There are <span class="math inline">\(r\)</span> nonzero entries on the diagonal, being <span class="math inline">\(A\)</span>’s nonzero singular values, and the left positions are filled by <span class="math inline">\(0\)</span> to form a <span class="math inline">\(m \times n\)</span> matrix. If <span class="math inline">\(r\)</span> equals <span class="math inline">\(m\)</span> or <span class="math inline">\(n\)</span> or both, some or all of the zero blocks do not appear.</p>

<div class="theorem">
<p><span id="thm:SVD" class="theorem"><strong>Theorem 5.2  (The Singular Value Decomposition)  </strong></span>Let <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> with rank <span class="math inline">\(r\)</span>. There exists an diagonal matrix <span class="math inline">\(\mathbb{\Sigma} \in \mathbb{R}^{m \times n}\)</span> as in (1) for which the first <span class="math inline">\(r\ \times r\)</span> block is a diagonal matrix with the first <span class="math inline">\(r\)</span> singular values of <span class="math inline">\(A\)</span> on its diagonal, and there exist <span class="math inline">\(U \in \mathbb{R}^{m \times m}\)</span> and <span class="math inline">\(V \in \mathbb{R}^{n \times n}\)</span> such that</p>
<span class="math display">\[
A = U \Sigma V^T
\]</span>
</div>
<div class="pr">
<p>Since <span class="math inline">\(A\)</span> has <span class="math inline">\(r\)</span> nonzero singular values which measure the length of <span class="math inline">\(A\bar{v}_i, \; i = 1, ...n\)</span>, there exists orthogonal basis <span class="math inline">\(\{A\bar{v}_1, ..., A\bar{v}_r\}\)</span> for <span class="math inline">\(\mathcal{R}(A)\)</span>, we can further normalize the set to produce the <em>orthonormal</em> set <span class="math inline">\(\bar{u}_1, ..., \bar{u}_r\)</span>:</p>
<p><span class="math display">\[
\bar{u}_i = \frac{A\bar{v}_i}{\sigma_i}, \;\; i = 1, ..., r
\]</span>
Now we can extend <span class="math inline">\(\{\bar{u}_1, ..., \bar{u}_r\}\)</span> to an orthonormal basis <span class="math inline">\(\{\bar{u}_1, ..., \bar{u}_m\}\)</span> of <span class="math inline">\(\mathbb{R}^m\)</span>, and let</p>
<p><span class="math display">\[
U = [\bar{u}_1 \;\; \cdots \;\; \bar{u}_m], \quad V = [\bar{v}_1 \;\; \cdots \;\; \bar{v}_n]
\]</span></p>
<p>and <span class="math inline">\(\Sigma\)</span> be as be as in (1) above. Write out</p>
<p><span class="math display">\[
\begin{split}
U\Sigma &amp;= [\bar{u}_1 \;\; \cdots \;\; \bar{u}_r \;\; \cdots \;\; \bar{u}_m]_{m \times m}
\begin{bmatrix}
\sigma_1 \\
&amp; \ddots &amp;  \\ 
&amp; &amp; \sigma_r \\
&amp; &amp; &amp; 0 \\
&amp; &amp; &amp; &amp; \ddots \\
&amp; &amp; &amp; &amp; &amp; 0 \\
\end{bmatrix}_{m\times n} \\
&amp;= [\sigma_1\bar{u}_1 \;\; \cdots \;\;   \sigma_r\bar{u}_r \;\; \bar{0} \;\;  \cdots \;\;  \bar{0}] \\
&amp; = [A\bar{v}_1 \;\; \cdots \;\; A\bar{v}_r \;\; A\bar{v}_{r+1} \;\; \cdots \;\; A \bar{v}_n] \\
&amp;= A_{m \times n}V_{n \times n}
\end{split}
\]</span></p>
<p>And because <span class="math inline">\(V\)</span> is orthogonal</p>
<p><span class="math display">\[
A = U \Sigma V^{-1} =  U \Sigma V^{T}
\]</span></p>
</div>
<p><span class="math inline">\(\bar{u}_i\)</span> and <span class="math inline">\(\bar{v}_i\)</span> are called <em>left eigenvector</em> and <em>right eigenvector</em> of <span class="math inline">\(A\)</span> respectively.</p>
<p>It’s easy to verify that the spectral decomposition <a href="eigenthings-and-quadratic-forms.html#spectral-decomposition">4.4.1</a> is a special case of SVD when <span class="math inline">\(A \in \mathbb{R}^{n}, \;\; m = n\)</span>. In that case, <span class="math inline">\(\Sigma\)</span> is a square matrix and <span class="math inline">\(U\)</span> is equal to <span class="math inline">\(V\)</span>.</p>
<p>When <span class="math inline">\(\Sigma\)</span> contains rows or columns of zeros (i.e, <span class="math inline">\(r &lt; \min(m, n)\)</span>), we can write SVD in a more compact form. Divide <span class="math inline">\(U, \Sigma, V\)</span> into submatrices</p>
<p><span class="math display">\[
U = [U_r \;\; U_{m-r}], \quad \text{where } U_r = [\bar{u}_1 \;\; \cdots \;\; \bar{u}_r] \\
V = [V_r \;\; V_{m-r}], \quad \text{where } V_r = [\bar{v}_1 \;\; \cdots \;\; \bar{v}_r] \\
\Sigma = 
\begin{bmatrix}
D &amp; 0 \\
0 &amp; 0
\end{bmatrix}
\quad \text{where } D = 
\begin{bmatrix}
\lambda_1 \\ 
 &amp; \ddots \\
 &amp; &amp; \lambda_r
\end{bmatrix}
\]</span>
The partitioned matrix multiplication shows that</p>
<p><span class="math display">\[
A = [U_r \;\; U_{m-r}]
\begin{bmatrix}
D &amp; 0 \\
0 &amp; 0
\end{bmatrix}
\begin{bmatrix}
V_r^T \\
V_{n-r}^T
\end{bmatrix}
= U_rDV_{r}^T
\]</span>
This more compact form is called the <strong>reduced form of SVD</strong>.</p>
<p>Another way to write this is</p>
<p><span class="math display">\[
A = \sum_{i=1}^{r}{\sigma_i}\bar{u}_i\bar{v}_i
\]</span></p>
<hr>
<p>Right multiply the non-compact form <span class="math inline">\(A = U\Sigma V^T\)</span> by <span class="math inline">\(A^T\)</span> , we get the spectral decomposition of symmetric matrix <span class="math inline">\(AA^T\)</span>.</p>
<p><span class="math display">\[
AA^T = (U \Sigma V^T)(U \Sigma V^T)^T = U \Sigma \Sigma^T VV^TU^T = U (\Sigma\Sigma^T) U^T       \tag{1}
\]</span></p>
<p>Therefore, <span class="math inline">\([\bar{u}_1 \;\; \cdots \;\; \bar{u}_n]\)</span> are revealed as the orthonormal basis for <span class="math inline">\(AA^T\)</span>’s eigenspace, as <span class="math inline">\([\bar{v}_1 \;\; \cdots \;\; \bar{v}_n]\)</span> are for <span class="math inline">\(A^TA\)</span>.</p>
<p>Formula (1) echoes the fact that <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> have the same set of nonzero eigenvalues, because <span class="math inline">\(\Sigma\Sigma^T = \Sigma^T\Sigma\)</span></p>
<p>In fact, if were to ask for a direction in which <span class="math inline">\(A^T\)</span> has its greatest stretching effect instead of <span class="math inline">\(A\)</span>, we would still result in the equivalent decomposition <span class="math inline">\(A^T = V\Sigma U^T\)</span>, with <span class="math inline">\(\bar{v}_i = \frac{A\bar{u}_i}{\sigma_i}\)</span>.</p>
<p>It’s also easy to test that <span class="math inline">\(\{A\bar{u}_1, ..., A\bar{u}_r\}\)</span> produces an orthogonal basis for <span class="math inline">\(\mathcal{R}(A^T)\)</span> . The process is analogous to theorem <a href="singular-value-decomposition.html#thm:svd-rank">5.1</a> where <span class="math inline">\(\{A\bar{v}_1, ..., A\bar{v}_r\}\)</span> are shown to span <span class="math inline">\(\mathcal{R}(A)\)</span>.</p>
<p>For any vector <span class="math inline">\(\bar{y}\)</span> in <span class="math inline">\(\mathcal{R}(A)\)</span>, we have</p>
<p><span class="math display">\[
\begin{align*}
\bar{y} &amp;= A^T\bar{x} \\
&amp;= A^T(c_1\bar{u}_1 + \cdots + c_1\bar{u}_n) \\
&amp;= c_1A\bar{u}_1 + \cdots + c_rA\bar{u}_r + \bar{0} + \cdots + \bar{0} &amp;&amp; (\text{because }A\bar{u}_i = \sigma_i\bar{v}_i)\\
&amp;= c_1A\bar{u}_1 + \cdots + c_rA\bar{u}_r 
\end{align*}
\]</span></p>
<p>Thus, SVD can be thought of an connection between two spectral decomposition</p>
<p><span class="math display">\[
A^TA = V (\Sigma^T\Sigma)V^T  \\
AA^T = U (\Sigma\Sigma^T) U^T
\]</span></p>
<p>This shed light on the relationship between SVD and the fundamental theorem of linear algebra <a href="vector-spaces.html#thm:fundamental-theorem">2.2</a></p>
<table>
<thead>
<tr class="header">
<th align="center">Subspace</th>
<th align="center">Columns</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\mathcal{R}(A)\)</span></td>
<td align="center">the first <span class="math inline">\(r\)</span> columns of <span class="math inline">\(U\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\mathcal{R}(A^T)\)</span></td>
<td align="center">the first <span class="math inline">\(r\)</span> columns of <span class="math inline">\(V\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\mathcal{N}(A)\)</span></td>
<td align="center">the last <span class="math inline">\(n - r\)</span> columns of <span class="math inline">\(V\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\mathcal{N}(A^T)\)</span></td>
<td align="center">the last <span class="math inline">\(m - r\)</span> columns of <span class="math inline">\(U\)</span></td>
</tr>
</tbody>
</table>
<div id="geometric-interpretation-of-svd" class="section level3" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Geometric Interpretation of SVD</h3>
<p><a href="https://www.cs.cornell.edu/courses/cs322/2008sp/stuff/TrefethenBau_Lec4_SVD.pdf" class="uri">https://www.cs.cornell.edu/courses/cs322/2008sp/stuff/TrefethenBau_Lec4_SVD.pdf</a></p>
<p><a href="http://db.cs.duke.edu/courses/cps111/spring07/notes/12.pdf" class="uri">http://db.cs.duke.edu/courses/cps111/spring07/notes/12.pdf</a></p>
</div>
<div id="uniqueness-of-svd" class="section level3" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Uniqueness of SVD</h3>
<p>The title of this section may be confusing, because the SVD is “almost” unique but not exactly so. Nonetheless, we will first come to the good part.</p>
<p>The good news is, the diagonal matrix <span class="math inline">\(\Sigma\)</span> with singular values are uniquely determined by <span class="math inline">\(A\)</span>, as long as we follow the convention to align singular values in ascending order. The uniqueness of <span class="math inline">\(\Sigma\)</span> is based directly on the fact that <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> have the (same) fixed set of nonzero eigenvalues.</p>
<p>Now we come to the real ambiguity about SVD, which lies in the choice of <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span>. Remember the decomposition</p>
<p><span class="math display">\[
A = \sum_{i = 1}^{r} \sigma_i \bar{u}_i \bar{v}_i^T
\]</span></p>
<p>It is immediately obvious that we can flipped <span class="math inline">\(\bar{u}_i\)</span> and <span class="math inline">\(\bar{v}_i\)</span> in pairs, and the decomposition would still be the same. In other words, we can exert the same permutation upon <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span>. Moreover, we can multiply one side by a nonzero factor and multiply its reciprocal on the side. This first ambiguity may not be of much practical value, and can be avoided by setting additional constraints.</p>
<p>The second source of ambiguity, however, is more subtle and complicated. if <span class="math inline">\(A\)</span> has repeated singular values, we pick an arbitrary orthogonal matrix <span class="math inline">\(D\)</span>, then <span class="math inline">\(U^* = UD\)</span> and <span class="math inline">\(V^{*} = VD\)</span> would still form a singular value decomposition <span class="math inline">\(A = U^*\Sigma {V^{*}}^T\)</span>. For instance, the identity matrix has an infinity of SVDs all of the form</p>
<p><span class="math display">\[
I = UIU^{T}
\]</span></p>
<p>where <span class="math inline">\(U\)</span> can be any orthogonal matrix of suitable size. If <span class="math inline">\(A\)</span> is not full rank, i.e. there are zero singular values. There is even more freedom. Suppose the rank of a tall matrix <span class="math inline">\(A\)</span> is <span class="math inline">\(r\)</span>, then its null space is of dimension <span class="math inline">\(n - r\)</span>. Here the <span class="math inline">\(r + 1\)</span>th through the <span class="math inline">\(m\)</span>th columns of <span class="math inline">\(U\)</span> are less constrained, and can be any set of <span class="math inline">\(m - r\)</span> orthonormal vectors in the in the left null space of <span class="math inline">\(A\)</span>. Moreover, the choice of these columns of <span class="math inline">\(U\)</span>
can be chosen independently of the last <span class="math inline">\(n - r\)</span> columns of <span class="math inline">\(V\)</span> (which form a orthonormal basis for the null space of <span class="math inline">\(A\)</span>).</p>
</div>
</div>
<div id="matrix-norms" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Matrix Norms</h2>
<p>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be matrices conformable for the operations below, a matrix norm should at first satisfy 3 axioms of norm:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\|A\| \ge 0\)</span> for all <span class="math inline">\(x \in X\)</span>, with equality if and only if all elements of <span class="math inline">\(A\)</span> is zero (nonnegative)<br />
</li>
<li><span class="math inline">\(\|\alpha A\| = |\alpha|\,\|A\|\)</span> (homogeneous)<br />
</li>
<li><span class="math inline">\(\|A + B\| &lt; \|A\| + \|B\|\)</span> (triangular inequality)</li>
</ol>
<p>Additionally, in the case of square matrices (thus, m = n), some (but not all) matrix norms satisfy the following condition, which is related to the fact that matrices are more than just vectors</p>
<ol start="4" style="list-style-type: decimal">
<li><span class="math inline">\(\|AB\| &lt; \|A\|\,\|B\|\)</span> for <span class="math inline">\(A, B \in \mathbb{R}^{n \times n}\)</span></li>
</ol>
<p>A matrix norm that satisfies this additional property is called a <strong>submultiplicative</strong> norm.</p>
<p>There are 2 main categories of matrix norms.</p>
<ul>
<li>induced norms (defined in terms of vector norms)<br />
</li>
<li>entry-wise norms (treat <span class="math inline">\(A_{m \times n}\)</span> like a long vector with <span class="math inline">\(m \times n\)</span> elements)</li>
</ul>
<div id="induced-norms" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Induced Norms</h3>
<p>Induced norms define matrix norms in terms of vectors, also called <em>operator norm</em> since <span class="math inline">\(A\)</span> acts like an operator in this definition. Note that matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> maps a vector <span class="math inline">\(\bar{x} \not = 0\)</span> from <span class="math inline">\(\mathbb{R}^n\)</span> to <span class="math inline">\(\mathbb{R}^m\)</span>. In particular, if the p-norm is used for both <span class="math inline">\(\mathbb{R}^n\)</span> and <span class="math inline">\(\mathbb{R}^m\)</span>, then the induced norm is</p>
<p><span class="math display">\[
\|A\|_p = \max \frac{\|A\bar{x}\|_p}{\|\bar{x}\|_p}
\]</span>
The subscript <span class="math inline">\(p\)</span> can be misleading, because the appropriate name for this matrix norm may not be “p-norm”, but rather “induced norm when p-norm is used in both spaces”. The p-norm of a matrix meaning usually means entry-wise p-norms or the Scatten p-norms defined in subsequent sections.</p>
<p>In the special cases where <span class="math inline">\(p = 1, 2, ..., \infty\)</span>, <span class="math inline">\(\|A\|_p\)</span> is the maximum absolute column sums, largest singular value, and the maximum absolute row sums</p>
<p><span class="math display">\[
\begin{aligned}
\|A\|_1 &amp;= \max \sum_{i=1}^{m}{|A_{ij}|} \\
\|A\|_2 &amp;= \sigma_1 \\
\|A\|_{\infty} &amp;= \max \sum_{j=1}^{n}{|A_{ij}|}
\end{aligned}
\]</span>
For symmetric matrix A, we have</p>
<p><span class="math display">\[
\|A\|_1 = \|A\|_{\infty}
\]</span>
and</p>
<p><span class="math display">\[
\|A\|_2 = \lambda_1
\]</span>
The induced 2-norm are also called the <strong>spectral norm</strong>.</p>
<p>By definition, the following inequality holds for induced matrix norms</p>
<p><span class="math display">\[
\|A\bar{x}\|_P \le \|A\|_p\|\bar{x}\|_p
\]</span></p>
<hr>

<div class="proposition">
<p><span id="prp:unnamed-chunk-1" class="proposition"><strong>Proposition 5.1  </strong></span>Induced matrix norms satisfies the additional submultiplicative property in that</p>
<span class="math display">\[
\|AB\|_p \le \|A\|_p\|B\|_p
\]</span>
</div>
<div class="pr">
<p>For any <span class="math inline">\(\bar{x} \in \mathbb{R}^n\)</span></p>
<p><span class="math display">\[
\|AB\bar{x}\|_p \le \|A\|_p\|B\bar{x}\|_p \le \|A\|_p\|B\|_p\|\bar{x}\|_p
\]</span>
si</p>
<p><span class="math display">\[
\|AB\|_p = \max \frac{\|A\bar{x}\|_p}{\|\bar{x}\|_p} \le \max \frac{\|A\|_p\|B\|_p\|\bar{x}\|_p}{\|\bar{x}\|_p} = \|A\|_p\|B\|_p
\]</span></p>
</div>
</div>
<div id="entry-wise-norm" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Entry-wise Norm</h3>
<p>Entry-wise norms treat an <span class="math inline">\(m \times n\)</span> matrix as a long vector of size <span class="math inline">\(m \times n\)</span>, denoted by <span class="math inline">\(\text{vec}(A)\)</span>. For example, using the p-norm for vectors, we get</p>
<p><span class="math display">\[
\|A\|_{p,p} = \|\text{vec}(A)\|_p = \Bigg(\sum_{j=1}^n\sum_{i=1}^{m}{|a_{ij}|}^p \Bigg)^{\frac{1}{p}}
\]</span></p>
<p>More generally, the p,q norm is defined by</p>
<p><span class="math display">\[
\|A\|_{p, q} = \Bigg (  \sum_{j=1}^n \Big (\sum_{i=1}^{n}{|a_{ij}|}^p \Big)^{\frac{q}{p}}    \Bigg)^{\frac{1}{q}}
\]</span>
Another important member norm of this norm family is the <strong>Frobenius norm</strong>, or the F-norm.</p>
<p><span class="math display">\[
\| A\|_F = \sqrt{\sum_{j=1}^n\sum_{j=1}^{m}{a_{ij}^2}} = \sqrt{\text{tr}(A^TA)} = \sqrt{\sum_{i=1}^{\min(m,n)}{\sigma_i^2}}  
\]</span>
where <span class="math inline">\(\sigma_i\)</span> is the nonzero singular value of <span class="math inline">\(A\)</span>.</p>

<div class="proposition">
<span id="prp:unnamed-chunk-2" class="proposition"><strong>Proposition 5.2  </strong></span>The F-norm is a submultiplicative norm.
</div>
<div class="pr">
<p>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are of appropriate size such that</p>
<p><span class="math display">\[
A = 
\begin{bmatrix}
a_1^T \\
\vdots \\
a_m^T
\end{bmatrix}
, \;
B = 
\begin{bmatrix}
b_1 &amp; \cdots &amp; b_m
\end{bmatrix} 
\\
\quad \\
\begin{aligned}
\|AB\| &amp;= \sqrt{\sum_{i, j}{(a_i^Tb_j)^2}} \\
&amp; \le \sqrt{\sum_{i, j}{\| a_i \|^2 \| b_j\|^2}} \\
&amp;= \sqrt{\sum_{i}{\|a_i\|^2}} \sqrt{\sum_{j}{\|b_j\|^2}} \\
&amp;= \|A\| \|B\|
\end{aligned}
\]</span></p>
<p>(The first inequality comes from the Cauchy-Schwarz inequality <span class="math inline">\(a \cdot b \le \|a\| \|b\|\)</span>)</p>
</div>
</div>
<div id="other-matrix-norms" class="section level3" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> Other Matrix Norms</h3>
<p>One can pick to one of two ways to generalize the F-norm. One is a generalization of the direct definition that F-norm is <span class="math inline">\(||\text{vec}(A)||_2\)</span>. For <span class="math inline">\(p \ge 1\)</span>, it is the <strong>Frobenius-p</strong> norm:</p>
<p><span class="math display">\[
\| A \|_{F_p} = \Bigg (\sum_{i,j}{|a_{ij}|^p} \Bigg)^{\frac{1}{p}}
\]</span>
The Frobenius p-norm is the ordinary Frobenius norm.</p>
<p>Another generalization stems from the relationship between F-norm and singular values of <span class="math inline">\(A\)</span>. The <strong>Schatten p norm</strong> is the p-norm of the vector composed of <span class="math inline">\(A\)</span>’s singular values</p>
<p><span class="math display">\[
\|A\|_{S_p} = \Big( \sum_{i = 1}^{\min(m, n)}{\sigma_i}^p\Big)^{\frac{1}{p}}
\]</span></p>
<p>The most familiar choices of <span class="math inline">\(p\)</span> are <span class="math inline">\(1, 2, ..., \infty\)</span>. Spectral norm and F-norm can be viewed as special cases of the Schatten norm. The case <span class="math inline">\(p = 2\)</span> yields the F-norm, and <span class="math inline">\(p = \infty\)</span> the spectral norm.</p>
<p>Finally, <span class="math inline">\(p = 1\)</span> yields the <strong>nuclear norm</strong> (also known as the trace norm), defined as</p>
<p><span class="math display">\[
\|A\|_N = \sigma_1 + \cdots + \sigma_r
\]</span></p>
</div>
<div id="unitary-invariant-norms" class="section level3" number="5.3.4">
<h3><span class="header-section-number">5.3.4</span> Unitary Invariant Norms</h3>

<div class="definition">
<p><span id="def:unitary-invariant" class="definition"><strong>Definition 5.2  (unitary invariant norms)  </strong></span>A matrix norm <span class="math inline">\(\|\cdot \|\)</span> is said to be unitary invariant if for all orthogonal matrices <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> of appropriate size</p>
<span class="math display">\[
\|A\| = \|UAV\|
\]</span>
</div>
<p>(unitary matrices refers to orthogonal matrices with complex-valued entries, but I focus on real matrices here.)</p>
<p>We mentioned that spectral norm, F-norm and nuclear norm are all unitary invariant norms. More than that, these 3 norms of any matrix stay the <strong>same</strong> when <span class="math inline">\(A\)</span> is multiplied by an orthogonal matrix.</p>
<p>Essentially, <strong>if a norm depends only on the singular values of a matrix</strong>, it is unitary invariant. Since for such norms:</p>
<p><span class="math display">\[
\|A\| = \|\Sigma\| \qquad \text{because the norm only depend on singular values}
\]</span>
Multiply two orthogonal matrices <span class="math inline">\(U\)</span> and <span class="math inline">\(V^T\)</span> on each side,</p>
<p><span class="math display">\[
\begin{aligned}
\|UA V^T\| &amp; = \| U \Sigma V^T\| \qquad \text{multiply by orthogonal matrix does not change norm}\\
\|UAV^T\| &amp;= \|A\|
\end{aligned}
\]</span></p>
<p>The spectral norm (induced p-norm), F-norm and Schatten norm are all unitary invariant. Because they can all be expressed in terms of singular values of <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\|A\|_2 &amp;= \sigma_1 \\
\|A\|_F &amp;= \sqrt{\sigma_1^2 + \cdots + \sigma_r^2} \\
\|A\|_N &amp;= \sigma_1 + \cdots + \sigma_r
\end{aligned}
\]</span></p>
</div>
</div>
<div id="low-rank-approximation" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Low Rank Approximation</h2>

<div class="theorem">
<p><span id="thm:eckart-young" class="theorem"><strong>Theorem 5.3  (Eckart–Young–Mirsky)  </strong></span>Let <span class="math inline">\(\|\cdot\|\)</span> be a unitary invariant norm. Suppose <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> where <span class="math inline">\(m &gt; n\)</span>, has svd <span class="math inline">\(A = \sum_{i = 1}^n \sigma_i \bar{u}_i \bar{v}^T_i\)</span>. Then the best rank-k approximation to <span class="math inline">\(A\)</span>, where <span class="math inline">\(k \le \text{rank}(A)\)</span>, is given by</p>
<p><span class="math display">\[
A_k = \sum _{i=1}^{k}\sigma_i \bar{u}_i \bar{v}_i^T
\]</span></p>
<p>in the sense that for any other rank-k matrix <span class="math inline">\(\tilde{A}\)</span></p>
<span class="math display">\[
\|A -  A_k \| \le \|A - \tilde{A} \|
\]</span>
</div>

<div class="todo">
Proof for Eckart-Young theorem
</div>
<p>A measure of the quality of the approximation is given by</p>
<p><span class="math display">\[
\frac{\|A_k\|^2}{\| A\|^2} = \frac{\sigma_1^2 + \cdots + \sigma_k^2}{\sigma_1^2 + \cdots + \sigma_r^2}
\]</span></p>
</div>
<div id="pseudoinverse" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Pseudoinverse</h2>

<div class="definition">
<p><span id="def:pseudo-inverse" class="definition"><strong>Definition 5.3  (pseudo inverse)  </strong></span>For any matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, a pseudoinverse of <span class="math inline">\(A\)</span> is defined as a matrix <span class="math inline">\(A^{+} \in \mathbb{R}^{n \times m}\)</span> satisfying aall 4 the so-called Moore-Penrose conditions</p>
<span class="math display">\[
\begin{aligned}
&amp;1. \quad AA^{+}A  &amp;= A &amp;&amp; AA^{+} \text{ needs not be the identity matrix, but it maps all columns of } A \text{ to themselves} \\
&amp;2. \quad A^{+}A A^{+} &amp;= A  &amp;&amp; A^{+} \text{ acts like an weak inverse} \\
&amp;3. \quad (AA^{+})^{T} &amp;= AA^{+}  &amp;&amp;  AA^{+} \text{ is symmetric} \\
&amp;3. \quad (A^{+}A)^{T} &amp;= A^{+}A  &amp;&amp;  A^{+}A \text{ is symmetric}
\end{aligned}
\]</span>
</div>
<p>For this reason, the pseudoinverse <span class="math inline">\(A^{+}\)</span> is also called moore-penrose inverse. It’s important to realize the existence and uniqueness (which we do not prove here) of a pseudoinverse. <strong>There is one precise pseudoinverse <span class="math inline">\(A^{+}\)</span> existing for any matrix <span class="math inline">\(A\)</span> satisfying the 4 properties</strong>. If we find a pseudoinverse, it is the pseudoinverse.</p>

<div class="rmdnote">
<p>A matrix satisfying the first condition of the definition is known as a <em>generalized inverse</em>, denoted by <span class="math inline">\(A^{-}\)</span>. If the matrix also satisfies the second definition, it is called a <em>generalized reflexive inverse</em>. Generalized inverses always exist but are not in general unique. Uniqueness is a consequence of the last two conditions.</p>
It follows that the pseudoinverse is a stricter kind of generalized inverse.
</div>
<p>Note that this definition only gives criterion of the pseudoinverse, but not a formula for computing it. The following corollary will lead us to a computationally simple and accurate way to compute the pseudoinverse using the SVD.</p>

<div class="corollary">
<p><span id="cor:unnamed-chunk-5" class="corollary"><strong>Corollary 3.1  </strong></span>The pseudoinverse of any diagonal <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(\Lambda\)</span> with <span class="math inline">\(r\)</span> nonzero entries</p>
<p><span class="math display">\[
\Sigma = \begin{bmatrix}
\lambda_1 \\ 
&amp; \ddots \\ 
&amp; &amp; \lambda_r \\ 
&amp; &amp; &amp; 0 \\ 
&amp; &amp; &amp; &amp; \ddots \\
&amp; &amp; &amp; &amp; &amp; 0 
\end{bmatrix}_{m \times n}
\]</span></p>
<p>is given by</p>
<p><span class="math display">\[
\Sigma^{+} = 
\begin{bmatrix}
\frac{1}{\lambda_1} \\ 
&amp; \ddots \\ 
&amp; &amp; \frac{1}{\lambda_r} \\ 
&amp; &amp; &amp; 0 \\ 
&amp; &amp; &amp; &amp; \ddots \\
&amp; &amp; &amp; &amp; &amp; 0 
\end{bmatrix}_{n \times m}
\]</span></p>
That is, the pseudoinverse of a diagonal matrix can be obtained by taking the reciprocal of all nonzero elements, with all zero entries stay the same, and then transpose the matrix.
</div>
<div class="pr">
<p>We start the proof with 1-by-1 matrices, which is essentially a number. For any <span class="math inline">\(x \in \mathbb{R}\)</span>, we defined</p>
<p><span class="math display">\[
x^{+} = 
\begin{cases} 
x^{-1} &amp;&amp; \text{if } x \not= 0 \\
0 &amp;&amp; \text{if } x = 0
\end{cases}
\]</span>
Then we can verify that such <span class="math inline">\(x^{+}\)</span> guarantee 4 moore-penrose conditions. Because the pseudoinverse is unique, we conclude that <span class="math inline">\(x^{+}\)</span> is a pseudoinverse of <span class="math inline">\(x\)</span> (interpreted as a 1-by-1 matrix).</p>
<p>Then, with <span class="math inline">\(\Sigma = \text{diag}(\lambda_1, ..., \lambda_r, 0, ... 0)_{m \times n}\)</span> and its claimed pseudoinverse <span class="math inline">\(\Sigma^{+} = \text{diag}(1 / \lambda_1, ..., 1 / \lambda_r, 0, ... 0)_{n \times m}\)</span>, we can verify they obey the rules in definition @def(pseudo-inverse).</p>
<p>one
<span class="math display">\[
\Sigma \Sigma^{+} \Sigma = \Sigma
\]</span></p>
<p>two
<span class="math display">\[
\Sigma^{+} \Sigma \Sigma^{+} = \Sigma^{+}
\]</span>
three</p>
<p><span class="math display">\[
(\Sigma \Sigma^{+})^{T} = \Sigma \Sigma^{+}
\]</span>
four</p>
<p><span class="math display">\[
(\Sigma^{+}\Sigma)^{T}  =\Sigma^{+}\Sigma
\]</span></p>
</div>
<p>With this result, we can give a formula of the pseudoinverse of any matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>.</p>

<div class="theorem">
<p><span id="thm:compute-pseudo-inverse" class="theorem"><strong>Theorem 5.4  </strong></span>For any matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, the svd is</p>
<p><span class="math display">\[
A = U\Sigma V^{T}
\]</span>
Then, the unique pseudoinverse matrix of <span class="math inline">\(n \times m\)</span> is given by</p>
<span class="math display">\[
A^{+} = UD^{+}V^{T}
\]</span>
</div>
<p>Again, we can verify <span class="math inline">\(A^{+} = UD^{+}V^{T}\)</span> satisfy the 4 criterion above.</p>
<p>It may seems that <span class="math inline">\(A^{+}\)</span> may depend on specific choice of <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span>, since in Section <a href="singular-value-decomposition.html#uniqueness-of-svd">5.2.2</a> we mentioned that they are not unique in singular value decomposition. But <span class="math inline">\(A^{+}\)</span> is indeed unique, and depends only on <span class="math inline">\(A\)</span>. We can prove this using the geometric view of either the least square solution or the least-norm solution in chapter <a href="linear-system.html#linear-system">6</a>.</p>
<div id="pseudoinverse-of-full-rank-matrix" class="section level3" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> Pseudoinverse of full rank matrix</h3>
<p>When <span class="math inline">\(A_{m \times n}\)</span> is full column rank or full rank, it’s possible to give a more straight form of <span class="math inline">\(A^{+}\)</span> in terms of <span class="math inline">\(A\)</span> itself, without relying on the svd.</p>

<div class="corollary">
<p><span id="cor:one-sided-inverse" class="corollary"><strong>Corollary 5.1  </strong></span>When <span class="math inline">\(A\)</span> is full rank, the Moore-Penrose inverse can be directly calculated as follows</p>
<ul>
<li><p><strong>case</strong> <span class="math inline">\(n &lt; m\)</span> when <span class="math inline">\(A\)</span> is full column rank: <span class="math inline">\(A^{+} = (A^TA)^{-1}A^T\)</span></p></li>
<li><p><strong>case</strong> <span class="math inline">\(m &lt; n\)</span> when <span class="math inline">\(A\)</span> is full row rank: <span class="math inline">\(A^{+} = A^T(AA^T)^{-1}\)</span></p>
</div></li>
</ul>
<p>Note that these two expressions are based on the invertibility of the gram matrix when <span class="math inline">\(A\)</span> is full rank.</p>
<p>We will prove the first case here. Using svd, we have</p>
<p><span class="math display">\[
\begin{split}
(A^TA)^{-1}A^T &amp;= (V\Sigma^T U^TU \Sigma V^T)^{-1} V\Sigma^T U^T \\
&amp;= (V\Sigma^T\Sigma V^T)^{-1} V\Sigma^TU^T \\
\end{split}
\]</span></p>
<p>Let <span class="math inline">\(S = \Sigma^T\Sigma\)</span></p>
<p><span class="math display">\[
\begin{split}
(A^TA)^{-1}A^T &amp;= VS^{-1}V^TV\Sigma^TU^T \\
&amp;= VS^{-1}\Sigma^TU^T
\end{split}
\]</span>
Notice that <span class="math inline">\(S^{-1} = \text{diag}(\frac{1}{\lambda_1^2}, ..., \frac{1}{\lambda_n^2})_{n \times n}\)</span> and <span class="math inline">\(\Sigma^T = \text{diag}(\lambda_1, ..., \lambda_n, 0, ..., 0)_{n \times m}\)</span>, so that <span class="math inline">\(S^{-1}\Sigma^T = \text{diag}(\frac{1}{\lambda_1}, ..., \frac{1}{\lambda_n}, 0, ..., 0)_{n \times m} = \Sigma^{+}\)</span>. In conclusion, we obtain</p>
<p><span class="math display">\[
(A^TA)^{-1}A^T = V\Sigma^{+}U^T = A^{+}
\]</span>
when <span class="math inline">\(A\)</span> is full column rank.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="eigenthings-and-quadratic-forms.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-system.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/enixam/math-foundations/edit/master/svd.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
