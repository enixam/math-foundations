<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Basic Matrix Algebra | Mathematical Notes for Machine Learning</title>
  <meta name="description" content="Chapter 1 Basic Matrix Algebra | Mathematical Notes for Machine Learning" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Basic Matrix Algebra | Mathematical Notes for Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="enixam/math-foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Basic Matrix Algebra | Mathematical Notes for Machine Learning" />
  
  
  

<meta name="author" content="Qiushi Yan" />


<meta name="date" content="2020-09-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="vector-spaces.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Math Notes for Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Linear Algebra</b></span></li>
<li class="chapter" data-level="1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html"><i class="fa fa-check"></i><b>1</b> Basic Matrix Algebra</a><ul>
<li class="chapter" data-level="1.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.1</b> Matrix Multiplication</a><ul>
<li class="chapter" data-level="1.1.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#geometric-transformations"><i class="fa fa-check"></i><b>1.1.1</b> Geometric Transformations</a></li>
<li class="chapter" data-level="1.1.2" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#selector-matrix"><i class="fa fa-check"></i><b>1.1.2</b> Selector matrix</a></li>
<li class="chapter" data-level="1.1.3" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#discrete-convolution"><i class="fa fa-check"></i><b>1.1.3</b> Discrete Convolution</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#elemetary-matrix-and-row-operations"><i class="fa fa-check"></i><b>1.2</b> Elemetary matrix and row operations</a></li>
<li class="chapter" data-level="1.3" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#lu-factorization"><i class="fa fa-check"></i><b>1.3</b> LU Factorization</a></li>
<li class="chapter" data-level="1.4" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#determinants"><i class="fa fa-check"></i><b>1.4</b> Determinants</a><ul>
<li class="chapter" data-level="1.4.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#cofactor-expansion"><i class="fa fa-check"></i><b>1.4.1</b> Cofactor Expansion</a></li>
<li class="chapter" data-level="1.4.2" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#geometric-interpretation-of-determinant"><i class="fa fa-check"></i><b>1.4.2</b> Geometric Interpretation of Determinant</a></li>
<li class="chapter" data-level="1.4.3" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#properties-of-determinant"><i class="fa fa-check"></i><b>1.4.3</b> Properties of Determinant</a></li>
<li class="chapter" data-level="1.4.4" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#cramers-rule"><i class="fa fa-check"></i><b>1.4.4</b> Cramer’s Rule</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#trace"><i class="fa fa-check"></i><b>1.5</b> Trace</a></li>
<li class="chapter" data-level="1.6" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#matrix-inversion"><i class="fa fa-check"></i><b>1.6</b> Matrix Inversion</a><ul>
<li class="chapter" data-level="1.6.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#the-matrix-inversion-lemma"><i class="fa fa-check"></i><b>1.6.1</b> The Matrix Inversion Lemma</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#matrix-multiplication-as-linear-transformation"><i class="fa fa-check"></i><b>1.7</b> Matrix Multiplication as Linear Transformation</a></li>
<li class="chapter" data-level="1.8" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#complexity-of-matrix-computation"><i class="fa fa-check"></i><b>1.8</b> Complexity of Matrix Computation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="vector-spaces.html"><a href="vector-spaces.html"><i class="fa fa-check"></i><b>2</b> Vector Spaces</a><ul>
<li class="chapter" data-level="2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#vector-space"><i class="fa fa-check"></i><b>2.1</b> Vector Space</a><ul>
<li class="chapter" data-level="2.1.1" data-path="vector-spaces.html"><a href="vector-spaces.html#euclidean-space"><i class="fa fa-check"></i><b>2.1.1</b> Euclidean Space</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="vector-spaces.html"><a href="vector-spaces.html#metric-spaces-normed-spaces-inner-product-spaces"><i class="fa fa-check"></i><b>2.2</b> Metric Spaces, Normed Spaces, Inner Product Spaces</a><ul>
<li class="chapter" data-level="2.2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#metric-and-norm"><i class="fa fa-check"></i><b>2.2.1</b> Metric and Norm</a></li>
<li class="chapter" data-level="2.2.2" data-path="vector-spaces.html"><a href="vector-spaces.html#inner-produc-outer-product-cross-product"><i class="fa fa-check"></i><b>2.2.2</b> Inner Produc, Outer Product, Cross Product</a></li>
<li class="chapter" data-level="2.2.3" data-path="vector-spaces.html"><a href="vector-spaces.html#restricted-definition-of-inner-products-in-rn"><i class="fa fa-check"></i><b>2.2.3</b> Restricted Definition of Inner Products in <span class="math inline">\(R^n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="vector-spaces.html"><a href="vector-spaces.html#subspaces"><i class="fa fa-check"></i><b>2.3</b> Subspaces</a></li>
<li class="chapter" data-level="2.4" data-path="vector-spaces.html"><a href="vector-spaces.html#fundamental-theorem"><i class="fa fa-check"></i><b>2.4</b> Fundamental Theorem of Linear Algebra</a></li>
<li class="chapter" data-level="2.5" data-path="vector-spaces.html"><a href="vector-spaces.html#rank"><i class="fa fa-check"></i><b>2.5</b> Rank</a><ul>
<li class="chapter" data-level="2.5.1" data-path="vector-spaces.html"><a href="vector-spaces.html#effect-of-operations-on-matrix-rank"><i class="fa fa-check"></i><b>2.5.1</b> Effect of Operations on Matrix Rank</a></li>
<li class="chapter" data-level="2.5.2" data-path="vector-spaces.html"><a href="vector-spaces.html#gram-matrix"><i class="fa fa-check"></i><b>2.5.2</b> Gram Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="vector-spaces.html"><a href="vector-spaces.html#bases-and-coordinate-systems"><i class="fa fa-check"></i><b>2.6</b> Bases and Coordinate Systems</a><ul>
<li class="chapter" data-level="2.6.1" data-path="vector-spaces.html"><a href="vector-spaces.html#change-of-basis"><i class="fa fa-check"></i><b>2.6.1</b> Change of Basis</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="vector-spaces.html"><a href="vector-spaces.html#complexity-of-vector-computations"><i class="fa fa-check"></i><b>2.7</b> Complexity of Vector Computations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="orthogonality.html"><a href="orthogonality.html"><i class="fa fa-check"></i><b>3</b> Orthogonality</a><ul>
<li class="chapter" data-level="3.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition"><i class="fa fa-check"></i><b>3.1</b> Orthogonal Decomposition</a><ul>
<li class="chapter" data-level="3.1.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-complements"><i class="fa fa-check"></i><b>3.1.1</b> Orthogonal Complements</a></li>
<li class="chapter" data-level="3.1.2" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-sets-and-orthogonal-basis"><i class="fa fa-check"></i><b>3.1.2</b> Orthogonal Sets and Orthogonal Basis</a></li>
<li class="chapter" data-level="3.1.3" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition-1"><i class="fa fa-check"></i><b>3.1.3</b> Orthogonal Decomposition</a></li>
<li class="chapter" data-level="3.1.4" data-path="orthogonality.html"><a href="orthogonality.html#best-approximation"><i class="fa fa-check"></i><b>3.1.4</b> Best Approximation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="orthogonality.html"><a href="orthogonality.html#projection-and-idempotent-matrices"><i class="fa fa-check"></i><b>3.2</b> Projection and idempotent matrices</a></li>
<li class="chapter" data-level="3.3" data-path="orthogonality.html"><a href="orthogonality.html#gram-schmidt-process"><i class="fa fa-check"></i><b>3.3</b> Gram-Schmidt Process</a></li>
<li class="chapter" data-level="3.4" data-path="orthogonality.html"><a href="orthogonality.html#qr-factorizaiton"><i class="fa fa-check"></i><b>3.4</b> QR Factorizaiton</a></li>
<li class="chapter" data-level="3.5" data-path="orthogonality.html"><a href="orthogonality.html#orthonormal-sets-and-orthogonal-matrices"><i class="fa fa-check"></i><b>3.5</b> Orthonormal Sets and Orthogonal Matrices</a><ul>
<li class="chapter" data-level="3.5.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-matrices"><i class="fa fa-check"></i><b>3.5.1</b> Orthogonal Matrices</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="orthogonality.html"><a href="orthogonality.html#lesat-squares-problems"><i class="fa fa-check"></i><b>3.6</b> Lesat Squares Problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html"><i class="fa fa-check"></i><b>4</b> Eigenthings and Quadratic Forms</a><ul>
<li class="chapter" data-level="4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>4.1</b> Eigenvectors and Eigenvalues</a><ul>
<li class="chapter" data-level="4.1.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#additional-properties-of-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>4.1.1</b> Additional Properties of Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="4.1.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#left-eigenvectors-and-right-eigenvectors"><i class="fa fa-check"></i><b>4.1.2</b> Left Eigenvectors and Right Eigenvectors</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#diagnolization-and-similar-matrices"><i class="fa fa-check"></i><b>4.2</b> Diagnolization and Similar Matrices</a><ul>
<li class="chapter" data-level="4.2.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#similarity"><i class="fa fa-check"></i><b>4.2.1</b> Similarity</a></li>
<li class="chapter" data-level="4.2.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#jordan-matrix"><i class="fa fa-check"></i><b>4.2.2</b> Jordan Matrix</a></li>
<li class="chapter" data-level="4.2.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#simultaneous-diagonalization"><i class="fa fa-check"></i><b>4.2.3</b> Simultaneous Diagonalization</a></li>
<li class="chapter" data-level="4.2.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>4.2.4</b> Cayley-Hamilton Theorem</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#symmetric-matrices"><i class="fa fa-check"></i><b>4.3</b> Symmetric Matrices</a><ul>
<li class="chapter" data-level="4.3.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#spectral-decomposition"><i class="fa fa-check"></i><b>4.3.1</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="4.3.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#a-orthogonality"><i class="fa fa-check"></i><b>4.3.2</b> A-Orthogonality</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#quadratic-forms"><i class="fa fa-check"></i><b>4.4</b> Quadratic Forms</a><ul>
<li class="chapter" data-level="4.4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#change-of-variable"><i class="fa fa-check"></i><b>4.4.1</b> Change of Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#classification-of-quadratic-forms"><i class="fa fa-check"></i><b>4.4.2</b> Classification of Quadratic Forms</a></li>
<li class="chapter" data-level="4.4.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#gershgorin-discs-and-diagonal-dominance"><i class="fa fa-check"></i><b>4.4.3</b> Gershgorin Discs and Diagonal Dominance</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#cholesky-factorization"><i class="fa fa-check"></i><b>4.5</b> Cholesky Factorization</a></li>
<li class="chapter" data-level="4.6" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#rayleigh-quotients"><i class="fa fa-check"></i><b>4.6</b> Rayleigh Quotients</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html"><i class="fa fa-check"></i><b>5</b> Singular Value Decomposition</a><ul>
<li class="chapter" data-level="5.1" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#singular-values"><i class="fa fa-check"></i><b>5.1</b> Singular Values</a></li>
<li class="chapter" data-level="5.2" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#svd-theorem"><i class="fa fa-check"></i><b>5.2</b> SVD</a></li>
<li class="chapter" data-level="5.3" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#matrix-norms"><i class="fa fa-check"></i><b>5.3</b> Matrix Norms</a><ul>
<li class="chapter" data-level="5.3.1" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#induced-norms"><i class="fa fa-check"></i><b>5.3.1</b> Induced Norms</a></li>
<li class="chapter" data-level="5.3.2" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#entry-wise-norm"><i class="fa fa-check"></i><b>5.3.2</b> Entry-wise Norm</a></li>
<li class="chapter" data-level="5.3.3" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#other-matrix-norms"><i class="fa fa-check"></i><b>5.3.3</b> Other Matrix Norms</a></li>
<li class="chapter" data-level="5.3.4" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#unitary-invariant-norms"><i class="fa fa-check"></i><b>5.3.4</b> Unitary Invariant Norms</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#low-rank-approximation"><i class="fa fa-check"></i><b>5.4</b> Low Rank Approximation</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-system.html"><a href="linear-system.html"><i class="fa fa-check"></i><b>6</b> Solutions of Linear System Ax = b</a><ul>
<li class="chapter" data-level="6.1" data-path="linear-system.html"><a href="linear-system.html#generalized-inverse"><i class="fa fa-check"></i><b>6.1</b> Generalized Inverse</a></li>
<li class="chapter" data-level="6.2" data-path="linear-system.html"><a href="linear-system.html#ill-conditioned-matrices"><i class="fa fa-check"></i><b>6.2</b> Ill-Conditioned Matrices</a><ul>
<li class="chapter" data-level="6.2.1" data-path="linear-system.html"><a href="linear-system.html#condition-number"><i class="fa fa-check"></i><b>6.2.1</b> Condition Number</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Multivariate Calculus</b></span></li>
<li class="chapter" data-level="7" data-path="partial-derivatives.html"><a href="partial-derivatives.html"><i class="fa fa-check"></i><b>7</b> Partial Derivatives</a><ul>
<li class="chapter" data-level="7.1" data-path="partial-derivatives.html"><a href="partial-derivatives.html#limit-and-continuity"><i class="fa fa-check"></i><b>7.1</b> Limit and Continuity</a></li>
<li class="chapter" data-level="7.2" data-path="partial-derivatives.html"><a href="partial-derivatives.html#partial-derivative"><i class="fa fa-check"></i><b>7.2</b> Partial Derivative</a><ul>
<li class="chapter" data-level="7.2.1" data-path="partial-derivatives.html"><a href="partial-derivatives.html#gradient-and-directional-derivative"><i class="fa fa-check"></i><b>7.2.1</b> Gradient and Directional Derivative</a></li>
<li class="chapter" data-level="7.2.2" data-path="partial-derivatives.html"><a href="partial-derivatives.html#linearization-of-two-variable-functions"><i class="fa fa-check"></i><b>7.2.2</b> Linearization of Two-variable Functions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="partial-derivatives.html"><a href="partial-derivatives.html#differentials"><i class="fa fa-check"></i><b>7.3</b> Differentials</a><ul>
<li class="chapter" data-level="7.3.1" data-path="partial-derivatives.html"><a href="partial-derivatives.html#continuity-partial-derivatives-and-differentiability"><i class="fa fa-check"></i><b>7.3.1</b> Continuity, Partial Derivatives and Differentiability</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="partial-derivatives.html"><a href="partial-derivatives.html#divergence-curl-and-laplacian"><i class="fa fa-check"></i><b>7.4</b> Divergence, Curl, and Laplacian</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="matrix-calculus.html"><a href="matrix-calculus.html"><i class="fa fa-check"></i><b>8</b> Matrix Calculus</a><ul>
<li class="chapter" data-level="8.1" data-path="matrix-calculus.html"><a href="matrix-calculus.html#the-chain-rule"><i class="fa fa-check"></i><b>8.1</b> The Chain Rule</a></li>
<li class="chapter" data-level="8.2" data-path="matrix-calculus.html"><a href="matrix-calculus.html#useful-identities-in-matirx-calculus"><i class="fa fa-check"></i><b>8.2</b> Useful Identities in Matirx Calculus</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="taylor-series.html"><a href="taylor-series.html"><i class="fa fa-check"></i><b>9</b> Taylor Series</a><ul>
<li class="chapter" data-level="9.1" data-path="taylor-series.html"><a href="taylor-series.html#convergence-of-taylor-series"><i class="fa fa-check"></i><b>9.1</b> Convergence of Taylor Series</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="multiple-integral.html"><a href="multiple-integral.html"><i class="fa fa-check"></i><b>10</b> Multiple Integral</a></li>
<li class="part"><span><b>III Probability Theory</b></span></li>
<li class="chapter" data-level="11" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html"><i class="fa fa-check"></i><b>11</b> Basics of Probability Theory</a><ul>
<li class="chapter" data-level="11.1" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#probabilty-space"><i class="fa fa-check"></i><b>11.1</b> Probabilty Space</a></li>
<li class="chapter" data-level="11.2" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#counting"><i class="fa fa-check"></i><b>11.2</b> Counting</a></li>
<li class="chapter" data-level="11.3" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#conditional-probability"><i class="fa fa-check"></i><b>11.3</b> Conditional Probability</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html"><i class="fa fa-check"></i><b>12</b> Random variables and moments</a><ul>
<li class="chapter" data-level="12.1" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#properties-of-expectation-and-variance"><i class="fa fa-check"></i><b>12.1</b> Properties of Expectation and Variance</a><ul>
<li class="chapter" data-level="12.1.1" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#random-vectors"><i class="fa fa-check"></i><b>12.1.1</b> Random vectors</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#other-summaries"><i class="fa fa-check"></i><b>12.2</b> Other Summaries</a></li>
<li class="chapter" data-level="12.3" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#moment-generating-functions"><i class="fa fa-check"></i><b>12.3</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="univariate-distributions.html"><a href="univariate-distributions.html"><i class="fa fa-check"></i><b>13</b> Univariate Distributions</a><ul>
<li class="chapter" data-level="13.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#uniform-distribution"><i class="fa fa-check"></i><b>13.1</b> Uniform Distribution</a></li>
<li class="chapter" data-level="13.2" data-path="univariate-distributions.html"><a href="univariate-distributions.html#normal-distribution"><i class="fa fa-check"></i><b>13.2</b> Normal Distribution</a><ul>
<li class="chapter" data-level="13.2.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#log-normal-distribution"><i class="fa fa-check"></i><b>13.2.1</b> Log Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="univariate-distributions.html"><a href="univariate-distributions.html#binomial-distribution-and-beta-distribution"><i class="fa fa-check"></i><b>13.3</b> Binomial Distribution and Beta Distribution</a></li>
<li class="chapter" data-level="13.4" data-path="univariate-distributions.html"><a href="univariate-distributions.html#poisson-distribution"><i class="fa fa-check"></i><b>13.4</b> Poisson Distribution</a><ul>
<li class="chapter" data-level="13.4.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#poisson-process"><i class="fa fa-check"></i><b>13.4.1</b> Poisson Process</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="univariate-distributions.html"><a href="univariate-distributions.html#exponential-distribution-and-gamma-distribution"><i class="fa fa-check"></i><b>13.5</b> Exponential Distribution and Gamma Distribution</a><ul>
<li class="chapter" data-level="13.5.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#properties"><i class="fa fa-check"></i><b>13.5.1</b> Properties</a></li>
<li class="chapter" data-level="13.5.2" data-path="univariate-distributions.html"><a href="univariate-distributions.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>13.5.2</b> Inverse Gamma Distribution</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="univariate-distributions.html"><a href="univariate-distributions.html#beta-distribution"><i class="fa fa-check"></i><b>13.6</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>14</b> Multivariate Distributions</a><ul>
<li class="chapter" data-level="14.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>14.1</b> Multivariate Normal Distribution</a><ul>
<li class="chapter" data-level="14.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#chi-square-distribution"><i class="fa fa-check"></i><b>14.1.1</b> Chi-square Distribution</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#dirichlet-distributon"><i class="fa fa-check"></i><b>14.2</b> Dirichlet Distributon</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="markov-chain.html"><a href="markov-chain.html"><i class="fa fa-check"></i><b>15</b> Markov Chain</a></li>
<li class="part"><span><b>IV Learning Theory</b></span></li>
<li class="chapter" data-level="16" data-path="the-learning-problem.html"><a href="the-learning-problem.html"><i class="fa fa-check"></i><b>16</b> The Learning Problem</a></li>
<li class="part"><span><b>V Optimization</b></span></li>
<li class="chapter" data-level="17" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html"><i class="fa fa-check"></i><b>17</b> Basics of Optimization</a><ul>
<li class="chapter" data-level="17.1" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#univariate-optimization"><i class="fa fa-check"></i><b>17.1</b> Univariate Optimization</a></li>
<li class="chapter" data-level="17.2" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#multivariate-optimization"><i class="fa fa-check"></i><b>17.2</b> Multivariate Optimization</a></li>
<li class="chapter" data-level="17.3" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#convex-functions"><i class="fa fa-check"></i><b>17.3</b> Convex Functions</a></li>
<li class="chapter" data-level="17.4" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#method-of-lagrange-multiplier"><i class="fa fa-check"></i><b>17.4</b> Method of Lagrange Multiplier</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="gradient-descent.html"><a href="gradient-descent.html"><i class="fa fa-check"></i><b>18</b> Gradient Descent</a></li>
<li class="part"><span><b>VI Applications</b></span></li>
<li class="chapter" data-level="19" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>19</b> Linear Models</a><ul>
<li class="chapter" data-level="19.1" data-path="linear-models.html"><a href="linear-models.html#ordinary-least-squares"><i class="fa fa-check"></i><b>19.1</b> Ordinary Least Squares</a><ul>
<li class="chapter" data-level="19.1.1" data-path="linear-models.html"><a href="linear-models.html#least-square-estimation"><i class="fa fa-check"></i><b>19.1.1</b> Least Square Estimation</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="linear-models.html"><a href="linear-models.html#weighted-least-squares"><i class="fa fa-check"></i><b>19.2</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="19.3" data-path="linear-models.html"><a href="linear-models.html#partial-least-squares"><i class="fa fa-check"></i><b>19.3</b> Partial Least Squares</a></li>
<li class="chapter" data-level="19.4" data-path="linear-models.html"><a href="linear-models.html#regularized-regression"><i class="fa fa-check"></i><b>19.4</b> Regularized Regression</a><ul>
<li class="chapter" data-level="19.4.1" data-path="linear-models.html"><a href="linear-models.html#ridge-regression"><i class="fa fa-check"></i><b>19.4.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="19.4.2" data-path="linear-models.html"><a href="linear-models.html#lasso"><i class="fa fa-check"></i><b>19.4.2</b> Lasso</a></li>
<li class="chapter" data-level="19.4.3" data-path="linear-models.html"><a href="linear-models.html#elastic-net"><i class="fa fa-check"></i><b>19.4.3</b> Elastic Net</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>20</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="21" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>21</b> Text Mining</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li>
  <a href="https://github.com/rstudio/bookdown" target="blank">Written with bookdown</a>
</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Notes for Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="basic-matrix-algebra" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Basic Matrix Algebra</h1>
<div id="matrix-multiplication" class="section level2">
<h2><span class="header-section-number">1.1</span> Matrix Multiplication</h2>
<p>A common way of looking at matrix-vector multiplication <span class="math inline">\(A\bar{x}\)</span> is to think of as a linear combination of column vectors in <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
A\bar{x} &amp;= [\bar{a}_1 \;\; \cdots \;\; \bar{a}_n] 
\begin{bmatrix}
x_1 \\
\vdots \\
x_n
\end{bmatrix} \\
&amp;= x_1\bar{a}_1 + \cdots + x_n\bar{a}_n
\end{aligned}
\]</span></p>
<p>Likewise, for any <span class="math inline">\(\bar{x}^{T} = [x_1, \cdots, x_n]^T\)</span> and matrix <span class="math inline">\(A_{m \times n}\)</span>, <span class="math inline">\(x^{T}A\)</span> can be thought of as a linear combination of rows in <span class="math inline">\(A\)</span> to produce a new row vector:</p>
<p><span class="math display">\[
[x_1 \;\; \cdots \;\; x_n] 
\begin{bmatrix}
\bar{a}_1^T \\
\vdots  \\
\bar{a}_n^T
\end{bmatrix} 
= x_1\bar{a}_1^T + \dots + x_n\bar{a}_n^T
\]</span>
For matrix-matrix multiplication <span class="math inline">\(AB\)</span>, besides the dot product definition we can see it as <strong>column row expansion</strong>.</p>
<p><br></p>


<div class="theorem">
<p><span id="thm:cr-expansion" class="theorem"><strong>Theorem 1.1  (Column-row expansion of <span class="math inline">\(AB\)</span>)  </strong></span>
if <span class="math inline">\(A\)</span> is <span class="math inline">\(m \times n\)</span> and <span class="math inline">\(B\)</span> is <span class="math inline">\(n \times p\)</span>, then</p>
<span class="math display">\[
\begin{aligned}
AB &amp;= [\text{col}_1(A) \cdots \text{col}_n(A)] 
\begin{bmatrix}
\text{row}_1(B) \\
\vdots \\ 
\text{row}_n(B)
\end{bmatrix} \\ 
&amp;= \text{col}_1(A)\text{row}_1(B) + \cdots +  \text{col}_n(A)\text{row}_n(B)
\end{aligned}
\]</span>
</div>

<p>Note that each <span class="math inline">\(\text{col}_1(A)\text{row}_1(B)\)</span> is a rank 1 <span class="math inline">\(m \times p\)</span> matrix.</p>
<p>The following subsections follows Chapter 7 of VMLS <span class="citation">(Boyd and Vandenberghe <a href="references.html#ref-boyd2018introduction" role="doc-biblioref">2018</a>)</span>, introducing some special matrices and their effect in matrix multiplication.</p>
<div id="geometric-transformations" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Geometric Transformations</h3>
<p><strong>scaling</strong></p>
<p><strong>dilation</strong></p>
<p><strong>rotation</strong></p>
<p><span class="math display">\[
\begin{bmatrix}
\cos\theta &amp; -\sin \theta \\
\sin \theta &amp; \cos \theta
\end{bmatrix}
\begin{bmatrix}
\rho \cos\alpha \\ 
\rho \sin\alpha 
\end{bmatrix} =
\begin{bmatrix} 
\rho(\cos\theta\cos\alpha - \sin\theta\sin\alpha) \\
\rho(\sin\theta \cos\alpha + \cos \theta \sin\alpha)
\end{bmatrix}
= 
\begin{bmatrix}
\rho \cos(\theta + \alpha) \\
\rho \sin(\theta + \alpha)
\end{bmatrix}
\]</span></p>
<p><strong>reflection</strong></p>
</div>
<div id="selector-matrix" class="section level3">
<h3><span class="header-section-number">1.1.2</span> Selector matrix</h3>
<p>Let the standard basis in <span class="math inline">\(\mathcal{R}^n\)</span> be</p>
<p><span class="math display">\[
\begin{aligned}
\bar{e}_1^T  &amp;= [1, 0, \cdots, ..., 0] \\
\bar{e}_2^T  &amp;= [0, 1, \cdots, ..., 0] \\
&amp; \vdots  \\
\bar{e}_n^T  &amp;= [0, 0, \cdots, ..., 1] 
\end{aligned}
\]</span></p>
<p>Then an <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> formed by a subset of such vectors are called a selector matrix. In other words, an <span class="math inline">\(m \times n\)</span> selector matrix <span class="math inline">\(A\)</span> is one in which each row is a standard unit vector</p>
<p><span class="math display">\[
A = \begin{bmatrix}
\bar{e}_{k_1}^T \\ 
\vdots \\
\bar{e}_{k_m}^T
\end{bmatrix}
\]</span></p>
<p>where <span class="math inline">\(1 \le k_1, ..., k_m \le n\)</span>. When a selector matrix multiplies a vector <span class="math inline">\(\bar{x}\)</span>, it returned the <span class="math inline">\(k_i\)</span>th entry of <span class="math inline">\(\bar{x}\)</span> on the <span class="math inline">\(i\)</span>th entry of <span class="math inline">\(y = A\bar{x}\)</span>. For example, we can construct a <span class="math inline">\(4 \times n\)</span> selector matrix</p>
<p><span class="math display">\[
A = 
\begin{bmatrix}
\bar{e}_2^T \\
\bar{e}_1^T \\ 
\bar{e}_5^T \\ 
\bar{e}_8^T
\end{bmatrix}
\]</span>
When multiplying <span class="math inline">\(\bar{x}\)</span></p>
<p><span class="math display">\[
\begin{bmatrix}
\bar{e}_2^T \\
\bar{e}_1^T \\ 
\bar{e}_5^T \\
\bar{e}_8^T
\end{bmatrix}
\begin{bmatrix}
x_1 \\
\vdots \\ 
x_n 
\end{bmatrix}
= 
\begin{bmatrix}
x_2 \\
x_1 \\
x_5 \\
x_8
\end{bmatrix}
\]</span>
The identity matrix, and the reverser matrix</p>
<p><span class="math display">\[
A = 
\begin{bmatrix}
\bar{e}_n^T \\ 
\vdots \\
\bar{e}_1^T
\end{bmatrix}
= 
\begin{bmatrix}
0 &amp; 0 &amp; \cdots &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; \cdots &amp; 1 &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
1 &amp; 0 &amp; \cdots &amp; 0 &amp; 0
\end{bmatrix}
\]</span>
are one of the selector matrices family.</p>
<p>Selector matrices have important applications in down-sampling, image cropping, and permutation. (P.144, VSLM)</p>
</div>
<div id="discrete-convolution" class="section level3">
<h3><span class="header-section-number">1.1.3</span> Discrete Convolution</h3>
<p>The (discrete) <em>convolution</em> of an <span class="math inline">\(n\)</span>-vector <span class="math inline">\(a\)</span> <span class="math inline">\(a\)</span> and <span class="math inline">\(m\)</span>-vector <span class="math inline">\(b\)</span> is a <span class="math inline">\((n + m - 1)\)</span>-vector, denoted <span class="math inline">\(a * b\)</span>, with entries</p>
<p><span class="math display">\[
c_k = \sum_{i + j = k + 1}a_ib_j, \quad k = 1, ..., n + m - 1
\]</span></p>
</div>
</div>
<div id="elemetary-matrix-and-row-operations" class="section level2">
<h2><span class="header-section-number">1.2</span> Elemetary matrix and row operations</h2>
<p>An <em>elementary matrix</em> is one that is obtained by performing a single elementary row
operation on an identity matrix <span class="math inline">\(I\)</span>. Each elementary matrix <span class="math inline">\(E\)</span> is invertible. The inverse of <span class="math inline">\(E\)</span> is the elementary matrix of the same type that transforms <span class="math inline">\(E\)</span> back into <span class="math inline">\(I\)</span>.</p>
<p>Left multiplication by a elementary matrix has a nice illustration. There are 3 primary types of elementary matrices (example for <span class="math inline">\(3 \times 3\)</span>):</p>
<p><span class="math display">\[
E_1 = 
\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
-4 &amp; 0 &amp; 1
\end{bmatrix} 
,\;
E_2 = 
\begin{bmatrix}
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} 
,\;
E_3 = 
\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 5
\end{bmatrix} 
\]</span>
<span class="math inline">\(E_1, E_2, E_3\)</span> represents 3 types of elementary row operations applicable to a <span class="math inline">\(3 \times 3\)</span> matrix,</p>
<ol style="list-style-type: decimal">
<li><p><em>Row addition</em>, a scalar multiple of the <span class="math inline">\(i\)</span>th row is added to the <span class="math inline">\(j\)</span>th row</p></li>
<li><p><em>Row interchange</em>, the <span class="math inline">\(i\)</span>th row and the <span class="math inline">\(j\)</span>th row of the matrix are interchanged</p></li>
<li><p><em>Row scaling</em>, the <span class="math inline">\(i\)</span>th row is multiplied by a scalar.</p></li>
</ol>
<p><span class="math display">\[
\begin{aligned}
E_1A &amp;= 
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
- 4a_{11} + a_{31} &amp; -4a_{12} + a_{32} &amp; -4a_{13} + a_{33}
\end{bmatrix}  \\ \\  
E_2A &amp;= 
\begin{bmatrix}
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{31} &amp; a_{32} &amp; a_{33}
\end{bmatrix} \\ \\
E_3A &amp;= 
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
5a_{31} &amp; 5a_{32} &amp; 5a_{33} \\
\end{bmatrix}
\end{aligned}
\]</span>
Thus, any row operation on <span class="math inline">\(A\)</span> is equivalent to left multiply a corresponding elementary matrix <span class="math inline">\(E\)</span>.</p>
<p>Since row operation are invertible, elementary matrices are invertible. This gives a general way of finding the inverse matrix of <span class="math inline">\(A\)</span>.</p>

<div class="theorem">
<span id="thm:find-inverse" class="theorem"><strong>Theorem 1.2  (an algorithm for finding inverse matrices)  </strong></span>Row reduce the augmented matrix <span class="math inline">\([A \;\; I]\)</span>, when <span class="math inline">\(A\)</span> becomes <span class="math inline">\(I\)</span>, <span class="math inline">\(I\)</span> becomes <span class="math inline">\(A^{-1}\)</span>. Otherwise <span class="math inline">\(A^{-1}\)</span> is not invertible.
</div>

</div>
<div id="lu-factorization" class="section level2">
<h2><span class="header-section-number">1.3</span> LU Factorization</h2>
<p><a href="https://fml-fam.github.io/blog/2020/07/03/matrix-factorizations-for-data-analysis/" class="uri">https://fml-fam.github.io/blog/2020/07/03/matrix-factorizations-for-data-analysis/</a></p>
<p>A <em>factorization</em> of matrix <span class="math inline">\(A\)</span> is an equation that expresses <span class="math inline">\(A\)</span> as a product of two or more matrices.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-1" class="definition"><strong>Definition 1.1  (LU factorization)  </strong></span>Suppose <span class="math inline">\(A\)</span> can be reduced to an echelon form <span class="math inline">\(U\)</span> <strong>using only row addition and row scaling</strong>, there exist a set of unit lower trangular matrices <span class="math inline">\(E_1, \dots, E_p\)</span> such that</p>
<p><span class="math display">\[
E_p \cdots E_1A = U  
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
A = (E_p \cdots E_1)^{-1}U = LU
\]</span></p>
where <span class="math inline">\(u\)</span> is a the upper triangular row echelon form (or upper trapezoidal), and <span class="math inline">\(L\)</span> an lower triangular matrix
<span class="math display">\[
L = (E_p \cdots E_1)^{-1}  
\]</span>
</div>

<p>LU decomposition expresses a matrix (don’t have to be square or invertible) as the product of a square lower triangular matrix <span class="math inline">\(L\)</span> and a rectangular upper triangular matrix <span class="math inline">\(U\)</span>.</p>
<p>From the definition, we know that row operations on <span class="math inline">\(A\)</span> must only be confined to <em>row addition</em> and <em>row scaling</em>, but not <em>row interchange</em>. Otherwise <span class="math inline">\((E_p \cdots E_1)^{-1}\)</span> cannot be lower triangular. The most common needs for row exchanges in row reduction is when <span class="math inline">\(a_{11}\)</span> is 0. For example, the following matrix does not have a LU factorization because it first requires exchanging two rows to produce row echelon form<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p><span class="math display">\[
\begin{bmatrix}
0 &amp; 1 \\
1 &amp; 0
\end{bmatrix}
\]</span></p>
<p>To give a former definition of LU factorization, one can show that if all <strong>principal submatrices are non-singular</strong><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>, then the factorization exists. Note that this condition is <strong>not necessary</strong>, the following matrix has a zero in (2, 2) position violating the rule, but it can still be expressed in terms of LU</p>
<p><span class="math display">\[
\begin{bmatrix}
2 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 1
\end{bmatrix}
= 
\begin{bmatrix}
2 &amp; 0 &amp; 0 \\
1 &amp; -\frac{1}{2} &amp; 0 \\
1 &amp;  \frac{1}{2} &amp; 1
\end{bmatrix}
\begin{bmatrix}
1 &amp; \frac{1}{2} &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}
\]</span></p>
<p>If an LU factorization exists, we shall notice that the LU form is “asymmetric” on the diagonal, in the sense that L has all <span class="math inline">\(1\)</span>s and <span class="math inline">\(U\)</span> has the pivots. We can factor out another diagonal matrix <span class="math inline">\(D\)</span> to also have all <span class="math inline">\(1\)</span>s on <span class="math inline">\(U\)</span>’s diagonal</p>
<p><span class="math display">\[
U = \begin{bmatrix}
d_{1} \\
&amp; d_{2} \\
&amp; &amp; \ddots \\ 
&amp; &amp; &amp; d_n
\end{bmatrix}
\begin{bmatrix}
1 &amp; u_{12} / d_1 &amp; \cdots &amp; u_{1n}/d_1 \\
&amp; 1 &amp; \cdots &amp; u_{2n}/d_2 \\
&amp; &amp; \ddots &amp; \vdots \\
&amp; &amp; &amp; 1
\end{bmatrix}
\]</span></p>
<p>Then the <span class="math inline">\(LU\)</span> factorization becomes <span class="math inline">\(A = LDU\)</span>, where <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span> have ones on its diagonal and <span class="math inline">\(D\)</span> is the diagonal matrix of pivots.</p>
<hr>
<p>In cases where row exchanges are needed to produce the echelon form, we can express the elimination process in terms of a permutation matrix <span class="math inline">\(P\)</span> and the other set of row addition operations, defined by <span class="math inline">\(L_1, ..., L_{k}\)</span></p>
<p><span class="math display">\[
PL_{k}\cdots L_1A = U
\]</span></p>
<p>Note that permutation matrix <span class="math inline">\(P\)</span> satisfies <span class="math inline">\(PP^T = I\)</span>. Multiplying both sides with <span class="math inline">\(P^T\)</span> and the inverses of <span class="math inline">\(L_1, ..., L_{k}\)</span>, we obtain</p>
<p><span class="math display">\[
A = \underbrace{ L_1^{-1} \cdots L_k^{-1}}_{L}P^TU
\]</span>
One can try to polish this form by obtain a decomposition in which the permutation matrix occurs before the lower triangular matrix <span class="math inline">\(L\)</span> (<span class="math inline">\(L\)</span> and <span class="math inline">\(P\)</span> are not the same after reordering)</p>
<p><span class="math display">\[
A = P^TLU
\]</span></p>
<p>It’s also common to write this decomposition as <span class="math inline">\(PA = LU\)</span>.</p>
</div>
<div id="determinants" class="section level2">
<h2><span class="header-section-number">1.4</span> Determinants</h2>
<p>In the <span class="math inline">\(2 \times 2\)</span> case, the determinant of <span class="math inline">\(A\)</span> is simply</p>
<p><span class="math display">\[
\begin{vmatrix}
a &amp; b \\
c &amp; d \\
\end{vmatrix}
= ad - bc
\]</span></p>
<p>When <span class="math inline">\(A\)</span> is <span class="math inline">\(3 \times 3\)</span>, we still get a not-too-messy explicit formula to compute its determinant</p>
<p><span class="math display">\[
\begin{vmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33}
\end{vmatrix}
= a_{11}a_{22} a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21} a_{32}  - a_{13}a_{22}a_{31} - a_{12}a_{21} a_{33} - a_{11}a_{23}a_{32} 
\]</span></p>
<p>This is the sum of products on the foward diagonal minus the sum of products on the backward diagonal.</p>
<div id="cofactor-expansion" class="section level3">
<h3><span class="header-section-number">1.4.1</span> Cofactor Expansion</h3>
<p>Cofactor expansion is a more practical way to compute determinant of bigger matrices. The <span class="math inline">\((i, j)\text{-cofactor}\)</span> of <span class="math inline">\(A\)</span> is a number <span class="math inline">\(C_{ij}\)</span> in <span class="math inline">\(\mathbb{R}\)</span> given by</p>
<p><span class="math display">\[
C_{ij} = (-1)^{i + j} \det A_{ij}
\]</span>
where <span class="math inline">\(A_{ij}\)</span> denotes the submatrix formed by deleting the <span class="math inline">\(i\)</span>h row and <span class="math inline">\(j\)</span>th column of <span class="math inline">\(A\)</span>.</p>

<div class="theorem">
<p><span id="thm:cofactor-expansion" class="theorem"><strong>Theorem 1.3  (cofactor expansion)  </strong></span>The determinant of an <span class="math inline">\(n \times n\)</span> matrix is given by a <strong>cofactor expasion</strong> across any row or column. For example, the expansion across the <span class="math inline">\(i\)</span>th row is:</p>
<p><span class="math display">\[
\det A = a_{i1}C_{i1} + a_{i2}C_{i2} + \cdots + a_{in}C_{in} 
\]</span></p>
<p>and cross the <span class="math inline">\(j\)</span>th column is</p>
<span class="math display">\[
\det A = a_{1j}C_{1j} + a_{2j}C_{2j} + \cdots + a_{nj}C_{nj} 
\]</span>
</div>

</div>
<div id="geometric-interpretation-of-determinant" class="section level3">
<h3><span class="header-section-number">1.4.2</span> Geometric Interpretation of Determinant</h3>
<p>Given matrix <span class="math inline">\(A_{n \times n}\)</span></p>
<p><span class="math display">\[
\begin{bmatrix}
a_1^{T} \\
a_2^{T} \\
\vdots \\
a_n^{T}
\end{bmatrix}
\]</span>
where <span class="math inline">\(a_1, ..., a_n\)</span> are row vectors of A. Then <span class="math inline">\(|\det A|\)</span> is the volume of parallelotope constrained by <span class="math inline">\(a_1, ..., a_n\)</span>. When <span class="math inline">\(A\)</span> is <span class="math inline">\(2\times2\)</span>, <span class="math inline">\(|\det A|\)</span> is simply the area of the parallelogram defined by two side <span class="math inline">\(a_1, a_2\)</span></p>
</div>
<div id="properties-of-determinant" class="section level3">
<h3><span class="header-section-number">1.4.3</span> Properties of Determinant</h3>
<p>A list of arithmetic properties of determinants, A is an <span class="math inline">\(n\times n\)</span> matrix:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\det(A^T) = \det(A)\)</span></li>
<li><span class="math inline">\(\det(kA) = k^n \det(A)\)</span></li>
<li><span class="math inline">\(\det(AB) = \det(A)\det(B)\)</span> (although <span class="math inline">\(AB \not = BA\)</span> in general), it follows that <span class="math inline">\(\det(A^n) = \det(A)^n\)</span><br />
</li>
<li><span class="math inline">\(\det(A^{-1}) = 1 / \det(A)\)</span>, if <span class="math inline">\(A\)</span> is invertible</li>
<li>determinant is equal to the product of eigenvalues (counting multiplicity) <span class="math inline">\(\det(A) = \prod_{i=1}^n{\lambda_i}\)</span><br />
</li>
<li>If the <span class="math inline">\(i\)</span>-th row (column) in A is a sum of the <span class="math inline">\(i\)</span>-th row (column) of a matrix <span class="math inline">\(B\)</span> and the <span class="math inline">\(i\)</span>-th row (column) of a matrix <span class="math inline">\(C\)</span> and all other rows in <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> are equal to the corresponding rows in <span class="math inline">\(A\)</span> (that is, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> differ in one row only), then <span class="math inline">\(\det(A)=\det(B)+\det(C)\)</span>. This can be proven by cofactor expansion across the <span class="math inline">\(i\)</span>th row. The same applies to columns.</li>
</ol>
<p>Row operations on <span class="math inline">\(A\)</span> has the following effect on <span class="math inline">\(\det A\)</span></p>
<ul>
<li><p>if we multiply a single row in <span class="math inline">\(A\)</span> by a scalar <span class="math inline">\(k \in \mathbb{R}\)</span>, then the determinant of the new matrix is <span class="math inline">\(k\det A\)</span></p></li>
<li><p>if we exchange two rows <span class="math inline">\(a_i^T\)</span> and <span class="math inline">\(a_j^T\)</span> of <span class="math inline">\(A\)</span>, determinant becomes <span class="math inline">\(-\det A\)</span></p></li>
<li><p>Add a multiple of one row to another row has <strong>no</strong> effect on determinant</p></li>
</ul>
<p>Note that all row operations don’t change whether or not a determinant is 0, only change it by a non-zero factor or change its sign.</p>
<p>The first two effects can be easily understood in connection with geometric meaning of determinant. What third property means is illustrated by the following example. Represent <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> with row vectors</p>
<p><span class="math display">\[
B = 
\begin{bmatrix}
a_1^T \\ 
\vdots \\
a_i^T \\
\vdots \\
a_j^T \\ 
\vdots \\
a_n^T
\end{bmatrix}
,
C  = 
\begin{bmatrix}
a_1^T \\ 
\vdots \\
 ka_j^T \\
\vdots \\
a_j^T \\ 
\vdots \\
a_n^T
\end{bmatrix}
\]</span>
And define <span class="math inline">\(A\)</span> as</p>
<p><span class="math display">\[
A = 
\begin{bmatrix}
a_1^T \\ 
\vdots \\
a_i^T + ka_j^T \\
\vdots \\
a_j^T \\ 
\vdots \\
a_n^T
\end{bmatrix}
\]</span>
It is obvious that here <span class="math inline">\(A\)</span> is on purpose the result of <span class="math inline">\(B\)</span> after performing row addition (add a multiple of the <span class="math inline">\(j\)</span>th row to the <span class="math inline">\(i\)</span>th row). And by property 6 <span class="math inline">\(\det(A) = \det(B) + \det(C)\)</span>. Also in this case, <span class="math inline">\(C\)</span> has determinant <span class="math inline">\(0\)</span>, and this property indeed proves that row addition does not change determinant. In more general cases, <span class="math inline">\(|C|\)</span> definitely does not have to be zero.</p>
</div>
<div id="cramers-rule" class="section level3">
<h3><span class="header-section-number">1.4.4</span> Cramer’s Rule</h3>
<p>Given an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> and <span class="math inline">\(\bar{b}\)</span> in <span class="math inline">\(\mathbb{\mathbb{R^n}}\)</span>, denote <span class="math inline">\(A_i(\bar{b})\)</span> as the matrix derived by <span class="math inline">\(A\)</span> by <strong>replacing</strong> column <span class="math inline">\(i\)</span> by vector <span class="math inline">\(\bar{b}\)</span>:</p>
<p><span class="math display">\[
A_i(\bar{b}) = [\bar{a}_1 \cdots \underbrace{\bar{b}}_{\text{column} \,i} \cdots \bar{a}_n]
\]</span></p>

<div class="theorem">
<p><span id="thm:cramer" class="theorem"><strong>Theorem 1.4  (Cramer’s rule)  </strong></span>Let <span class="math inline">\(A\)</span> be an invertible <span class="math inline">\(n \times n\)</span> matrix. For any <span class="math inline">\(\bar{b}\)</span> in <span class="math inline">\(\mathbb{R^n}\)</span>, the unique solution <span class="math inline">\(\bar{x}\)</span> of <span class="math inline">\(A\bar{x} = \bar{b}\)</span> has entries given by:</p>
<span class="math display">\[
x_i = \frac{\det A_i(\bar{b})}{\det A}
\]</span>
</div>

</div>
</div>
<div id="trace" class="section level2">
<h2><span class="header-section-number">1.5</span> Trace</h2>
<p>The <em>trace</em> of square matrix <span class="math inline">\(A\)</span> is the sum of its diagonal entries <span class="math inline">\(\sum_{i = 1}^{n}A_{ii}\)</span>.</p>
<p>The trace has the following properties:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\text{tr}(A + B) = \text{tr}(A) + \text{tr}(B)\)</span><br />
</li>
<li><span class="math inline">\(\text{tr}(kA) = k\,\text{tr}(A)\)</span>, <span class="math inline">\(k\)</span> is a scalar<br />
</li>
<li><span class="math inline">\(\text{tr}(A^T) = \text{tr}(A)\)</span><br />
</li>
<li>For <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> such that <span class="math inline">\(AB\)</span> is square, <span class="math inline">\(\text{tr}(AB) = \text{tr}(BA)\)</span><br />
</li>
<li>Trace of product of multiple matrices is invariant to <em>cyclic permutations</em>, <span class="math inline">\(\text{tr}(ABC) = \text{tr}(BCA) = \text{tr}(CAB)\)</span>. Note that the reordering cannot be done arbitrarily, fro example <span class="math inline">\(\text{tr}(ABC) \not= \text{tr}(ACG)\)</span> in general.</li>
<li>Trace is equal to the sum of its eigenvalues (repeated according to multiplicity) <span class="math inline">\(\text{tr}(A) = \sum_{i = 1}^n{\lambda_i}\)</span><br />
</li>
<li><span class="math inline">\(\text{tr}(\bar{a}\bar{a}^T) = \bar{a}^T\bar{a}\)</span></li>
</ol>
</div>
<div id="matrix-inversion" class="section level2">
<h2><span class="header-section-number">1.6</span> Matrix Inversion</h2>

<div class="rmdnote">
<p>Note that the inverse of a matrix is only defined for square matrces, so is determinants in Section <a href="basic-matrix-algebra.html#determinants">1.4</a>.</p>
In practice <span class="math inline">\(A^{-1}\)</span> is seldom computed, because computing both <span class="math inline">\(A^{-1}\)</span> and <span class="math inline">\(A^{-1}\bar{b}\)</span> to solve linear equations takes about 3 times as many arithmetic operations as solving <span class="math inline">\(A\bar{x} = \bar{b}\)</span> by row reduction.
</div>

Assume that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are both non-singular

<div class="theorem">
<p><span id="thm:unnamed-chunk-3" class="theorem"><strong>Theorem 1.5  </strong></span>If A and B are both invertible matrces, we have</p>
<p><span class="math display">\[
(A^{-1})^{-1} = A  
\]</span></p>
<p><span class="math display">\[
(AB)^{-1} = B^{-1}A^{-1}
\]</span></p>
<span class="math display">\[
(A^T)^{-1} = (A^{-1})^T
\]</span>
</div>

<p>In Section <a href="basic-matrix-algebra.html#thm:find-inverse">1.2</a>, we derive an algorithm of finding inverse matrices by row reductions on the augmented matrix <span class="math inline">\([A \; | \; I]\)</span>. However, Cramer’s rule <a href="basic-matrix-algebra.html#thm:cramer">1.4</a> leads to a general formula of calculating <span class="math inline">\(A^{-1}\)</span>, if it exists.</p>
<p>The <span class="math inline">\(j\)</span>th column of <span class="math inline">\(A^{-1}\)</span> is a vector <span class="math inline">\(\bar{x}\)</span> that satisfies:</p>
<p><span class="math display">\[
A\bar{x} = \bar{e}_j
\]</span>
By Cramer’s rule
<span class="math display">\[
\{(i,j) \text{ entry of } A^{-1}\} = x_i = \frac{\det A_i{(\bar{e}_j)}}{\det A}
\]</span></p>
<p>A cofactor expansion <a href="basic-matrix-algebra.html#thm:cofactor-expansion">1.3</a> down column <span class="math inline">\(i\)</span> of <span class="math inline">\(A_i{(\bar{e}_j)}\)</span> shows that:</p>
<p><span class="math display">\[
\det A_i{(\bar{e}_j)} = (-1)^{i + j}\det A_{ji} = C_{ji}
\]</span>
where <span class="math inline">\(C_{ji}\)</span> is a cofactor of <span class="math inline">\(A\)</span>. Note that the (<span class="math inline">\(i\)</span>, <span class="math inline">\(j\)</span>)th entry of <span class="math inline">\(A^{-1}\)</span> is the cofactor <span class="math inline">\(C_{ji}\)</span> divided by <span class="math inline">\(\det A\)</span> (the subscript is reversed). Thus</p>
<p><span class="math display" id="eq:inverse-adjugate">\[\begin{equation}
\tag{1.1}
A^{-1} = \frac{1}{\det A} 
\begin{bmatrix}
C_{11} &amp; C_{21} &amp; \cdots &amp; C_{n1} \\ 
C_{12} &amp; C_{22} &amp; \cdots &amp; C_{n2} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
C_{1n} &amp; C_{2n} &amp; \cdots &amp; C_{nn} \\
\end{bmatrix}
\end{equation}\]</span></p>
<p>The right side of Eq <a href="basic-matrix-algebra.html#eq:inverse-adjugate">(1.1)</a> is called the <em>adjugate</em> of <span class="math inline">\(A\)</span>, often denoted by <span class="math inline">\(\text{adj}\, A\)</span>.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-4" class="theorem"><strong>Theorem 1.6  (An Inverse Formula)  </strong></span>Let <span class="math inline">\(A\)</span> be an invertible <span class="math inline">\(n \times n\)</span> matrix. Then</p>
<span class="math display">\[
A^{-1} = \frac{1}{\det A}\text{adj}\, A
\]</span>
</div>

<hr>
<p>Interestingly, theorem <a href="eigenthings-and-quadratic-forms.html#thm:cayley-hamilton">4.2</a> in later chapters shows that for invertible matrix <span class="math inline">\(A\)</span>, its inverse <span class="math inline">\(A^{-1}\)</span> can be represented as a polynomial of <span class="math inline">\(A\)</span>.</p>
<div id="the-matrix-inversion-lemma" class="section level3">
<h3><span class="header-section-number">1.6.1</span> The Matrix Inversion Lemma</h3>

<div class="lemma">
<p><span id="lem:matrix-inversion-lemma" class="lemma"><strong>Lemma 1.1  (The Matrix Inversion Lemma)  </strong></span>Let <span class="math inline">\(A\)</span> be an invertible <span class="math inline">\(n \times n\)</span> square matrix, and <span class="math inline">\(\bar{v}\)</span> and <span class="math inline">\(\bar{u}\)</span> be d-dimensional vectors. Then, <span class="math inline">\(A + \bar{u}\bar{v}^T\)</span> is invertible if and only if <span class="math inline">\(\bar{v}^TA\bar{u} \not = -1\)</span>. In such a case, the inverse formula is given by</p>
<span class="math display">\[
(A + \bar{u}\bar{v}^T) = A^{-1} - \frac{A^{-1}\bar{u}\bar{v}^TA^{-1}}{1 + \bar{v}^TA^{-1}\bar{u}}
\]</span>
</div>

<hr>

<div class="theorem">
<p><span id="thm:woodbury" class="theorem"><strong>Theorem 1.7  (Woodbury Identity)  </strong></span>Let <span class="math inline">\(A\)</span> be an invertible <span class="math inline">\(n \times n\)</span> square matrix, and <span class="math inline">\(U, V\)</span> nonzero <span class="math inline">\(n \times k\)</span> matrices for some small values of <span class="math inline">\(k\)</span>. Then, <span class="math inline">\(A + UV^T\)</span> is invertible if and only if the <span class="math inline">\(k \times k\)</span> amtrix <span class="math inline">\((I +V^TA^{-1}U)\)</span> is invertible. In such a case, the inverse formula is given by</p>
<span class="math display">\[
(A + UV)^{-1} = A^{-1} - A^{-1}U(I + V^TA^{-1}U)^{-1}V^TA^{-1}
\]</span>
</div>

<p>It’s easy to find that the Woodbury Identity is an extension of the matrix inversion lemma, where <span class="math inline">\(\bar{u} \rightarrow U\)</span> and <span class="math inline">\(\bar{v} \rightarrow V\)</span>.</p>
<hr>

<div class="proposition">
<p><span id="prp:unnamed-chunk-5" class="proposition"><strong>Proposition 1.1  </strong></span>For square matrix <span class="math inline">\(P\)</span> (<strong>not</strong> assuming invertible), if <span class="math inline">\(I + P\)</span> is invertible, then <span class="math inline">\((I + P)^{-1}\)</span> satisfies:</p>
<span class="math display">\[
(I + P)^{-1} = I - (I + P)^{-1}P
\]</span>
</div>

<p>As a quick check, right and left multiply <span class="math inline">\((I + P)\)</span> to see if we get an identity.</p>
<p><span class="math display">\[
\begin{split}
(I + P)^{-1}(I + P) &amp;= I \\
&amp;= \Big(I - (I + P)^{-1}P \Big )(I + P) \\
&amp;= I + P -(1 + P)^{-1}P(I + P) \\
&amp;= (I + P)( I - (I + P)^{-1}P) \\
&amp;= (I + P)(I + P)^{-1}\\
&amp;= I
\end{split}
\]</span>
and left multiply</p>
<span class="math display">\[
\begin{split}
(I + P)(I + P)^{-1} &amp;= I \\
&amp;= (I + P)(I - (I + P)^{-1}P) \\
&amp;= I + P - P\\
&amp;= I
\end{split}
\]</span>
<hr>

<div class="proposition">
<p><span id="prp:push-through-identity" class="proposition"><strong>Proposition 1.2  (Push Through Identity)  </strong></span>Let <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> be <span class="math inline">\(m \times n\)</span> matrices, we have</p>
<p><span class="math display">\[
U^T(I + VU^T)^{-1} = (I + U^TV)^{-1}U^T
\]</span>
The above result shows the following for any <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> and any scalar <span class="math inline">\(\lambda &gt; 0\)</span></p>
<span class="math display">\[
A^T(\lambda I + AA^T)^{-1} = (\lambda I + A^TA)^{-1}A^T
\]</span>
</div>

</div>
</div>
<div id="matrix-multiplication-as-linear-transformation" class="section level2">
<h2><span class="header-section-number">1.7</span> Matrix Multiplication as Linear Transformation</h2>
<p>Another way to look at <span class="math inline">\(A_{m \times n} \, x _{ n \times 1} = b_{m \times 1}\)</span>, besides linear combination of column vectors, is to think of the matrix <span class="math inline">\(A\)</span> as an force that “acts” on a vector <span class="math inline">\(x\)</span> in <span class="math inline">\(\mathbb{R^n}\)</span> by multiplication to produce a new vector called <span class="math inline">\(b\)</span> in <span class="math inline">\(\mathbb{\mathbb{R^m}}\)</span>.</p>
<p>A transformation <span class="math inline">\(T\)</span> from <span class="math inline">\(\mathbb{R^n}\)</span> to <span class="math inline">\(\mathbb{R^m}\)</span> is a rule that assigns each vector {x} in <span class="math inline">\(\mathbb{R^n}\)</span> a vector <span class="math inline">\(T(x)\)</span> in <span class="math inline">\(\mathbb{R^m}\)</span>, which is called the <strong>image</strong> of {x} (under the action of <span class="math inline">\(T\)</span>).</p>
<p>It can be show that such transformations induced by multiplying a matrix is a type of <strong>linear transformation</strong>, because it satisfies all required properties to be linear:</p>
<span class="math display">\[
\begin{aligned}
\text{vector addition} \quad A(\bar{u} + \bar{v}) &amp;= A\bar{u} + A\bar{v}  \\ 
\text{scalar multiplication} \quad A(c\bar{u}) &amp;= cA\bar{u}
\end{aligned}
\]</span>
<hr>

<div class="theorem">
<p><span id="thm:unnamed-chunk-6" class="theorem"><strong>Theorem 1.8  (left multiplication as linear transformation)  </strong></span>There is a one to one relationship between a linear transformation and a matrix. Let <span class="math inline">\(T: \mathbb{R^n} \rightarrow \mathbb{R^m}\)</span> be a linear transformation. Then there exists a <strong>unique</strong> matrix <span class="math inline">\(A\)</span> such that:</p>
<p><span class="math display">\[
T(x) = Ax \quad \text{for all} \; x \; \text{in} \; \mathbb{R^n}  
\]</span></p>
In fact, <span class="math inline">\(A\)</span> is a <span class="math inline">\(m \times n\)</span> matrix whose <span class="math inline">\(j\)</span>th column is the vector <span class="math inline">\(T(\bar{e_j})\)</span>, where <span class="math inline">\(\bar{e_j}\)</span> is the <span class="math inline">\(j\)</span>th basis of <span class="math inline">\(\mathbb{R^n}\)</span>
</div>


<div class="proof">
Proof
</div>

<p><span class="math display">\[
\bar{x} = x_1\bar{e_1} + \dots + x_n{\bar{e_n}}
\]</span>
And because <span class="math inline">\(T(\bar{x})\)</span> is a linear transformation:</p>
<p><span class="math display">\[
\begin{split}
T(\bar{x}) &amp;= x_1T(\bar{e}_1) + \dots + x_nT(\bar{e}_n) \\
&amp;= [T(\bar{e}_1) \, \cdots \, T(\bar{e}_n)]\bar{x} \\
&amp;= (A\bar{x})_{m \times 1}
\end{split}
\]</span></p>
<p>In other words, the transformation is specified once we know what all basis in <span class="math inline">\(\mathbb{R^n}\)</span> become in <span class="math inline">\(\mathbb{R^m}\)</span>.</p>
<p>The matrix <span class="math inline">\(A\)</span> is called the <strong>standard matrix for the linear transformation</strong> <span class="math inline">\(T\)</span>.</p>
<hr>


<div class="definition">
<span id="def:unnamed-chunk-8" class="definition"><strong>Definition 1.2  (A mapping is onto <span class="math inline">\(\mathbb{R^m}\)</span>)  </strong></span>A mapping <span class="math inline">\(T: \mathbb{R^n} \rightarrow \mathbb{R^m}\)</span> is said to be <strong>onto</strong>  if each <span class="math inline">\(\bar{b}\)</span> in <span class="math inline">\(\mathbb{R^m}\)</span> is the image of at least one <span class="math inline">\(\bar{x}\)</span> in 
</div>

<p>Equivalently, <span class="math inline">\(T\)</span> is onto <span class="math inline">\(\mathbb{R^m}\)</span> means that there exists at least one solution of <span class="math inline">\(T(\bar{x}) = \bar{b}\)</span> for any <span class="math inline">\(\bar{b}\)</span> in <span class="math inline">\(\mathbb{R}^m\)</span></p>
<p><br></p>

<div class="definition">
<p><span id="def:unnamed-chunk-9" class="definition"><strong>Definition 1.3  (one-to-one mapping)  </strong></span></p>
A mapping T: <span class="math inline">\(\mathbb{R^n} \rightarrow \mathbb{R^m}\)</span> is said to be <strong>one-to-one</strong> if each <span class="math inline">\(\bar{b}\)</span> in <span class="math inline">\(\mathbb{R^m}\)</span> is the image of <em>at most</em> one <span class="math inline">\(\bar{x}\)</span> in <span class="math inline">\(\mathbb{R^n}\)</span>
</div>

<p>Equivalently, <span class="math inline">\(T\)</span> is one-to-one if, for each <span class="math inline">\(\bar{b}\)</span> in <span class="math inline">\(\mathbb{R^m}\)</span>, the equation <span class="math inline">\(T(\bar{x}) = \bar{b}\)</span> has either a unique solution or none at all.</p>
<p><br></p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-10" class="theorem"><strong>Theorem 1.9  </strong></span>Let <span class="math inline">\(\mathbb{R}^n \rightarrow \mathbb{R}^m\)</span> be a linear transformation, and <span class="math inline">\(A\)</span> the standard matrix. Then</p>
<ul>
<li><p><span class="math inline">\(T\)</span> maps <span class="math inline">\(mathbb{R}^n\)</span> onto <span class="math inline">\(\mathbb{R}^m\)</span> if only if columns of <span class="math inline">\(A\)</span> span <span class="math inline">\(\mathbb{R}^m\)</span></p></li>
<li><p><span class="math inline">\(R\)</span> is one-to-one if and only if columns of <span class="math inline">\(A\)</span> are linearly independent</p>
</div></li>
</ul>
</div>
<div id="complexity-of-matrix-computation" class="section level2">
<h2><span class="header-section-number">1.8</span> Complexity of Matrix Computation</h2>
<p>This chapter concludes with a summary of complexity (number of flops needed) in common matrix computations.</p>
<p>An <span class="math inline">\(m \times n\)</span> matrix is often represented by an <span class="math inline">\(m \times n\)</span> array of floating numbers in a computer, requiring <span class="math inline">\(8mn\)</span> bytes (8 bytes for on element). In the case of a sparse matrix, we can store only the nonzero entries with row index <span class="math inline">\(i\)</span> (integer), column index <span class="math inline">\(j\)</span> (integer) and its value <span class="math inline">\(A_{ij}\)</span> (floating point). Let <span class="math inline">\(\text{nnz}(A)\)</span> be the number of nonzero entries, since we only need 4 bytes to store a integer, storing a sparse matrix requires roughly <span class="math inline">\(16\text{nnz}(A)\)</span> bytes.</p>
<p>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(m \times n\)</span> matrix, and other matrices / vectors be of conformable size for arithmetic operations. The complexity (number of flops) of common matrix operations are listed below</p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Operation</th>
<th align="center">Expression</th>
<th align="center">Complexity</th>
<th align="center">Explanation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">matrix addition</td>
<td align="center"><span class="math inline">\(A + B\)</span></td>
<td align="center"><span class="math inline">\(mn\)</span> <br> and <span class="math inline">\(\min(\text{nnz}(A), \text{nnz}(B))\)</span> if one of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is sparse</td>
<td align="center">For any entry <span class="math inline">\(i,j\)</span> for which one of <span class="math inline">\(A_{ij}\)</span> and <span class="math inline">\(B_{ij}\)</span> is zero, no arithmetic operations are needed to find <span class="math inline">\((A + B)_{ij}\)</span></td>
</tr>
<tr class="even">
<td align="center">scalar multiplication</td>
<td align="center"><span class="math inline">\(kA\)</span></td>
<td align="center"><span class="math inline">\(mn\)</span> <br> <span class="math inline">\(\text{nnz}(A)\)</span> when <span class="math inline">\(A\)</span> is sparse</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">transposition</td>
<td align="center"><span class="math inline">\(A^T\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center">Computing <span class="math inline">\(A^T\)</span> needs only copying, but no flops</td>
</tr>
<tr class="even">
<td align="center">matrix-vector multiplication</td>
<td align="center"><span class="math inline">\(A\bar{x}\)</span></td>
<td align="center"><span class="math inline">\(m(2n - 1)\)</span></td>
<td align="center">equivalent to <span class="math inline">\(m\)</span> inner products between rows of <span class="math inline">\(A\)</span> and <span class="math inline">\(\bar{x}\)</span></td>
</tr>
<tr class="odd">
<td align="center">matrix-vector multiplication when <span class="math inline">\(A\)</span> is sparse</td>
<td align="center">between <span class="math inline">\(\text{nnz}(A)\)</span> and <span class="math inline">\(2\text{nnz}(A) - 1\)</span></td>
<td align="center"><span class="math inline">\(A\bar{x}\)</span></td>
<td align="center">First we need <span class="math inline">\(\text{nnz}(A)\)</span> multiplications, and certain number of additions. We need most additions when nonzero entries are arranged next to each other, and least when they are separated in different columns and rows (for example for diagonal matrix <span class="math inline">\(A\)</span>, we need no additions at all)</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>In particular, a non-singular matrix with <span class="math inline">\(a_{11} = 0\)</span> cannot have LU decomposition<a href="basic-matrix-algebra.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>square matrices whose diagonal entries are those of the original matrix<a href="basic-matrix-algebra.html#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="vector-spaces.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/enixam/math-foundations/edit/master/matrix-algebra.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
