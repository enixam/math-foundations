
# Orthogonality 

Some definitions:  

If $\boldsymbol{u}$ and $\boldsymbol{v}$ and both vectors in $\mathbb{\mathbb{R^n}}$, then the number $\boldsymbol{u}^T\boldsymbol{v}$ is called the **inner product** of $\boldsymbol{u}$ and $\boldsymbol{v}$, also denoted by $\langle\boldsymbol{u}, \boldsymbol{v}\rangle$ or $\boldsymbol{u} \cdot \boldsymbol{v}$.  Also, when $\boldsymbol{u}$ is perpendicular to $\boldsymbol{v}$ we have $\boldsymbol{u} \cdot \boldsymbol{v} = 0$.

<br>
**Norm** is a function from a vector space to a nonnegative scalar that satisfies certain properties. 

The L2 norm (most common measure of length of a vector) of $\boldsymbol{v}$ is  $||\boldsymbol{u}||$ defined by 

$$
||\boldsymbol{v}||_2 = \sqrt{v_1^2 + \cdots + v_n^2} = \sqrt{\boldsymbol{v}^T\boldsymbol{v}}
$$
The subscript $2$ can be left out in most cases. 

Moreover, the L1 norm of $\boldsymbol{v}$ is 

$$
||\boldsymbol{v}||_1 = |v_1| + \cdots + |v_n|
$$

More generally, the Lp norm is: 

$$
||\boldsymbol{v}||_p = (\sum_{i = 1}^n{x_i^p})^{1/p}
$$

For all these definitions, it can be shown that 

$$
||c\boldsymbol{v}|| = |c| \times ||\boldsymbol{v}||
$$

<br>
The **Euclidean distance** between $\boldsymbol{u}$ and $\boldsymbol{v}$ is the L2 norm of the vector $\boldsymbol{u} - \boldsymbol{v}$  

$$
\text{dist}(\boldsymbol{u}, \boldsymbol{v}) = ||\boldsymbol{u} - \boldsymbol{v}||_2
$$

## Orthogonal decomposition 


### Orthogonal complements

if vector $\boldsymbol{v}$ is orthogonal to every vector in a subspace $W$ of $\mathbb{R^n}$, then $\boldsymbol{v}$ is said to be orthogonal to $W$. The subspace that contains the set of vectors that are orthogonal to $W$ is called the **orthogonal complement**, denoted by $W^{\perp}$. 

This corresponds to discussions in \@ref(four-subspaces), where 

$$
(\text{row} A)^{\perp} = \text{Nul} A \\
(\text{col} A)^{\perp} = \text{Nul} A^T
$$

### Orthogonal sets and orthogonal basis

An orthogonal set is a set of vectors 
$\{\boldsymbol{u_1}, \dots, \boldsymbol{u_p}\}$ in $\mathbb{R^n}$, in which each pair of distinct vectors is orthogonal: $\boldsymbol{u_i}^{T} \boldsymbol{u_j} = 0 \quad i\not = j$. Note that the set do not necessarily span the whole $\mathbb{R^n}$, but a subspace $W$. 

Since vectors in orthogonal sets is mutually perpendicular, they must also be linearly independent and could form a basis for a subspace $W$. In such case, they are called **orthogonal basis**.  

There is a particular advantage in using orthogonal basis rather than other basis, because we can find a easy representation of any vector in $W$.  

```{theorem}
For each $\boldsymbol{y}$ in $W$, we have the following linear combination

$$
y = c_1\boldsymbol{u_1} + \cdots + c_p\boldsymbol{u_p}
$$
  
and 

$$
c_i = \frac{\boldsymbol{y} \cdot \boldsymbol{u_i}}{\boldsymbol{u_i} \cdot \boldsymbol{u_i}} \quad i = 1, \cdots, p
$$
  
where $\{\boldsymbol{u_1}, \dots, \boldsymbol{u_p}\}$ is an orthogonal basis. 
```

**Proof**

$$
\begin{split}
\boldsymbol{u_1} \cdot \boldsymbol{y} &= \boldsymbol{u_1} \cdot (c_1\boldsymbol{u_1} + \cdots + c_p\boldsymbol{u_p}) \\
  &= c_1 \boldsymbol{u_1} \cdot \boldsymbol{u_1}
\end{split}
$$
So: 

$$
c_1 = \frac{\boldsymbol{u_1} \cdot \boldsymbol{y}}{\boldsymbol{u_1} \cdot \boldsymbol{u_1}}
$$

Derivations for other $c_i$ is similar. 



### Orthogonal decomposition

**Orthogonal decomposition** split $\boldsymbol{y}$ in $\mathbb{R^n}$ into two vectors, one in $W$ and one in its orthogonal compliment $W^{\perp}$. The trick is to use $\hat{\boldsymbol{y}}$ as $\boldsymbol{y}$'s projection onto $W$, which can be represented as illustrated in \@ref(orthogonal-sets-and-orthogonal-basis), and the other term, often referred to as error term in statistics, is simply $\boldsymbol{y}- \hat{\boldsymbol{y}}$. 

$$
\boldsymbol{y} = \hat{\boldsymbol{y}} + \boldsymbol{z}= c_1\boldsymbol{u_1} + \cdots + c_p\boldsymbol{u_p} + \boldsymbol{z} 
$$

Where 

$$
c_i = \frac{\boldsymbol{y} \cdot \boldsymbol{u}_i}{\boldsymbol{u}_i \cdot \boldsymbol{u}_i}\boldsymbol{u}_i \quad i = 1, \cdots, p 
$$

### Best approximation 

```{theorem, label = 'best-approximation', name = "The Best Approximation Theorem"}
Given $\boldsymbol{y}$ be any vector in $\mathbb{R^n}$, with its subspace $W$, let $\hat{\boldsymbol{y}}$ be the orthogonal projection of $\boldsymbol{y}$ onto $W$. Then $\hat{\boldsymbol{y}}$ is the closest point in $W$ to $\boldsymbol{y}$ in the sense that 

$$
||\boldsymbol{y} - \hat{\boldsymbol{y}}|| \le ||\boldsymbol{y} - \boldsymbol{v}||
$$
```

**PROOF**

Take $\boldsymbol{v}$ distinct from $\hat{\boldsymbol{y}}$ in $W$, we know that $\boldsymbol{y} - \hat{\boldsymbol{y}}$ is perpendicular to $\boldsymbol{v}$. According to Pythoagorean theorem, we have

```{r, echo = FALSE, fig.cap="figure from page p352, ch6 [@lay2006-3]", out.width = "120%"}
knitr::include_graphics("images/best-approximation.png")
```

$$
||\boldsymbol{y}-  \boldsymbol{v}||^2 = ||\boldsymbol{\hat{y}} - \boldsymbol{v}||^2 + ||\boldsymbol{y} -\boldsymbol{\hat{y}}||^2 
$$
When $\boldsymbol{v}$ is distinct from $\boldsymbol{\hat{y}}$, $||\boldsymbol{\hat{y}} - \boldsymbol{v}||^2$ is non-negative, so the error term of choosing $\boldsymbol{v}$ is always larger than that of the orthogonal projection $\boldsymbol{\hat{y}}$.  



## Gram-Schmidt process

Then Gram-Schmidt process is a simple algorithm that transforms a set of linearly independent vectors into orthogonal or orthonormal basis for a subspace. In its essence, it is a sequential projection of $\boldsymbol{x}_{i}$ onto the space spanned by the previously created orthogonal set $\{\boldsymbol{v}_{1}, ..., \boldsymbol{v}_{i-1}\}$, and take the term in the orthogonal compliment to be $\boldsymbol{v}_{i+1}$   

```{theorem gram-schmidt, name = "the Gram-Schmidt process"}
Given a basis $\{\boldsymbol{x}_1, ..., \boldsymbol{x}_p\}$ for a nonzero subspace $W$ of $\mathbb{R}^n$, define 

$$
\begin{aligned}
\boldsymbol{v}_1 &= \boldsymbol{x}_1 \\
\boldsymbol{v}_2 &= \boldsymbol{x}_2 - \frac{\boldsymbol{x}_2 \cdot \boldsymbol{v}_1}{\boldsymbol{v}_1 \cdot \boldsymbol{v}_1}\boldsymbol{v}_1 \\
\boldsymbol{v}_3 &= \boldsymbol{x}_3 
- \frac{\boldsymbol{x}_3 \cdot \boldsymbol{v}_1}{\boldsymbol{v}_1 \cdot \boldsymbol{v}_1}\boldsymbol{v}_1 
- \frac{\boldsymbol{x}_3 \cdot \boldsymbol{v}_2}{\boldsymbol{v}_2 \cdot \boldsymbol{v}_2}\boldsymbol{v}_2
\\
& \vdots \\
\boldsymbol{v}_p &= \boldsymbol{x}_p 
- \frac{\boldsymbol{x}_p \cdot \boldsymbol{v}_1}{\boldsymbol{v}_1 \cdot \boldsymbol{v}_1}\boldsymbol{v}_1 
- \frac{\boldsymbol{x}_p \cdot \boldsymbol{v}_2}{\boldsymbol{v}_2 \cdot \boldsymbol{v}_2}\boldsymbol{v}_2
- \cdots
- \frac{\boldsymbol{x}_p \cdot \boldsymbol{v}_{p-1}}{\boldsymbol{v}_{p-1} \cdot \boldsymbol{v}_{p-1}}\boldsymbol{v}_{p-1}
\end{aligned}
$$
  
Then $\{\boldsymbol{v}_1, ..., \boldsymbol{v}_p\}$ is an orthogonal basis for $W$. In addition 

$$
\text{Span}\{\boldsymbol{v}_1, ..., \boldsymbol{v}_p\} = \text{Span}\{\boldsymbol{x}_1, ..., \boldsymbol{x}_p\} 
$$
```

To make $\{\boldsymbol{v}_1, ..., \boldsymbol{v}_p\}$ an *orthonormal* basis, there is simply one more step of normalization 

$$
\{\frac{\boldsymbol{v}_i}{||\boldsymbol{v}_i||}, \;i = 1, ... p\} 
$$


### QR factorizaiton 

For $A \in \mathbb{R}^{m \times n}$



## Orthonormal sets and orthogonal matrices

An orthogonal set whose components are all unit vectors are said to be **orthonormal** sets. 

### Orthogonal matrices  

An *orthogonal matrix* is a square matrix $Q$ whose inverse is its transpose:  

$$
(\#eq:orthogonal-matrix)
QQ^T = Q^TQ = I
$$

Another way of defining it is that an orthogonal matrix has both **orthonormal columns** and **orthonormal rows**.  

Orthogonal matrices have a nice property that they preserve inner products: 

$$
(Q\boldsymbol{x})^T(Q\boldsymbol{y}) = \boldsymbol{x}^TQ^TQ\boldsymbol{y} = \boldsymbol{x}^TI\boldsymbol{y} = \boldsymbol{x}^T\boldsymbol{y}
$$

A direct result is that $Q$ preserves L2 norms 

$$
||Q\boldsymbol{x}||_2 = \sqrt{(Q\boldsymbol{x})^T(Q\boldsymbol{x})} = \sqrt{\boldsymbol{x}^T\boldsymbol{x}} = ||\boldsymbol{x}||_2
$$

Therefore multiplication by an orthogonal matrix can be considered as a transformation that preserves length, but may rotate or reflect the vector about the origin. 

Note that $Q$ may not necessarily be a square matrix to satisfy $Q^TQ = I$. For exmaple $Q \in \mathbb{R}^{m \times n}, n < m$, but its columns and rows can still be orthonormal, then $QQ^T = I$. But in most cases the term orthogonal implies a square matrix $Q$. 