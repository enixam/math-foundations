# Expectation and variance

## Properties of Expectation and Variance 

This section provides some properties of $E(X)$ and $\text{Var}(X)$ commonly-used in probabilistic calculations. Suppose all expectations $E(\cdot)$ exists 


- **Non-negativity**: If $X \ge 0$ then $E(X) \ge 0$  
- **Linearity of expectation**  
    - $E(\alpha X) = \alpha E(X)$ ($\alpha$ is constant)  
    - $E(X + Y) = E(X) + E(Y)$ ($Y$ is also r.v., and **not** necessarily independent of $X$). More generally,  for any r.v. $X_1, ..., X_n$ $E(X_1 + \cdots + X_n) =\sum_{i=1}^n{E(X_i)}$  
    

- **Monotonicity**: If $X \le Y$, then $E(X) \le E(Y)$  

- **Non-multiplicativity** ^[Proof: $E(XY) = E(X)E(Y) + \text{Cov}(X, Y)$. Start with covariance $$\begin{split}
\text{Cov}(X, Y) &= E[(X - \mu_x)(Y - \mu_y)] \\
&= E(XY - X\mu_y - Y\mu_x + \mu_x\mu_y) \\
&= E(XY) - \mu_yE(X) - \mu_xE(Y) + \mu_x\mu_y \\
&= E(XY) - 2\mu_x\mu_y + \mu_x\mu_y \\
& = E(XY) - \mu_x\mu_y
\end{split}$$]: $E(XY) = E(X)E(Y) + \text{Cov}(X, Y)$. This means in general $E(XY) \not= E(X)E(Y)$ holds except that $X, Y$ are independent (although this is not a necessary condition) 

- **law of the unconscious statistician (LOTUS)**. Suppose $g(X)$ is a function of $X$. If $X$ is discrete then $E[g(X)] = \sum g(x)f_X(x)$, and if $X$ is continuous $E[g(X)] = \int g(x)f(x)dx$  

- $E(X^2) = \text{Var}(X) + [E(X)]^2$ ^[Proof: $E(X^2) = \text{Var}(X) + \big( E(X) \big)$. $$
\begin{split}
E(X^2) &= E \big[ (X \pm \mu)^2 \big] \\
&= E(X - \mu)^2 + \mu^2 + 2 \mu E(X -\mu) \qquad \text{the 3rd term is zero} \\
&= \text{Var}(X) + [E(X)]^2
\end{split}
$$]



An important extension of the previous identity is used listed in the following lemma. 

```{lemma mean-square-expectation}

The fact that $\text{E}(X^2) = \text{Var}(X) + \mu^2$ is a special case of the following identity


$$
E(X  - c)^2 = \text{Var}(X) + (\mu - c)^2
$$
  
where $c = 0$
```







```{block2, type = "proof"}
Proof
```


Use the fact that variance is not affected when adding a constant 

$$
\begin{split}
\text{Var}(X) &= \text{Var}(X - c) \\
&= E(X - c)^2 - (E(X - c))^2 \\ 
&= E(X - c)^2 - (\mu - c)^2
\end{split}
$$

<hr> 

For $\text{Var}(X)$, we have the following properties 

- **non-negative**: $\text{Var}(X) \ge 0$ with equality only if $X$ follows degenerate distribution  
- **invariant to adding constant**: $\text{Var}(X + \alpha) = \text{Var}(X)$  

- If all values are scaled by a constant, the variance is scaled by the square of that constant:  $\text{Var}(\alpha X) = \alpha^2\text{Var}(X)$  

- The variance of a sum of two random variables is given by 

$$
\begin{aligned}
\text{Var}(aX + bY) &= a^2\text{Var}(X) + b^2\text{Var}(Y) + 2ab \cdot\text{Cov}(X, Y) \\
\text{Var}(aX - bY) &= a^2\text{Var}(X) + b^2\text{Var}(Y) - 2ab \cdot\text{Cov}(X, Y)
\end{aligned}
$$
By extension, if $X$ and $Y$ are uncorrelated, $\text{Var}(aX + bY) = a^2\text{Var}(X) + b^2\text{Var}(Y)$


- **variance-covariance operations**: the variance of a linear combination of r.v. is the sum of variance plus all pairs of covariances 

$$
\text{Var}(\sum_{i=1}^{n} \alpha_i X_i) =
\sum_{i=1}^{n} \alpha^2 \text{Var}(X_i) + \sum_{i \not = j}a_ia_j \text{Cov}(X_i, X_j)
$$


### Stain's Lemma
  


### Random Vectors 


## Conditional expectation

https://en.m.wikipedia.org/wiki/Conditional_expectation